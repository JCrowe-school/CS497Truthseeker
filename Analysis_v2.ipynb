{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b56a26-dc6a-43bb-8398-a76752038969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 111,593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASET_PATH = \"./Truth_Seeker_Model_Dataset.csv\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "df = df.loc[~((df['5_label_majority_answer'] == 'NO MAJORITY') | (df['3_label_majority_answer'] == 'NO MAJORITY'))]\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59cffe6a-71d1-425e-9f78-a089d4e5e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = 'Statement: ' + df['statement'] + '| Tweet: ' +df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13803549-d9c5-4d3f-8d3c-c496cba66ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic.| Tweet: @POTUS Biden Blunders - 6 Month Update\\n\\nInflation, Delta mismanagement, COVID for kids, Abandoning Americans in Afghanistan, Arming the Taliban, S. Border crisis, Breaking job growth, Abuse of power (Many Exec Orders, $3.5T through Reconciliation, Eviction Moratorium)...what did I miss?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8caac61c-165e-4325-83cd-08de139abaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"BinaryNumTarget\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53567947-0e0f-4120-8a9b-92c070809b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_truthfulness_4way(row):\n",
    "    if row['target'] == True:\n",
    "        if row['5_label_majority_answer'] == 'Agree':\n",
    "            return \"True\"\n",
    "        elif row['5_label_majority_answer'] == 'Disagree':\n",
    "            return \"False\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Agree':\n",
    "            return \"Mostly True\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Disagree':\n",
    "            return \"Mostly False\"\n",
    "    else:\n",
    "        if row['5_label_majority_answer'] == 'Agree':\n",
    "            return \"False\"\n",
    "        elif row['5_label_majority_answer'] == 'Disagree':\n",
    "            return \"True\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Agree':\n",
    "            return \"Mostly False\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Disagree':\n",
    "            return \"Mostly True\"\n",
    "\n",
    "def generate_truthfulness_2way(row):\n",
    "    if row['target'] == True:\n",
    "        if row['3_label_majority_answer'] == 'Agree':\n",
    "            return \"True\"\n",
    "        elif row['3_label_majority_answer'] == 'Disagree':\n",
    "            return \"False\"\n",
    "    else:\n",
    "        if row['3_label_majority_answer'] == 'Agree':\n",
    "            return \"False\"\n",
    "        elif row['3_label_majority_answer'] == 'Disagree':\n",
    "            return \"True\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af7d7a7-da69-45f3-a5c2-054e4e63ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "df2['4-way-label'] = df.apply(lambda x: generate_truthfulness_4way(x), axis=1)\n",
    "df2['2-way-label'] = df.apply(lambda x: generate_truthfulness_2way(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af0b815c-bc6b-4e8a-bbfc-74034db88b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4-way-label</th>\n",
       "      <th>2-way-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110397</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100855</th>\n",
       "      <td>Mostly False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58691</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72511</th>\n",
       "      <td>Mostly True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64040</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27862</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115320</th>\n",
       "      <td>Mostly False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81209</th>\n",
       "      <td>Mostly False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119907</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111593 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         4-way-label 2-way-label\n",
       "110397         False       False\n",
       "100855  Mostly False       False\n",
       "58691           True        True\n",
       "4455            True        True\n",
       "72511    Mostly True        True\n",
       "...              ...         ...\n",
       "64040           True        True\n",
       "27862           True        True\n",
       "115320  Mostly False       False\n",
       "81209   Mostly False       False\n",
       "119907         False       False\n",
       "\n",
       "[111593 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f91527-667e-4278-8f96-d58d438f7e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_511802/487703377.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df2['2-way-label'] = df2['2-way-label'].replace({'True': 0, 'False': 1})\n"
     ]
    }
   ],
   "source": [
    "df2['2-way-label'] = df2['2-way-label'].replace({'True': 0, 'False': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5affeb06-8cd1-4b58-9000-09419efac91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4-way-label</th>\n",
       "      <th>2-way-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110397</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100855</th>\n",
       "      <td>Mostly False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58691</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72511</th>\n",
       "      <td>Mostly True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64040</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27862</th>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115320</th>\n",
       "      <td>Mostly False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81209</th>\n",
       "      <td>Mostly False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119907</th>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111593 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         4-way-label  2-way-label\n",
       "110397         False            1\n",
       "100855  Mostly False            1\n",
       "58691           True            0\n",
       "4455            True            0\n",
       "72511    Mostly True            0\n",
       "...              ...          ...\n",
       "64040           True            0\n",
       "27862           True            0\n",
       "115320  Mostly False            1\n",
       "81209   Mostly False            1\n",
       "119907         False            1\n",
       "\n",
       "[111593 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15df6ee4-c79f-478a-b893-f5c29195ed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         4-way-label  2-way-label\n",
      "110397         False            1\n",
      "100855  Mostly False            1\n",
      "58691           True            0\n",
      "4455            True            0\n",
      "72511    Mostly True            0\n",
      "...              ...          ...\n",
      "64040           True            0\n",
      "27862           True            0\n",
      "115320  Mostly False            1\n",
      "81209   Mostly False            1\n",
      "119907         False            1\n",
      "\n",
      "[111593 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2['2-way-label'].plot(kind='hist')\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cefbc76-0e2c-4afc-bc90-c1cd2c172cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df2['2-way-label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2658dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01944a71-51db-47e2-b7f8-41fe07d1609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/jcrowe/truth/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5031cf4e-3047-4bdd-99d7-90d1f2fb2d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic.| Tweet: @POTUS Biden Blunders - 6 Month Update\n",
      "\n",
      "Inflation, Delta mismanagement, COVID for kids, Abandoning Americans in Afghanistan, Arming the Taliban, S. Border crisis, Breaking job growth, Abuse of power (Many Exec Orders, $3.5T through Reconciliation, Eviction Moratorium)...what did I miss?\n",
      "Tokenized:  ['statement', ':', 'end', 'of', 'ev', '##iction', 'mora', '##torium', 'means', 'millions', 'of', 'americans', 'could', 'lose', 'their', 'housing', 'in', 'the', 'middle', 'of', 'a', 'pan', '##de', '##mic', '.', '|', 't', '##wee', '##t', ':', '@', 'pot', '##us', 'bid', '##en', 'blu', '##nder', '##s', '-', '6', 'month', 'update', 'inflation', ',', 'delta', 'mis', '##mana', '##gement', ',', 'co', '##vid', 'for', 'kids', ',', 'abandoning', 'americans', 'in', 'afghanistan', ',', 'arm', '##ing', 'the', 'taliban', ',', 's', '.', 'border', 'crisis', ',', 'breaking', 'job', 'growth', ',', 'abuse', 'of', 'power', '(', 'many', 'ex', '##ec', 'orders', ',', '$', '3', '.', '5', '##t', 'through', 'reconciliation', ',', 'ev', '##iction', 'mora', '##torium', ')', '.', '.', '.', 'what', 'did', 'i', 'miss', '?']\n",
      "Token IDs:  [4861, 1024, 2203, 1997, 23408, 28097, 26821, 24390, 2965, 8817, 1997, 4841, 2071, 4558, 2037, 3847, 1999, 1996, 2690, 1997, 1037, 6090, 3207, 7712, 1012, 1064, 1056, 28394, 2102, 1024, 1030, 8962, 2271, 7226, 2368, 14154, 11563, 2015, 1011, 1020, 3204, 10651, 14200, 1010, 7160, 28616, 24805, 20511, 1010, 2522, 17258, 2005, 4268, 1010, 19816, 4841, 1999, 7041, 1010, 2849, 2075, 1996, 16597, 1010, 1055, 1012, 3675, 5325, 1010, 4911, 3105, 3930, 1010, 6905, 1997, 2373, 1006, 2116, 4654, 8586, 4449, 1010, 1002, 1017, 1012, 1019, 2102, 2083, 16088, 1010, 23408, 28097, 26821, 24390, 1007, 1012, 1012, 1012, 2054, 2106, 1045, 3335, 1029]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea3b9c89-44de-4609-a518-3052a589908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111593/111593 [01:09<00:00, 1603.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca91cb45-1857-4574-a619-8cbba56273d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                               | 0/111593 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/nbuser/jcrowe/truth/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111593/111593 [01:31<00:00, 1219.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "MAX_SENTENCE_LENGTH = 410\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dba6647-c6d0-4fe5-994b-661ab9b432cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ae78572-5c97-48b2-b8fd-1635b5745830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic.| Tweet: BREAKING NEWS: Mitch McConnell accuses President Biden of pushing socialism by implementing the eviction moratorium that will stop millions of Americans from being thrown out on the street this month. RT if you think that Mitch is a heartless idiot!\n",
      "Token IDs: tensor([  101,  4861,  1024,  2758,  6221,  8398,  1000,  4122,  2000,  2131,\n",
      "         9436,  1997,  1996,  2976,  6263, 11897,  1012,  1000,  1064,  1056,\n",
      "        28394,  2102,  1024,  1030,  5977,  4819, 22758,  2002,  2469,  4297,\n",
      "        17572,  2019,  1000, 13760,  1000,  1999,  7912, 21877, 10483,  2072,\n",
      "         2016,  1005,  1055,  2018,  1037,  2524,  2006,  2005,  8398,  2005,\n",
      "         1018,  1012,  1019,  2086,  2085,  1048,  2863,  2080,  1998,  2053,\n",
      "         4415,  2678,  1005,  1055,  1997,  3424,  7011,  5278,  2046,  8398,\n",
      "        22054,  2035,  2058,  1996,  4274,  2023,  2442,  2022,  1996,  1002,\n",
      "         2321,  2566,  3178,  6263, 11897,  2976,  7904,  1005,  1055,  7226,\n",
      "         2368,  2003,  3228,  1037,  5333,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Labels: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = 10\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[index])\n",
    "print('Token IDs:', input_ids[index])\n",
    "print ('Labels:', labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5374dda6-4c51-4179-a733-e5889f8dd0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([111593, 410])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f2e335c-d319-42a0-a4f8-719cc21c29bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89,274 training samples\n",
      "22,319 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 80-20 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f06f5391-9385-4566-9200-92eebe31108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 2\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = RandomSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6bc13d4-68d2-481a-8431-21764fed64ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "402ce2a1-a2bf-43af-86e8-322076484fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08ab44ef-2a35-4b1d-a9a0-83598f033094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/jcrowe/truth/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f73d228-43b8-4a2e-afc4-18df28d17523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# I'm running for 2 due to time.\n",
    "epochs = 2\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "debfe942-a9b0-4990-88d6-50093725dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "427c284e-eb0e-4e17-a9d0-7f990c56176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b26116-d8a9-482c-9867-8d1fab53a7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    #torch.cuda.device(2) #did not work\n",
    "\n",
    "    # Limit PyTorch to only 75% of GPU to not impact normal daily performance\n",
    "    # torch.cuda.set_per_process_memory_fraction(0.75, 0)\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49e6e9bf-6f44-439f-bc7a-97b017247a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = './checkpoints/checkpoint_with_maxlength_410'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c612aa1f-af4e-47df-88b9-97f64475d915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of  44,637.    Elapsed: 0:00:02. Training loss. 0.004337724298238754 Num fake examples 42 Num true examples 38\n",
      "  Batch    80  of  44,637.    Elapsed: 0:00:03. Training loss. 0.005030684173107147 Num fake examples 88 Num true examples 72\n",
      "  Batch   120  of  44,637.    Elapsed: 0:00:05. Training loss. 0.00823903363198042 Num fake examples 127 Num true examples 113\n",
      "  Batch   160  of  44,637.    Elapsed: 0:00:06. Training loss. 0.009292326867580414 Num fake examples 159 Num true examples 161\n",
      "  Batch   200  of  44,637.    Elapsed: 0:00:08. Training loss. 0.009026182815432549 Num fake examples 202 Num true examples 198\n",
      "  Batch   240  of  44,637.    Elapsed: 0:00:09. Training loss. 0.007299569435417652 Num fake examples 240 Num true examples 240\n",
      "  Batch   280  of  44,637.    Elapsed: 0:00:11. Training loss. 0.01029890961945057 Num fake examples 285 Num true examples 275\n",
      "  Batch   320  of  44,637.    Elapsed: 0:00:12. Training loss. 0.004590651951730251 Num fake examples 317 Num true examples 323\n",
      "  Batch   360  of  44,637.    Elapsed: 0:00:14. Training loss. 0.004782174713909626 Num fake examples 358 Num true examples 362\n",
      "  Batch   400  of  44,637.    Elapsed: 0:00:16. Training loss. 0.00585531909018755 Num fake examples 405 Num true examples 395\n",
      "  Batch   440  of  44,637.    Elapsed: 0:00:17. Training loss. 0.00568765914067626 Num fake examples 451 Num true examples 429\n",
      "  Batch   480  of  44,637.    Elapsed: 0:00:19. Training loss. 2.697657585144043 Num fake examples 490 Num true examples 470\n",
      "  Batch   520  of  44,637.    Elapsed: 0:00:20. Training loss. 0.004266431089490652 Num fake examples 532 Num true examples 508\n",
      "  Batch   560  of  44,637.    Elapsed: 0:00:22. Training loss. 0.003952859900891781 Num fake examples 572 Num true examples 548\n",
      "  Batch   600  of  44,637.    Elapsed: 0:00:23. Training loss. 0.002912195399403572 Num fake examples 616 Num true examples 584\n",
      "  Batch   640  of  44,637.    Elapsed: 0:00:25. Training loss. 0.0044289217330515385 Num fake examples 654 Num true examples 626\n",
      "  Batch   680  of  44,637.    Elapsed: 0:00:27. Training loss. 0.005765322130173445 Num fake examples 690 Num true examples 670\n",
      "  Batch   720  of  44,637.    Elapsed: 0:00:28. Training loss. 0.0037964892107993364 Num fake examples 731 Num true examples 709\n",
      "  Batch   760  of  44,637.    Elapsed: 0:00:30. Training loss. 3.0200939178466797 Num fake examples 765 Num true examples 755\n",
      "  Batch   800  of  44,637.    Elapsed: 0:00:31. Training loss. 0.0035940716043114662 Num fake examples 798 Num true examples 802\n",
      "  Batch   840  of  44,637.    Elapsed: 0:00:33. Training loss. 2.7524526119232178 Num fake examples 840 Num true examples 840\n",
      "  Batch   880  of  44,637.    Elapsed: 0:00:34. Training loss. 0.0039078895933926105 Num fake examples 880 Num true examples 880\n",
      "  Batch   920  of  44,637.    Elapsed: 0:00:36. Training loss. 0.0040338062681257725 Num fake examples 919 Num true examples 921\n",
      "  Batch   960  of  44,637.    Elapsed: 0:00:37. Training loss. 0.004188240971416235 Num fake examples 955 Num true examples 965\n",
      "  Batch 1,000  of  44,637.    Elapsed: 0:00:39. Training loss. 2.8775079250335693 Num fake examples 989 Num true examples 1011\n",
      "  Batch 1,040  of  44,637.    Elapsed: 0:00:40. Training loss. 0.0048148296773433685 Num fake examples 1033 Num true examples 1047\n",
      "  Batch 1,080  of  44,637.    Elapsed: 0:00:41. Training loss. 2.3709795475006104 Num fake examples 1068 Num true examples 1092\n",
      "  Batch 1,120  of  44,637.    Elapsed: 0:00:43. Training loss. 0.004503417760133743 Num fake examples 1112 Num true examples 1128\n",
      "  Batch 1,160  of  44,637.    Elapsed: 0:00:45. Training loss. 0.003915362525731325 Num fake examples 1157 Num true examples 1163\n",
      "  Batch 1,200  of  44,637.    Elapsed: 0:00:46. Training loss. 0.0032426263205707073 Num fake examples 1196 Num true examples 1204\n",
      "  Batch 1,240  of  44,637.    Elapsed: 0:00:47. Training loss. 0.003917708061635494 Num fake examples 1230 Num true examples 1250\n",
      "  Batch 1,280  of  44,637.    Elapsed: 0:00:49. Training loss. 0.002568586962297559 Num fake examples 1274 Num true examples 1286\n",
      "  Batch 1,320  of  44,637.    Elapsed: 0:00:50. Training loss. 0.0030217438470572233 Num fake examples 1312 Num true examples 1328\n",
      "  Batch 1,360  of  44,637.    Elapsed: 0:00:52. Training loss. 0.004064757376909256 Num fake examples 1353 Num true examples 1367\n",
      "  Batch 1,400  of  44,637.    Elapsed: 0:00:53. Training loss. 0.006365342065691948 Num fake examples 1385 Num true examples 1415\n",
      "  Batch 1,440  of  44,637.    Elapsed: 0:00:54. Training loss. 0.004878424108028412 Num fake examples 1418 Num true examples 1462\n",
      "  Batch 1,480  of  44,637.    Elapsed: 0:00:56. Training loss. 0.005016958341002464 Num fake examples 1463 Num true examples 1497\n",
      "  Batch 1,520  of  44,637.    Elapsed: 0:00:57. Training loss. 0.005215157754719257 Num fake examples 1502 Num true examples 1538\n",
      "  Batch 1,560  of  44,637.    Elapsed: 0:00:59. Training loss. 0.004912626929581165 Num fake examples 1545 Num true examples 1575\n",
      "  Batch 1,600  of  44,637.    Elapsed: 0:01:00. Training loss. 0.0028442800976336002 Num fake examples 1577 Num true examples 1623\n",
      "  Batch 1,640  of  44,637.    Elapsed: 0:01:01. Training loss. 0.003971198573708534 Num fake examples 1612 Num true examples 1668\n",
      "  Batch 1,680  of  44,637.    Elapsed: 0:01:03. Training loss. 0.003731817938387394 Num fake examples 1655 Num true examples 1705\n",
      "  Batch 1,720  of  44,637.    Elapsed: 0:01:04. Training loss. 0.006023863330483437 Num fake examples 1692 Num true examples 1748\n",
      "  Batch 1,760  of  44,637.    Elapsed: 0:01:06. Training loss. 0.004341870546340942 Num fake examples 1725 Num true examples 1795\n",
      "  Batch 1,800  of  44,637.    Elapsed: 0:01:07. Training loss. 0.0038262472953647375 Num fake examples 1761 Num true examples 1839\n",
      "  Batch 1,840  of  44,637.    Elapsed: 0:01:08. Training loss. 0.004746881313621998 Num fake examples 1799 Num true examples 1881\n",
      "  Batch 1,880  of  44,637.    Elapsed: 0:01:10. Training loss. 0.0030078841373324394 Num fake examples 1830 Num true examples 1930\n",
      "  Batch 1,920  of  44,637.    Elapsed: 0:01:11. Training loss. 0.002661125734448433 Num fake examples 1879 Num true examples 1961\n",
      "  Batch 1,960  of  44,637.    Elapsed: 0:01:13. Training loss. 0.0029737758450210094 Num fake examples 1910 Num true examples 2010\n",
      "  Batch 2,000  of  44,637.    Elapsed: 0:01:14. Training loss. 0.0028029042296111584 Num fake examples 1952 Num true examples 2048\n",
      "  Batch 2,040  of  44,637.    Elapsed: 0:01:16. Training loss. 0.002560095861554146 Num fake examples 1989 Num true examples 2091\n",
      "  Batch 2,080  of  44,637.    Elapsed: 0:01:17. Training loss. 0.002476775785908103 Num fake examples 2026 Num true examples 2134\n",
      "  Batch 2,120  of  44,637.    Elapsed: 0:01:19. Training loss. 0.0025507155805826187 Num fake examples 2062 Num true examples 2178\n",
      "  Batch 2,160  of  44,637.    Elapsed: 0:01:20. Training loss. 2.6285836696624756 Num fake examples 2106 Num true examples 2214\n",
      "  Batch 2,200  of  44,637.    Elapsed: 0:01:21. Training loss. 0.004393681418150663 Num fake examples 2146 Num true examples 2254\n",
      "  Batch 2,240  of  44,637.    Elapsed: 0:01:23. Training loss. 0.0027833576314151287 Num fake examples 2180 Num true examples 2300\n",
      "  Batch 2,280  of  44,637.    Elapsed: 0:01:24. Training loss. 0.0027932855300605297 Num fake examples 2223 Num true examples 2337\n",
      "  Batch 2,320  of  44,637.    Elapsed: 0:01:26. Training loss. 0.002940807258710265 Num fake examples 2263 Num true examples 2377\n",
      "  Batch 2,360  of  44,637.    Elapsed: 0:01:27. Training loss. 0.0033493959344923496 Num fake examples 2301 Num true examples 2419\n",
      "  Batch 2,400  of  44,637.    Elapsed: 0:01:28. Training loss. 0.004328776150941849 Num fake examples 2339 Num true examples 2461\n",
      "  Batch 2,440  of  44,637.    Elapsed: 0:01:30. Training loss. 0.004896924830973148 Num fake examples 2380 Num true examples 2500\n",
      "  Batch 2,480  of  44,637.    Elapsed: 0:01:31. Training loss. 0.004043208435177803 Num fake examples 2427 Num true examples 2533\n",
      "  Batch 2,520  of  44,637.    Elapsed: 0:01:33. Training loss. 0.008775025606155396 Num fake examples 2467 Num true examples 2573\n",
      "  Batch 2,560  of  44,637.    Elapsed: 0:01:34. Training loss. 0.003929096274077892 Num fake examples 2496 Num true examples 2624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2,600  of  44,637.    Elapsed: 0:01:36. Training loss. 0.005547362845391035 Num fake examples 2537 Num true examples 2663\n",
      "  Batch 2,640  of  44,637.    Elapsed: 0:01:37. Training loss. 2.775083541870117 Num fake examples 2577 Num true examples 2703\n",
      "  Batch 2,680  of  44,637.    Elapsed: 0:01:38. Training loss. 0.00566071318462491 Num fake examples 2615 Num true examples 2745\n",
      "  Batch 2,720  of  44,637.    Elapsed: 0:01:40. Training loss. 2.6940231323242188 Num fake examples 2660 Num true examples 2780\n",
      "  Batch 2,760  of  44,637.    Elapsed: 0:01:41. Training loss. 0.0048941937275230885 Num fake examples 2694 Num true examples 2826\n",
      "  Batch 2,800  of  44,637.    Elapsed: 0:01:43. Training loss. 0.006894731894135475 Num fake examples 2734 Num true examples 2866\n",
      "  Batch 2,840  of  44,637.    Elapsed: 0:01:44. Training loss. 0.005916972644627094 Num fake examples 2769 Num true examples 2911\n",
      "  Batch 2,880  of  44,637.    Elapsed: 0:01:45. Training loss. 0.0031659086234867573 Num fake examples 2810 Num true examples 2950\n",
      "  Batch 2,920  of  44,637.    Elapsed: 0:01:47. Training loss. 0.0036764931865036488 Num fake examples 2850 Num true examples 2990\n",
      "  Batch 2,960  of  44,637.    Elapsed: 0:01:48. Training loss. 0.0031763012520968914 Num fake examples 2898 Num true examples 3022\n",
      "  Batch 3,000  of  44,637.    Elapsed: 0:01:50. Training loss. 0.003062523901462555 Num fake examples 2937 Num true examples 3063\n",
      "  Batch 3,040  of  44,637.    Elapsed: 0:01:51. Training loss. 0.0032202121801674366 Num fake examples 2978 Num true examples 3102\n",
      "  Batch 3,080  of  44,637.    Elapsed: 0:01:52. Training loss. 0.0032289200462400913 Num fake examples 3017 Num true examples 3143\n",
      "  Batch 3,120  of  44,637.    Elapsed: 0:01:54. Training loss. 0.005220027640461922 Num fake examples 3053 Num true examples 3187\n",
      "  Batch 3,160  of  44,637.    Elapsed: 0:01:55. Training loss. 0.002815850544720888 Num fake examples 3086 Num true examples 3234\n",
      "  Batch 3,200  of  44,637.    Elapsed: 0:01:56. Training loss. 0.003254952374845743 Num fake examples 3125 Num true examples 3275\n",
      "  Batch 3,240  of  44,637.    Elapsed: 0:01:58. Training loss. 0.004070781171321869 Num fake examples 3167 Num true examples 3313\n",
      "  Batch 3,280  of  44,637.    Elapsed: 0:01:59. Training loss. 0.007557708770036697 Num fake examples 3214 Num true examples 3346\n",
      "  Batch 3,320  of  44,637.    Elapsed: 0:02:01. Training loss. 0.007463994901627302 Num fake examples 3249 Num true examples 3391\n",
      "  Batch 3,360  of  44,637.    Elapsed: 0:02:02. Training loss. 0.009017175063490868 Num fake examples 3288 Num true examples 3432\n",
      "  Batch 3,400  of  44,637.    Elapsed: 0:02:03. Training loss. 0.0028987829573452473 Num fake examples 3330 Num true examples 3470\n",
      "  Batch 3,440  of  44,637.    Elapsed: 0:02:05. Training loss. 0.003725373884662986 Num fake examples 3367 Num true examples 3513\n",
      "  Batch 3,480  of  44,637.    Elapsed: 0:02:06. Training loss. 0.00314507307484746 Num fake examples 3407 Num true examples 3553\n",
      "  Batch 3,520  of  44,637.    Elapsed: 0:02:08. Training loss. 0.004631070885807276 Num fake examples 3448 Num true examples 3592\n",
      "  Batch 3,560  of  44,637.    Elapsed: 0:02:09. Training loss. 0.004411131143569946 Num fake examples 3485 Num true examples 3635\n",
      "  Batch 3,600  of  44,637.    Elapsed: 0:02:10. Training loss. 0.0032200394198298454 Num fake examples 3529 Num true examples 3671\n",
      "  Batch 3,640  of  44,637.    Elapsed: 0:02:12. Training loss. 0.004001111723482609 Num fake examples 3570 Num true examples 3710\n",
      "  Batch 3,680  of  44,637.    Elapsed: 0:02:13. Training loss. 0.0036741849035024643 Num fake examples 3604 Num true examples 3756\n",
      "  Batch 3,720  of  44,637.    Elapsed: 0:02:14. Training loss. 2.8358261585235596 Num fake examples 3644 Num true examples 3796\n",
      "  Batch 3,760  of  44,637.    Elapsed: 0:02:16. Training loss. 2.650805711746216 Num fake examples 3681 Num true examples 3839\n",
      "  Batch 3,800  of  44,637.    Elapsed: 0:02:17. Training loss. 0.0042474232614040375 Num fake examples 3721 Num true examples 3879\n",
      "  Batch 3,840  of  44,637.    Elapsed: 0:02:19. Training loss. 0.003936612978577614 Num fake examples 3761 Num true examples 3919\n",
      "  Batch 3,880  of  44,637.    Elapsed: 0:02:20. Training loss. 0.004114374052733183 Num fake examples 3807 Num true examples 3953\n",
      "  Batch 3,920  of  44,637.    Elapsed: 0:02:21. Training loss. 0.0039311605505645275 Num fake examples 3849 Num true examples 3991\n",
      "  Batch 3,960  of  44,637.    Elapsed: 0:02:23. Training loss. 0.002780110342428088 Num fake examples 3895 Num true examples 4025\n",
      "  Batch 4,000  of  44,637.    Elapsed: 0:02:24. Training loss. 0.0037933143321424723 Num fake examples 3935 Num true examples 4065\n",
      "  Batch 4,040  of  44,637.    Elapsed: 0:02:26. Training loss. 0.005664321128278971 Num fake examples 3974 Num true examples 4106\n",
      "  Batch 4,080  of  44,637.    Elapsed: 0:02:27. Training loss. 0.003831976791843772 Num fake examples 4019 Num true examples 4141\n",
      "  Batch 4,120  of  44,637.    Elapsed: 0:02:28. Training loss. 0.004152562469244003 Num fake examples 4061 Num true examples 4179\n",
      "  Batch 4,160  of  44,637.    Elapsed: 0:02:30. Training loss. 0.004522302187979221 Num fake examples 4104 Num true examples 4216\n",
      "  Batch 4,200  of  44,637.    Elapsed: 0:02:31. Training loss. 0.0032247924245893955 Num fake examples 4147 Num true examples 4253\n",
      "  Batch 4,240  of  44,637.    Elapsed: 0:02:32. Training loss. 0.002952458569779992 Num fake examples 4191 Num true examples 4289\n",
      "  Batch 4,280  of  44,637.    Elapsed: 0:02:34. Training loss. 2.901442527770996 Num fake examples 4229 Num true examples 4331\n",
      "  Batch 4,320  of  44,637.    Elapsed: 0:02:35. Training loss. 0.005212345626205206 Num fake examples 4266 Num true examples 4374\n",
      "  Batch 4,360  of  44,637.    Elapsed: 0:02:37. Training loss. 0.0029788147658109665 Num fake examples 4302 Num true examples 4418\n",
      "  Batch 4,400  of  44,637.    Elapsed: 0:02:38. Training loss. 0.003705990966409445 Num fake examples 4340 Num true examples 4460\n",
      "  Batch 4,440  of  44,637.    Elapsed: 0:02:39. Training loss. 0.004133939743041992 Num fake examples 4373 Num true examples 4507\n",
      "  Batch 4,480  of  44,637.    Elapsed: 0:02:41. Training loss. 0.004564501810818911 Num fake examples 4410 Num true examples 4550\n",
      "  Batch 4,520  of  44,637.    Elapsed: 0:02:42. Training loss. 0.006813365966081619 Num fake examples 4447 Num true examples 4593\n",
      "  Batch 4,560  of  44,637.    Elapsed: 0:02:43. Training loss. 0.006043895147740841 Num fake examples 4488 Num true examples 4632\n",
      "  Batch 4,600  of  44,637.    Elapsed: 0:02:45. Training loss. 0.004520145244896412 Num fake examples 4520 Num true examples 4680\n",
      "  Batch 4,640  of  44,637.    Elapsed: 0:02:46. Training loss. 0.004242791328579187 Num fake examples 4558 Num true examples 4722\n",
      "  Batch 4,680  of  44,637.    Elapsed: 0:02:48. Training loss. 0.002306997310370207 Num fake examples 4606 Num true examples 4754\n",
      "  Batch 4,720  of  44,637.    Elapsed: 0:02:49. Training loss. 0.0031081014312803745 Num fake examples 4650 Num true examples 4790\n",
      "  Batch 4,760  of  44,637.    Elapsed: 0:02:50. Training loss. 0.002738315612077713 Num fake examples 4692 Num true examples 4828\n",
      "  Batch 4,800  of  44,637.    Elapsed: 0:02:52. Training loss. 0.003793856594711542 Num fake examples 4724 Num true examples 4876\n",
      "  Batch 4,840  of  44,637.    Elapsed: 0:02:53. Training loss. 0.004241074435412884 Num fake examples 4771 Num true examples 4909\n",
      "  Batch 4,880  of  44,637.    Elapsed: 0:02:54. Training loss. 0.0031612596940249205 Num fake examples 4814 Num true examples 4946\n",
      "  Batch 4,920  of  44,637.    Elapsed: 0:02:56. Training loss. 0.004253244027495384 Num fake examples 4850 Num true examples 4990\n",
      "  Batch 4,960  of  44,637.    Elapsed: 0:02:57. Training loss. 0.003265273291617632 Num fake examples 4890 Num true examples 5030\n",
      "  Batch 5,000  of  44,637.    Elapsed: 0:02:58. Training loss. 0.004201705567538738 Num fake examples 4928 Num true examples 5072\n",
      "  Batch 5,040  of  44,637.    Elapsed: 0:03:00. Training loss. 0.0050995443016290665 Num fake examples 4963 Num true examples 5117\n",
      "  Batch 5,080  of  44,637.    Elapsed: 0:03:01. Training loss. 0.004805273376405239 Num fake examples 4995 Num true examples 5165\n",
      "  Batch 5,120  of  44,637.    Elapsed: 0:03:03. Training loss. 0.005903313867747784 Num fake examples 5032 Num true examples 5208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,160  of  44,637.    Elapsed: 0:03:04. Training loss. 0.005383627023547888 Num fake examples 5072 Num true examples 5248\n",
      "  Batch 5,200  of  44,637.    Elapsed: 0:03:05. Training loss. 2.4754676818847656 Num fake examples 5110 Num true examples 5290\n",
      "  Batch 5,240  of  44,637.    Elapsed: 0:03:07. Training loss. 0.007156681269407272 Num fake examples 5154 Num true examples 5326\n",
      "  Batch 5,280  of  44,637.    Elapsed: 0:03:08. Training loss. 0.005650166887789965 Num fake examples 5190 Num true examples 5370\n",
      "  Batch 5,320  of  44,637.    Elapsed: 0:03:09. Training loss. 0.0043754735961556435 Num fake examples 5225 Num true examples 5415\n",
      "  Batch 5,360  of  44,637.    Elapsed: 0:03:11. Training loss. 0.006977427750825882 Num fake examples 5264 Num true examples 5456\n",
      "  Batch 5,400  of  44,637.    Elapsed: 0:03:12. Training loss. 0.004458259791135788 Num fake examples 5308 Num true examples 5492\n",
      "  Batch 5,440  of  44,637.    Elapsed: 0:03:13. Training loss. 0.002977767726406455 Num fake examples 5348 Num true examples 5532\n",
      "  Batch 5,480  of  44,637.    Elapsed: 0:03:15. Training loss. 0.004490261897444725 Num fake examples 5379 Num true examples 5581\n",
      "  Batch 5,520  of  44,637.    Elapsed: 0:03:16. Training loss. 0.0043637752532958984 Num fake examples 5418 Num true examples 5622\n",
      "  Batch 5,560  of  44,637.    Elapsed: 0:03:17. Training loss. 0.004346204921603203 Num fake examples 5460 Num true examples 5660\n",
      "  Batch 5,600  of  44,637.    Elapsed: 0:03:19. Training loss. 0.0035102369729429483 Num fake examples 5502 Num true examples 5698\n",
      "  Batch 5,640  of  44,637.    Elapsed: 0:03:20. Training loss. 0.003811646718531847 Num fake examples 5543 Num true examples 5737\n",
      "  Batch 5,680  of  44,637.    Elapsed: 0:03:22. Training loss. 0.0031486221123486757 Num fake examples 5587 Num true examples 5773\n",
      "  Batch 5,720  of  44,637.    Elapsed: 0:03:23. Training loss. 0.003117520362138748 Num fake examples 5622 Num true examples 5818\n",
      "  Batch 5,760  of  44,637.    Elapsed: 0:03:24. Training loss. 0.003665813710540533 Num fake examples 5659 Num true examples 5861\n",
      "  Batch 5,800  of  44,637.    Elapsed: 0:03:26. Training loss. 0.0028319242410361767 Num fake examples 5698 Num true examples 5902\n",
      "  Batch 5,840  of  44,637.    Elapsed: 0:03:27. Training loss. 0.003299524076282978 Num fake examples 5741 Num true examples 5939\n",
      "  Batch 5,880  of  44,637.    Elapsed: 0:03:29. Training loss. 0.003359940368682146 Num fake examples 5785 Num true examples 5975\n",
      "  Batch 5,920  of  44,637.    Elapsed: 0:03:30. Training loss. 0.0029995273798704147 Num fake examples 5816 Num true examples 6024\n",
      "  Batch 5,960  of  44,637.    Elapsed: 0:03:31. Training loss. 0.0024461348075419664 Num fake examples 5859 Num true examples 6061\n",
      "  Batch 6,000  of  44,637.    Elapsed: 0:03:33. Training loss. 0.0037194923497736454 Num fake examples 5901 Num true examples 6099\n",
      "  Batch 6,040  of  44,637.    Elapsed: 0:03:34. Training loss. 0.0025650658644735813 Num fake examples 5946 Num true examples 6134\n",
      "  Batch 6,080  of  44,637.    Elapsed: 0:03:35. Training loss. 0.002957809716463089 Num fake examples 5986 Num true examples 6174\n",
      "  Batch 6,120  of  44,637.    Elapsed: 0:03:37. Training loss. 0.003459915518760681 Num fake examples 6026 Num true examples 6214\n",
      "  Batch 6,160  of  44,637.    Elapsed: 0:03:38. Training loss. 0.0034376983530819416 Num fake examples 6067 Num true examples 6253\n",
      "  Batch 6,200  of  44,637.    Elapsed: 0:03:40. Training loss. 0.003906883765012026 Num fake examples 6097 Num true examples 6303\n",
      "  Batch 6,240  of  44,637.    Elapsed: 0:03:41. Training loss. 0.003899254370480776 Num fake examples 6129 Num true examples 6351\n",
      "  Batch 6,280  of  44,637.    Elapsed: 0:03:42. Training loss. 0.0036134147085249424 Num fake examples 6167 Num true examples 6393\n",
      "  Batch 6,320  of  44,637.    Elapsed: 0:03:44. Training loss. 0.003318449016660452 Num fake examples 6210 Num true examples 6430\n",
      "  Batch 6,360  of  44,637.    Elapsed: 0:03:45. Training loss. 0.003072512336075306 Num fake examples 6241 Num true examples 6479\n",
      "  Batch 6,400  of  44,637.    Elapsed: 0:03:46. Training loss. 2.9285006523132324 Num fake examples 6280 Num true examples 6520\n",
      "  Batch 6,440  of  44,637.    Elapsed: 0:03:48. Training loss. 0.0030686769168823957 Num fake examples 6316 Num true examples 6564\n",
      "  Batch 6,480  of  44,637.    Elapsed: 0:03:49. Training loss. 2.852055549621582 Num fake examples 6356 Num true examples 6604\n",
      "  Batch 6,520  of  44,637.    Elapsed: 0:03:51. Training loss. 0.003878061892464757 Num fake examples 6398 Num true examples 6642\n",
      "  Batch 6,560  of  44,637.    Elapsed: 0:03:52. Training loss. 0.004000536631792784 Num fake examples 6435 Num true examples 6685\n",
      "  Batch 6,600  of  44,637.    Elapsed: 0:03:53. Training loss. 0.0031866992358118296 Num fake examples 6468 Num true examples 6732\n",
      "  Batch 6,640  of  44,637.    Elapsed: 0:03:55. Training loss. 0.0035748551599681377 Num fake examples 6502 Num true examples 6778\n",
      "  Batch 6,680  of  44,637.    Elapsed: 0:03:56. Training loss. 0.004417171701788902 Num fake examples 6537 Num true examples 6823\n",
      "  Batch 6,720  of  44,637.    Elapsed: 0:03:57. Training loss. 0.005398958455771208 Num fake examples 6571 Num true examples 6869\n",
      "  Batch 6,760  of  44,637.    Elapsed: 0:03:59. Training loss. 0.0036466671153903008 Num fake examples 6613 Num true examples 6907\n",
      "  Batch 6,800  of  44,637.    Elapsed: 0:04:00. Training loss. 0.00329814525321126 Num fake examples 6657 Num true examples 6943\n",
      "  Batch 6,840  of  44,637.    Elapsed: 0:04:02. Training loss. 0.003393994877114892 Num fake examples 6702 Num true examples 6978\n",
      "  Batch 6,880  of  44,637.    Elapsed: 0:04:03. Training loss. 0.003938081208616495 Num fake examples 6742 Num true examples 7018\n",
      "  Batch 6,920  of  44,637.    Elapsed: 0:04:04. Training loss. 0.006006473675370216 Num fake examples 6781 Num true examples 7059\n",
      "  Batch 6,960  of  44,637.    Elapsed: 0:04:06. Training loss. 0.005286856554448605 Num fake examples 6815 Num true examples 7105\n",
      "  Batch 7,000  of  44,637.    Elapsed: 0:04:07. Training loss. 0.0037729679606854916 Num fake examples 6849 Num true examples 7151\n",
      "  Batch 7,040  of  44,637.    Elapsed: 0:04:08. Training loss. 0.004993919283151627 Num fake examples 6877 Num true examples 7203\n",
      "  Batch 7,080  of  44,637.    Elapsed: 0:04:10. Training loss. 0.007869306951761246 Num fake examples 6914 Num true examples 7246\n",
      "  Batch 7,120  of  44,637.    Elapsed: 0:04:11. Training loss. 0.005673578009009361 Num fake examples 6952 Num true examples 7288\n",
      "  Batch 7,160  of  44,637.    Elapsed: 0:04:13. Training loss. 0.004160381853580475 Num fake examples 6993 Num true examples 7327\n",
      "  Batch 7,200  of  44,637.    Elapsed: 0:04:14. Training loss. 0.005310322158038616 Num fake examples 7030 Num true examples 7370\n",
      "  Batch 7,240  of  44,637.    Elapsed: 0:04:15. Training loss. 0.0038757415022701025 Num fake examples 7058 Num true examples 7422\n",
      "  Batch 7,280  of  44,637.    Elapsed: 0:04:17. Training loss. 0.0037613525055348873 Num fake examples 7106 Num true examples 7454\n",
      "  Batch 7,320  of  44,637.    Elapsed: 0:04:18. Training loss. 0.0034176812041550875 Num fake examples 7152 Num true examples 7488\n",
      "  Batch 7,360  of  44,637.    Elapsed: 0:04:19. Training loss. 0.004073216579854488 Num fake examples 7194 Num true examples 7526\n",
      "  Batch 7,400  of  44,637.    Elapsed: 0:04:21. Training loss. 0.004749109037220478 Num fake examples 7235 Num true examples 7565\n",
      "  Batch 7,440  of  44,637.    Elapsed: 0:04:22. Training loss. 0.004929021932184696 Num fake examples 7279 Num true examples 7601\n",
      "  Batch 7,480  of  44,637.    Elapsed: 0:04:24. Training loss. 0.007346233353018761 Num fake examples 7316 Num true examples 7644\n",
      "  Batch 7,520  of  44,637.    Elapsed: 0:04:25. Training loss. 0.005504862871021032 Num fake examples 7353 Num true examples 7687\n",
      "  Batch 7,560  of  44,637.    Elapsed: 0:04:26. Training loss. 0.005444645415991545 Num fake examples 7395 Num true examples 7725\n",
      "  Batch 7,600  of  44,637.    Elapsed: 0:04:28. Training loss. 0.004225384443998337 Num fake examples 7436 Num true examples 7764\n",
      "  Batch 7,640  of  44,637.    Elapsed: 0:04:29. Training loss. 0.003053078195080161 Num fake examples 7471 Num true examples 7809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 7,680  of  44,637.    Elapsed: 0:04:31. Training loss. 0.0028185760602355003 Num fake examples 7508 Num true examples 7852\n",
      "  Batch 7,720  of  44,637.    Elapsed: 0:04:32. Training loss. 0.0030230102129280567 Num fake examples 7555 Num true examples 7885\n",
      "  Batch 7,760  of  44,637.    Elapsed: 0:04:33. Training loss. 0.004068086389452219 Num fake examples 7592 Num true examples 7928\n",
      "  Batch 7,800  of  44,637.    Elapsed: 0:04:35. Training loss. 0.004043779335916042 Num fake examples 7626 Num true examples 7974\n",
      "  Batch 7,840  of  44,637.    Elapsed: 0:04:36. Training loss. 0.004418394528329372 Num fake examples 7663 Num true examples 8017\n",
      "  Batch 7,880  of  44,637.    Elapsed: 0:04:38. Training loss. 0.0036345920525491238 Num fake examples 7704 Num true examples 8056\n",
      "  Batch 7,920  of  44,637.    Elapsed: 0:04:39. Training loss. 0.0036560306325554848 Num fake examples 7740 Num true examples 8100\n",
      "  Batch 7,960  of  44,637.    Elapsed: 0:04:40. Training loss. 0.0036801202222704887 Num fake examples 7781 Num true examples 8139\n",
      "  Batch 8,000  of  44,637.    Elapsed: 0:04:42. Training loss. 0.003624681383371353 Num fake examples 7830 Num true examples 8170\n",
      "  Batch 8,040  of  44,637.    Elapsed: 0:04:43. Training loss. 0.005139529705047607 Num fake examples 7861 Num true examples 8219\n",
      "  Batch 8,080  of  44,637.    Elapsed: 0:04:45. Training loss. 0.005002160556614399 Num fake examples 7901 Num true examples 8259\n",
      "  Batch 8,120  of  44,637.    Elapsed: 0:04:47. Training loss. 0.004600755870342255 Num fake examples 7936 Num true examples 8304\n",
      "  Batch 8,160  of  44,637.    Elapsed: 0:04:48. Training loss. 0.004783917218446732 Num fake examples 7973 Num true examples 8347\n",
      "  Batch 8,200  of  44,637.    Elapsed: 0:04:50. Training loss. 0.00694772694259882 Num fake examples 8017 Num true examples 8383\n",
      "  Batch 8,240  of  44,637.    Elapsed: 0:04:51. Training loss. 0.0064949775114655495 Num fake examples 8055 Num true examples 8425\n",
      "  Batch 8,280  of  44,637.    Elapsed: 0:04:52. Training loss. 0.00614260695874691 Num fake examples 8092 Num true examples 8468\n",
      "  Batch 8,320  of  44,637.    Elapsed: 0:04:54. Training loss. 0.005053765140473843 Num fake examples 8126 Num true examples 8514\n",
      "  Batch 8,360  of  44,637.    Elapsed: 0:04:55. Training loss. 2.689774513244629 Num fake examples 8166 Num true examples 8554\n",
      "  Batch 8,400  of  44,637.    Elapsed: 0:04:57. Training loss. 0.0035125608555972576 Num fake examples 8205 Num true examples 8595\n",
      "  Batch 8,440  of  44,637.    Elapsed: 0:04:58. Training loss. 0.003380131209269166 Num fake examples 8249 Num true examples 8631\n",
      "  Batch 8,480  of  44,637.    Elapsed: 0:05:00. Training loss. 0.0028979626949876547 Num fake examples 8290 Num true examples 8670\n",
      "  Batch 8,520  of  44,637.    Elapsed: 0:05:01. Training loss. 0.004130670800805092 Num fake examples 8319 Num true examples 8721\n",
      "  Batch 8,560  of  44,637.    Elapsed: 0:05:03. Training loss. 0.0035682590678334236 Num fake examples 8354 Num true examples 8766\n",
      "  Batch 8,600  of  44,637.    Elapsed: 0:05:04. Training loss. 0.0035572736524045467 Num fake examples 8392 Num true examples 8808\n",
      "  Batch 8,640  of  44,637.    Elapsed: 0:05:05. Training loss. 0.003481214866042137 Num fake examples 8431 Num true examples 8849\n",
      "  Batch 8,680  of  44,637.    Elapsed: 0:05:07. Training loss. 0.0034379474818706512 Num fake examples 8472 Num true examples 8888\n",
      "  Batch 8,720  of  44,637.    Elapsed: 0:05:08. Training loss. 0.003506738692522049 Num fake examples 8516 Num true examples 8924\n",
      "  Batch 8,760  of  44,637.    Elapsed: 0:05:10. Training loss. 0.00429402245208621 Num fake examples 8556 Num true examples 8964\n",
      "  Batch 8,800  of  44,637.    Elapsed: 0:05:11. Training loss. 0.005465702153742313 Num fake examples 8596 Num true examples 9004\n",
      "  Batch 8,840  of  44,637.    Elapsed: 0:05:13. Training loss. 0.004309867508709431 Num fake examples 8641 Num true examples 9039\n",
      "  Batch 8,880  of  44,637.    Elapsed: 0:05:14. Training loss. 0.00521722249686718 Num fake examples 8677 Num true examples 9083\n",
      "  Batch 8,920  of  44,637.    Elapsed: 0:05:16. Training loss. 0.005331020802259445 Num fake examples 8722 Num true examples 9118\n",
      "  Batch 8,960  of  44,637.    Elapsed: 0:05:17. Training loss. 0.004602999426424503 Num fake examples 8762 Num true examples 9158\n",
      "  Batch 9,000  of  44,637.    Elapsed: 0:05:19. Training loss. 0.003801008453592658 Num fake examples 8794 Num true examples 9206\n",
      "  Batch 9,040  of  44,637.    Elapsed: 0:05:20. Training loss. 0.004047054331749678 Num fake examples 8825 Num true examples 9255\n",
      "  Batch 9,080  of  44,637.    Elapsed: 0:05:21. Training loss. 0.003828571178019047 Num fake examples 8865 Num true examples 9295\n",
      "  Batch 9,120  of  44,637.    Elapsed: 0:05:23. Training loss. 0.004409958142787218 Num fake examples 8906 Num true examples 9334\n",
      "  Batch 9,160  of  44,637.    Elapsed: 0:05:24. Training loss. 0.004503441508859396 Num fake examples 8944 Num true examples 9376\n",
      "  Batch 9,200  of  44,637.    Elapsed: 0:05:26. Training loss. 0.004633793607354164 Num fake examples 8990 Num true examples 9410\n",
      "  Batch 9,240  of  44,637.    Elapsed: 0:05:27. Training loss. 0.004022084176540375 Num fake examples 9027 Num true examples 9453\n",
      "  Batch 9,280  of  44,637.    Elapsed: 0:05:29. Training loss. 0.0038540575187653303 Num fake examples 9065 Num true examples 9495\n",
      "  Batch 9,320  of  44,637.    Elapsed: 0:05:30. Training loss. 2.856714963912964 Num fake examples 9100 Num true examples 9540\n",
      "  Batch 9,360  of  44,637.    Elapsed: 0:05:32. Training loss. 2.9576199054718018 Num fake examples 9132 Num true examples 9588\n",
      "  Batch 9,400  of  44,637.    Elapsed: 0:05:33. Training loss. 0.005035717040300369 Num fake examples 9172 Num true examples 9628\n",
      "  Batch 9,440  of  44,637.    Elapsed: 0:05:35. Training loss. 0.00400449987500906 Num fake examples 9213 Num true examples 9667\n",
      "  Batch 9,480  of  44,637.    Elapsed: 0:05:36. Training loss. 0.004412217531353235 Num fake examples 9250 Num true examples 9710\n",
      "  Batch 9,520  of  44,637.    Elapsed: 0:05:37. Training loss. 0.004769923165440559 Num fake examples 9282 Num true examples 9758\n",
      "  Batch 9,560  of  44,637.    Elapsed: 0:05:39. Training loss. 0.004414691124111414 Num fake examples 9319 Num true examples 9801\n",
      "  Batch 9,600  of  44,637.    Elapsed: 0:05:41. Training loss. 0.006073339842259884 Num fake examples 9353 Num true examples 9847\n",
      "  Batch 9,640  of  44,637.    Elapsed: 0:05:42. Training loss. 0.005823838524520397 Num fake examples 9396 Num true examples 9884\n",
      "  Batch 9,680  of  44,637.    Elapsed: 0:05:44. Training loss. 0.004362305626273155 Num fake examples 9434 Num true examples 9926\n",
      "  Batch 9,720  of  44,637.    Elapsed: 0:05:45. Training loss. 0.0040181297808885574 Num fake examples 9474 Num true examples 9966\n",
      "  Batch 9,760  of  44,637.    Elapsed: 0:05:47. Training loss. 0.0034888330847024918 Num fake examples 9514 Num true examples 10006\n",
      "  Batch 9,800  of  44,637.    Elapsed: 0:05:48. Training loss. 0.003193938871845603 Num fake examples 9553 Num true examples 10047\n",
      "  Batch 9,840  of  44,637.    Elapsed: 0:05:50. Training loss. 2.8580853939056396 Num fake examples 9596 Num true examples 10084\n",
      "  Batch 9,880  of  44,637.    Elapsed: 0:05:51. Training loss. 0.0042924764566123486 Num fake examples 9633 Num true examples 10127\n",
      "  Batch 9,920  of  44,637.    Elapsed: 0:05:53. Training loss. 0.00490891607478261 Num fake examples 9668 Num true examples 10172\n",
      "  Batch 9,960  of  44,637.    Elapsed: 0:05:54. Training loss. 0.004531889222562313 Num fake examples 9712 Num true examples 10208\n",
      "  Batch 10,000  of  44,637.    Elapsed: 0:05:56. Training loss. 0.003938152454793453 Num fake examples 9755 Num true examples 10245\n",
      "  Batch 10,040  of  44,637.    Elapsed: 0:05:57. Training loss. 0.005466597154736519 Num fake examples 9794 Num true examples 10286\n",
      "  Batch 10,080  of  44,637.    Elapsed: 0:05:59. Training loss. 0.004880258347839117 Num fake examples 9838 Num true examples 10322\n",
      "  Batch 10,120  of  44,637.    Elapsed: 0:06:00. Training loss. 2.708500385284424 Num fake examples 9873 Num true examples 10367\n",
      "  Batch 10,160  of  44,637.    Elapsed: 0:06:02. Training loss. 0.0060976301319897175 Num fake examples 9917 Num true examples 10403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10,200  of  44,637.    Elapsed: 0:06:03. Training loss. 0.004991017747670412 Num fake examples 9967 Num true examples 10433\n",
      "  Batch 10,240  of  44,637.    Elapsed: 0:06:05. Training loss. 0.005397241096943617 Num fake examples 10001 Num true examples 10479\n",
      "  Batch 10,280  of  44,637.    Elapsed: 0:06:06. Training loss. 2.678415536880493 Num fake examples 10045 Num true examples 10515\n",
      "  Batch 10,320  of  44,637.    Elapsed: 0:06:08. Training loss. 0.003925509750843048 Num fake examples 10078 Num true examples 10562\n",
      "  Batch 10,360  of  44,637.    Elapsed: 0:06:09. Training loss. 0.0029263191390782595 Num fake examples 10114 Num true examples 10606\n",
      "  Batch 10,400  of  44,637.    Elapsed: 0:06:10. Training loss. 0.0028263796120882034 Num fake examples 10150 Num true examples 10650\n",
      "  Batch 10,440  of  44,637.    Elapsed: 0:06:12. Training loss. 2.8986425399780273 Num fake examples 10189 Num true examples 10691\n",
      "  Batch 10,480  of  44,637.    Elapsed: 0:06:13. Training loss. 0.0036711259745061398 Num fake examples 10238 Num true examples 10722\n",
      "  Batch 10,520  of  44,637.    Elapsed: 0:06:14. Training loss. 0.004321441054344177 Num fake examples 10270 Num true examples 10770\n",
      "  Batch 10,560  of  44,637.    Elapsed: 0:06:16. Training loss. 0.00448041595518589 Num fake examples 10309 Num true examples 10811\n",
      "  Batch 10,600  of  44,637.    Elapsed: 0:06:17. Training loss. 0.0063814446330070496 Num fake examples 10344 Num true examples 10856\n",
      "  Batch 10,640  of  44,637.    Elapsed: 0:06:18. Training loss. 0.006530713755637407 Num fake examples 10385 Num true examples 10895\n",
      "  Batch 10,680  of  44,637.    Elapsed: 0:06:20. Training loss. 0.00553111732006073 Num fake examples 10431 Num true examples 10929\n",
      "  Batch 10,720  of  44,637.    Elapsed: 0:06:21. Training loss. 0.0049028899520635605 Num fake examples 10469 Num true examples 10971\n",
      "  Batch 10,760  of  44,637.    Elapsed: 0:06:23. Training loss. 2.598139524459839 Num fake examples 10512 Num true examples 11008\n",
      "  Batch 10,800  of  44,637.    Elapsed: 0:06:24. Training loss. 0.00482591986656189 Num fake examples 10548 Num true examples 11052\n",
      "  Batch 10,840  of  44,637.    Elapsed: 0:06:25. Training loss. 0.004381031729280949 Num fake examples 10585 Num true examples 11095\n",
      "  Batch 10,880  of  44,637.    Elapsed: 0:06:27. Training loss. 0.0027062061708420515 Num fake examples 10627 Num true examples 11133\n",
      "  Batch 10,920  of  44,637.    Elapsed: 0:06:28. Training loss. 0.004446754232048988 Num fake examples 10664 Num true examples 11176\n",
      "  Batch 10,960  of  44,637.    Elapsed: 0:06:29. Training loss. 0.004480150528252125 Num fake examples 10699 Num true examples 11221\n",
      "  Batch 11,000  of  44,637.    Elapsed: 0:06:31. Training loss. 0.004287104122340679 Num fake examples 10742 Num true examples 11258\n",
      "  Batch 11,040  of  44,637.    Elapsed: 0:06:32. Training loss. 0.004078409634530544 Num fake examples 10777 Num true examples 11303\n",
      "  Batch 11,080  of  44,637.    Elapsed: 0:06:34. Training loss. 0.0036733667366206646 Num fake examples 10813 Num true examples 11347\n",
      "  Batch 11,120  of  44,637.    Elapsed: 0:06:35. Training loss. 0.003372355829924345 Num fake examples 10852 Num true examples 11388\n",
      "  Batch 11,160  of  44,637.    Elapsed: 0:06:36. Training loss. 0.0028251304756850004 Num fake examples 10885 Num true examples 11435\n",
      "  Batch 11,200  of  44,637.    Elapsed: 0:06:38. Training loss. 0.0023505156859755516 Num fake examples 10934 Num true examples 11466\n",
      "  Batch 11,240  of  44,637.    Elapsed: 0:06:39. Training loss. 0.004339487291872501 Num fake examples 10962 Num true examples 11518\n",
      "  Batch 11,280  of  44,637.    Elapsed: 0:06:40. Training loss. 0.00400165282189846 Num fake examples 11003 Num true examples 11557\n",
      "  Batch 11,320  of  44,637.    Elapsed: 0:06:42. Training loss. 0.002909128786996007 Num fake examples 11043 Num true examples 11597\n",
      "  Batch 11,360  of  44,637.    Elapsed: 0:06:43. Training loss. 0.00353270024061203 Num fake examples 11082 Num true examples 11638\n",
      "  Batch 11,400  of  44,637.    Elapsed: 0:06:44. Training loss. 0.003026350401341915 Num fake examples 11114 Num true examples 11686\n",
      "  Batch 11,440  of  44,637.    Elapsed: 0:06:46. Training loss. 0.0029738489538431168 Num fake examples 11158 Num true examples 11722\n",
      "  Batch 11,480  of  44,637.    Elapsed: 0:06:47. Training loss. 0.0036154044792056084 Num fake examples 11198 Num true examples 11762\n",
      "  Batch 11,520  of  44,637.    Elapsed: 0:06:48. Training loss. 0.003041465999558568 Num fake examples 11231 Num true examples 11809\n",
      "  Batch 11,560  of  44,637.    Elapsed: 0:06:50. Training loss. 0.002697349525988102 Num fake examples 11274 Num true examples 11846\n",
      "  Batch 11,600  of  44,637.    Elapsed: 0:06:51. Training loss. 0.002416685689240694 Num fake examples 11311 Num true examples 11889\n",
      "  Batch 11,640  of  44,637.    Elapsed: 0:06:53. Training loss. 0.003016294678673148 Num fake examples 11351 Num true examples 11929\n",
      "  Batch 11,680  of  44,637.    Elapsed: 0:06:54. Training loss. 0.003294967580586672 Num fake examples 11389 Num true examples 11971\n",
      "  Batch 11,720  of  44,637.    Elapsed: 0:06:55. Training loss. 0.0029124217107892036 Num fake examples 11433 Num true examples 12007\n",
      "  Batch 11,760  of  44,637.    Elapsed: 0:06:57. Training loss. 0.0038335993885993958 Num fake examples 11465 Num true examples 12055\n",
      "  Batch 11,800  of  44,637.    Elapsed: 0:06:58. Training loss. 0.004331332631409168 Num fake examples 11502 Num true examples 12098\n",
      "  Batch 11,840  of  44,637.    Elapsed: 0:06:59. Training loss. 0.0037059520836919546 Num fake examples 11544 Num true examples 12136\n",
      "  Batch 11,880  of  44,637.    Elapsed: 0:07:01. Training loss. 0.01089996937662363 Num fake examples 11577 Num true examples 12183\n",
      "  Batch 11,920  of  44,637.    Elapsed: 0:07:02. Training loss. 0.005956903100013733 Num fake examples 11622 Num true examples 12218\n",
      "  Batch 11,960  of  44,637.    Elapsed: 0:07:03. Training loss. 0.00533600989729166 Num fake examples 11662 Num true examples 12258\n",
      "  Batch 12,000  of  44,637.    Elapsed: 0:07:05. Training loss. 0.004673751071095467 Num fake examples 11697 Num true examples 12303\n",
      "  Batch 12,040  of  44,637.    Elapsed: 0:07:06. Training loss. 0.0037856916896998882 Num fake examples 11730 Num true examples 12350\n",
      "  Batch 12,080  of  44,637.    Elapsed: 0:07:07. Training loss. 0.004897428210824728 Num fake examples 11762 Num true examples 12398\n",
      "  Batch 12,120  of  44,637.    Elapsed: 0:07:09. Training loss. 0.0039631156250834465 Num fake examples 11799 Num true examples 12441\n",
      "  Batch 12,160  of  44,637.    Elapsed: 0:07:10. Training loss. 0.004537899047136307 Num fake examples 11843 Num true examples 12477\n",
      "  Batch 12,200  of  44,637.    Elapsed: 0:07:12. Training loss. 0.004262629430741072 Num fake examples 11880 Num true examples 12520\n",
      "  Batch 12,240  of  44,637.    Elapsed: 0:07:13. Training loss. 0.004081557039171457 Num fake examples 11922 Num true examples 12558\n",
      "  Batch 12,280  of  44,637.    Elapsed: 0:07:14. Training loss. 0.004365043248981237 Num fake examples 11958 Num true examples 12602\n",
      "  Batch 12,320  of  44,637.    Elapsed: 0:07:16. Training loss. 5.535607814788818 Num fake examples 11997 Num true examples 12643\n",
      "  Batch 12,360  of  44,637.    Elapsed: 0:07:17. Training loss. 2.893580675125122 Num fake examples 12038 Num true examples 12682\n",
      "  Batch 12,400  of  44,637.    Elapsed: 0:07:18. Training loss. 2.726015090942383 Num fake examples 12070 Num true examples 12730\n",
      "  Batch 12,440  of  44,637.    Elapsed: 0:07:20. Training loss. 0.0029690649826079607 Num fake examples 12109 Num true examples 12771\n",
      "  Batch 12,480  of  44,637.    Elapsed: 0:07:21. Training loss. 0.005018248222768307 Num fake examples 12152 Num true examples 12808\n",
      "  Batch 12,520  of  44,637.    Elapsed: 0:07:22. Training loss. 0.006043237168341875 Num fake examples 12192 Num true examples 12848\n",
      "  Batch 12,560  of  44,637.    Elapsed: 0:07:24. Training loss. 0.009295977652072906 Num fake examples 12230 Num true examples 12890\n",
      "  Batch 12,600  of  44,637.    Elapsed: 0:07:25. Training loss. 0.006468139588832855 Num fake examples 12264 Num true examples 12936\n",
      "  Batch 12,640  of  44,637.    Elapsed: 0:07:27. Training loss. 0.005914123263210058 Num fake examples 12301 Num true examples 12979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 12,680  of  44,637.    Elapsed: 0:07:28. Training loss. 0.005264408886432648 Num fake examples 12337 Num true examples 13023\n",
      "  Batch 12,720  of  44,637.    Elapsed: 0:07:29. Training loss. 0.004317945800721645 Num fake examples 12384 Num true examples 13056\n",
      "  Batch 12,760  of  44,637.    Elapsed: 0:07:31. Training loss. 0.002964756917208433 Num fake examples 12425 Num true examples 13095\n",
      "  Batch 12,800  of  44,637.    Elapsed: 0:07:32. Training loss. 0.0032511891331523657 Num fake examples 12464 Num true examples 13136\n",
      "  Batch 12,840  of  44,637.    Elapsed: 0:07:33. Training loss. 0.003173163626343012 Num fake examples 12503 Num true examples 13177\n",
      "  Batch 12,880  of  44,637.    Elapsed: 0:07:35. Training loss. 0.0033897575922310352 Num fake examples 12535 Num true examples 13225\n",
      "  Batch 12,920  of  44,637.    Elapsed: 0:07:36. Training loss. 0.0025975164026021957 Num fake examples 12568 Num true examples 13272\n",
      "  Batch 12,960  of  44,637.    Elapsed: 0:07:37. Training loss. 0.0028766952455043793 Num fake examples 12602 Num true examples 13318\n",
      "  Batch 13,000  of  44,637.    Elapsed: 0:07:39. Training loss. 0.0025349946226924658 Num fake examples 12646 Num true examples 13354\n",
      "  Batch 13,040  of  44,637.    Elapsed: 0:07:40. Training loss. 0.00267159310169518 Num fake examples 12691 Num true examples 13389\n",
      "  Batch 13,080  of  44,637.    Elapsed: 0:07:41. Training loss. 0.0030312337912619114 Num fake examples 12735 Num true examples 13425\n",
      "  Batch 13,120  of  44,637.    Elapsed: 0:07:43. Training loss. 0.0025795702822506428 Num fake examples 12771 Num true examples 13469\n",
      "  Batch 13,160  of  44,637.    Elapsed: 0:07:44. Training loss. 0.0033296537585556507 Num fake examples 12815 Num true examples 13505\n",
      "  Batch 13,200  of  44,637.    Elapsed: 0:07:46. Training loss. 0.0033195572905242443 Num fake examples 12854 Num true examples 13546\n",
      "  Batch 13,240  of  44,637.    Elapsed: 0:07:47. Training loss. 0.002599182538688183 Num fake examples 12893 Num true examples 13587\n",
      "  Batch 13,280  of  44,637.    Elapsed: 0:07:48. Training loss. 0.00270291231572628 Num fake examples 12934 Num true examples 13626\n",
      "  Batch 13,320  of  44,637.    Elapsed: 0:07:50. Training loss. 0.003644061740487814 Num fake examples 12972 Num true examples 13668\n",
      "  Batch 13,360  of  44,637.    Elapsed: 0:07:51. Training loss. 0.0031175727490335703 Num fake examples 13009 Num true examples 13711\n",
      "  Batch 13,400  of  44,637.    Elapsed: 0:07:52. Training loss. 0.0028145452961325645 Num fake examples 13044 Num true examples 13756\n",
      "  Batch 13,440  of  44,637.    Elapsed: 0:07:54. Training loss. 0.002810328733175993 Num fake examples 13080 Num true examples 13800\n",
      "  Batch 13,480  of  44,637.    Elapsed: 0:07:55. Training loss. 0.0023589455522596836 Num fake examples 13118 Num true examples 13842\n",
      "  Batch 13,520  of  44,637.    Elapsed: 0:07:57. Training loss. 0.002216416411101818 Num fake examples 13159 Num true examples 13881\n",
      "  Batch 13,560  of  44,637.    Elapsed: 0:07:58. Training loss. 0.0022777479607611895 Num fake examples 13210 Num true examples 13910\n",
      "  Batch 13,600  of  44,637.    Elapsed: 0:07:59. Training loss. 0.0019270502962172031 Num fake examples 13246 Num true examples 13954\n",
      "  Batch 13,640  of  44,637.    Elapsed: 0:08:01. Training loss. 0.0021539395675063133 Num fake examples 13286 Num true examples 13994\n",
      "  Batch 13,680  of  44,637.    Elapsed: 0:08:02. Training loss. 0.0026499563828110695 Num fake examples 13327 Num true examples 14033\n",
      "  Batch 13,720  of  44,637.    Elapsed: 0:08:03. Training loss. 0.003437552135437727 Num fake examples 13360 Num true examples 14080\n",
      "  Batch 13,760  of  44,637.    Elapsed: 0:08:05. Training loss. 2.996828317642212 Num fake examples 13393 Num true examples 14127\n",
      "  Batch 13,800  of  44,637.    Elapsed: 0:08:06. Training loss. 0.003892360720783472 Num fake examples 13432 Num true examples 14168\n",
      "  Batch 13,840  of  44,637.    Elapsed: 0:08:07. Training loss. 2.8219220638275146 Num fake examples 13477 Num true examples 14203\n",
      "  Batch 13,880  of  44,637.    Elapsed: 0:08:09. Training loss. 0.004124890547245741 Num fake examples 13517 Num true examples 14243\n",
      "  Batch 13,920  of  44,637.    Elapsed: 0:08:10. Training loss. 0.00427410751581192 Num fake examples 13558 Num true examples 14282\n",
      "  Batch 13,960  of  44,637.    Elapsed: 0:08:11. Training loss. 0.004095024429261684 Num fake examples 13596 Num true examples 14324\n",
      "  Batch 14,000  of  44,637.    Elapsed: 0:08:13. Training loss. 0.003256869502365589 Num fake examples 13635 Num true examples 14365\n",
      "  Batch 14,040  of  44,637.    Elapsed: 0:08:14. Training loss. 0.003335355082526803 Num fake examples 13670 Num true examples 14410\n",
      "  Batch 14,080  of  44,637.    Elapsed: 0:08:16. Training loss. 0.002793454099446535 Num fake examples 13711 Num true examples 14449\n",
      "  Batch 14,120  of  44,637.    Elapsed: 0:08:17. Training loss. 0.002190590836107731 Num fake examples 13753 Num true examples 14487\n",
      "  Batch 14,160  of  44,637.    Elapsed: 0:08:18. Training loss. 0.0030124038457870483 Num fake examples 13787 Num true examples 14533\n",
      "  Batch 14,200  of  44,637.    Elapsed: 0:08:20. Training loss. 0.00283022946678102 Num fake examples 13826 Num true examples 14574\n",
      "  Batch 14,240  of  44,637.    Elapsed: 0:08:21. Training loss. 0.0036931587383151054 Num fake examples 13869 Num true examples 14611\n",
      "  Batch 14,280  of  44,637.    Elapsed: 0:08:22. Training loss. 0.00402885302901268 Num fake examples 13914 Num true examples 14646\n",
      "  Batch 14,320  of  44,637.    Elapsed: 0:08:24. Training loss. 0.00368861248716712 Num fake examples 13951 Num true examples 14689\n",
      "  Batch 14,360  of  44,637.    Elapsed: 0:08:25. Training loss. 0.0037791822105646133 Num fake examples 13983 Num true examples 14737\n",
      "  Batch 14,400  of  44,637.    Elapsed: 0:08:26. Training loss. 0.003125836607068777 Num fake examples 14017 Num true examples 14783\n",
      "  Batch 14,440  of  44,637.    Elapsed: 0:08:28. Training loss. 0.0043947454541921616 Num fake examples 14056 Num true examples 14824\n",
      "  Batch 14,480  of  44,637.    Elapsed: 0:08:29. Training loss. 0.004935445263981819 Num fake examples 14089 Num true examples 14871\n",
      "  Batch 14,520  of  44,637.    Elapsed: 0:08:30. Training loss. 0.004450025502592325 Num fake examples 14134 Num true examples 14906\n",
      "  Batch 14,560  of  44,637.    Elapsed: 0:08:32. Training loss. 0.0033069071359932423 Num fake examples 14176 Num true examples 14944\n",
      "  Batch 14,600  of  44,637.    Elapsed: 0:08:33. Training loss. 0.004572225268930197 Num fake examples 14213 Num true examples 14987\n",
      "  Batch 14,640  of  44,637.    Elapsed: 0:08:35. Training loss. 0.0037026270292699337 Num fake examples 14252 Num true examples 15028\n",
      "  Batch 14,680  of  44,637.    Elapsed: 0:08:36. Training loss. 0.004979968070983887 Num fake examples 14283 Num true examples 15077\n",
      "  Batch 14,720  of  44,637.    Elapsed: 0:08:37. Training loss. 2.6442906856536865 Num fake examples 14320 Num true examples 15120\n",
      "  Batch 14,760  of  44,637.    Elapsed: 0:08:39. Training loss. 0.004398615099489689 Num fake examples 14354 Num true examples 15166\n",
      "  Batch 14,800  of  44,637.    Elapsed: 0:08:40. Training loss. 0.003993786871433258 Num fake examples 14390 Num true examples 15210\n",
      "  Batch 14,840  of  44,637.    Elapsed: 0:08:41. Training loss. 0.003567471168935299 Num fake examples 14422 Num true examples 15258\n",
      "  Batch 14,880  of  44,637.    Elapsed: 0:08:43. Training loss. 0.002527199685573578 Num fake examples 14473 Num true examples 15287\n",
      "  Batch 14,920  of  44,637.    Elapsed: 0:08:44. Training loss. 0.003115232801064849 Num fake examples 14508 Num true examples 15332\n",
      "  Batch 14,960  of  44,637.    Elapsed: 0:08:45. Training loss. 2.872490644454956 Num fake examples 14556 Num true examples 15364\n",
      "  Batch 15,000  of  44,637.    Elapsed: 0:08:47. Training loss. 0.0038119000382721424 Num fake examples 14599 Num true examples 15401\n",
      "  Batch 15,040  of  44,637.    Elapsed: 0:08:48. Training loss. 0.004113771952688694 Num fake examples 14631 Num true examples 15449\n",
      "  Batch 15,080  of  44,637.    Elapsed: 0:08:50. Training loss. 0.0033751572482287884 Num fake examples 14660 Num true examples 15500\n",
      "  Batch 15,120  of  44,637.    Elapsed: 0:08:51. Training loss. 0.0030964971520006657 Num fake examples 14703 Num true examples 15537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15,160  of  44,637.    Elapsed: 0:08:52. Training loss. 0.003645986318588257 Num fake examples 14743 Num true examples 15577\n",
      "  Batch 15,200  of  44,637.    Elapsed: 0:08:54. Training loss. 0.002570203971117735 Num fake examples 14776 Num true examples 15624\n",
      "  Batch 15,240  of  44,637.    Elapsed: 0:08:55. Training loss. 0.002748622326180339 Num fake examples 14818 Num true examples 15662\n",
      "  Batch 15,280  of  44,637.    Elapsed: 0:08:56. Training loss. 0.00350895500741899 Num fake examples 14856 Num true examples 15704\n",
      "  Batch 15,320  of  44,637.    Elapsed: 0:08:58. Training loss. 0.0039450861513614655 Num fake examples 14899 Num true examples 15741\n",
      "  Batch 15,360  of  44,637.    Elapsed: 0:08:59. Training loss. 0.003703142050653696 Num fake examples 14933 Num true examples 15787\n",
      "  Batch 15,400  of  44,637.    Elapsed: 0:09:00. Training loss. 0.0038548624143004417 Num fake examples 14968 Num true examples 15832\n",
      "  Batch 15,440  of  44,637.    Elapsed: 0:09:02. Training loss. 0.004789556376636028 Num fake examples 15003 Num true examples 15877\n",
      "  Batch 15,480  of  44,637.    Elapsed: 0:09:03. Training loss. 0.00448912288993597 Num fake examples 15051 Num true examples 15909\n",
      "  Batch 15,520  of  44,637.    Elapsed: 0:09:04. Training loss. 0.003915142733603716 Num fake examples 15087 Num true examples 15953\n",
      "  Batch 15,560  of  44,637.    Elapsed: 0:09:06. Training loss. 2.7434403896331787 Num fake examples 15126 Num true examples 15994\n",
      "  Batch 15,600  of  44,637.    Elapsed: 0:09:07. Training loss. 0.003787896130234003 Num fake examples 15162 Num true examples 16038\n",
      "  Batch 15,640  of  44,637.    Elapsed: 0:09:09. Training loss. 0.003897137241438031 Num fake examples 15201 Num true examples 16079\n",
      "  Batch 15,680  of  44,637.    Elapsed: 0:09:10. Training loss. 0.004303463734686375 Num fake examples 15238 Num true examples 16122\n",
      "  Batch 15,720  of  44,637.    Elapsed: 0:09:11. Training loss. 0.003877808805555105 Num fake examples 15274 Num true examples 16166\n",
      "  Batch 15,760  of  44,637.    Elapsed: 0:09:13. Training loss. 0.0036280162166804075 Num fake examples 15308 Num true examples 16212\n",
      "  Batch 15,800  of  44,637.    Elapsed: 0:09:14. Training loss. 0.0033909264020621777 Num fake examples 15350 Num true examples 16250\n",
      "  Batch 15,840  of  44,637.    Elapsed: 0:09:15. Training loss. 0.00364927900955081 Num fake examples 15383 Num true examples 16297\n",
      "  Batch 15,880  of  44,637.    Elapsed: 0:09:17. Training loss. 2.6883113384246826 Num fake examples 15424 Num true examples 16336\n",
      "  Batch 15,920  of  44,637.    Elapsed: 0:09:18. Training loss. 0.00514588225632906 Num fake examples 15468 Num true examples 16372\n",
      "  Batch 15,960  of  44,637.    Elapsed: 0:09:19. Training loss. 0.003906544763594866 Num fake examples 15500 Num true examples 16420\n",
      "  Batch 16,000  of  44,637.    Elapsed: 0:09:21. Training loss. 0.004873105324804783 Num fake examples 15538 Num true examples 16462\n",
      "  Batch 16,040  of  44,637.    Elapsed: 0:09:22. Training loss. 0.004196480847895145 Num fake examples 15583 Num true examples 16497\n",
      "  Batch 16,080  of  44,637.    Elapsed: 0:09:23. Training loss. 0.014787559397518635 Num fake examples 15625 Num true examples 16535\n",
      "  Batch 16,120  of  44,637.    Elapsed: 0:09:25. Training loss. 0.0069146789610385895 Num fake examples 15666 Num true examples 16574\n",
      "  Batch 16,160  of  44,637.    Elapsed: 0:09:26. Training loss. 0.004423190839588642 Num fake examples 15709 Num true examples 16611\n",
      "  Batch 16,200  of  44,637.    Elapsed: 0:09:28. Training loss. 0.005634729750454426 Num fake examples 15751 Num true examples 16649\n",
      "  Batch 16,240  of  44,637.    Elapsed: 0:09:29. Training loss. 2.5571749210357666 Num fake examples 15791 Num true examples 16689\n",
      "  Batch 16,280  of  44,637.    Elapsed: 0:09:30. Training loss. 0.004887170158326626 Num fake examples 15832 Num true examples 16728\n",
      "  Batch 16,320  of  44,637.    Elapsed: 0:09:32. Training loss. 0.006116621661931276 Num fake examples 15874 Num true examples 16766\n",
      "  Batch 16,360  of  44,637.    Elapsed: 0:09:33. Training loss. 0.0033766343258321285 Num fake examples 15912 Num true examples 16808\n",
      "  Batch 16,400  of  44,637.    Elapsed: 0:09:34. Training loss. 0.0044308858923614025 Num fake examples 15954 Num true examples 16846\n",
      "  Batch 16,440  of  44,637.    Elapsed: 0:09:36. Training loss. 0.004263150971382856 Num fake examples 15992 Num true examples 16888\n",
      "  Batch 16,480  of  44,637.    Elapsed: 0:09:37. Training loss. 0.004154866095632315 Num fake examples 16037 Num true examples 16923\n",
      "  Batch 16,520  of  44,637.    Elapsed: 0:09:38. Training loss. 0.00551303057000041 Num fake examples 16076 Num true examples 16964\n",
      "  Batch 16,560  of  44,637.    Elapsed: 0:09:40. Training loss. 0.004492341075092554 Num fake examples 16113 Num true examples 17007\n",
      "  Batch 16,600  of  44,637.    Elapsed: 0:09:41. Training loss. 0.00506930984556675 Num fake examples 16156 Num true examples 17044\n",
      "  Batch 16,640  of  44,637.    Elapsed: 0:09:43. Training loss. 2.617438554763794 Num fake examples 16193 Num true examples 17087\n",
      "  Batch 16,680  of  44,637.    Elapsed: 0:09:44. Training loss. 0.004213461186736822 Num fake examples 16229 Num true examples 17131\n",
      "  Batch 16,720  of  44,637.    Elapsed: 0:09:45. Training loss. 0.003810593392699957 Num fake examples 16261 Num true examples 17179\n",
      "  Batch 16,760  of  44,637.    Elapsed: 0:09:47. Training loss. 0.004623165354132652 Num fake examples 16301 Num true examples 17219\n",
      "  Batch 16,800  of  44,637.    Elapsed: 0:09:48. Training loss. 2.541767120361328 Num fake examples 16329 Num true examples 17271\n",
      "  Batch 16,840  of  44,637.    Elapsed: 0:09:49. Training loss. 0.005009249784052372 Num fake examples 16372 Num true examples 17308\n",
      "  Batch 16,880  of  44,637.    Elapsed: 0:09:51. Training loss. 0.004557271488010883 Num fake examples 16412 Num true examples 17348\n",
      "  Batch 16,920  of  44,637.    Elapsed: 0:09:52. Training loss. 0.004808660596609116 Num fake examples 16449 Num true examples 17391\n",
      "  Batch 16,960  of  44,637.    Elapsed: 0:09:54. Training loss. 0.0035241139121353626 Num fake examples 16493 Num true examples 17427\n",
      "  Batch 17,000  of  44,637.    Elapsed: 0:09:55. Training loss. 0.004497563932090998 Num fake examples 16537 Num true examples 17463\n",
      "  Batch 17,040  of  44,637.    Elapsed: 0:09:56. Training loss. 0.004937449935823679 Num fake examples 16580 Num true examples 17500\n",
      "  Batch 17,080  of  44,637.    Elapsed: 0:09:58. Training loss. 0.004077021032571793 Num fake examples 16621 Num true examples 17539\n",
      "  Batch 17,120  of  44,637.    Elapsed: 0:09:59. Training loss. 0.004111208952963352 Num fake examples 16668 Num true examples 17572\n",
      "  Batch 17,160  of  44,637.    Elapsed: 0:10:00. Training loss. 0.0045433202758431435 Num fake examples 16709 Num true examples 17611\n",
      "  Batch 17,200  of  44,637.    Elapsed: 0:10:02. Training loss. 0.0038756299763917923 Num fake examples 16751 Num true examples 17649\n",
      "  Batch 17,240  of  44,637.    Elapsed: 0:10:03. Training loss. 0.00368449161760509 Num fake examples 16787 Num true examples 17693\n",
      "  Batch 17,280  of  44,637.    Elapsed: 0:10:04. Training loss. 0.004089601803570986 Num fake examples 16823 Num true examples 17737\n",
      "  Batch 17,320  of  44,637.    Elapsed: 0:10:06. Training loss. 0.0042557502165436745 Num fake examples 16867 Num true examples 17773\n",
      "  Batch 17,360  of  44,637.    Elapsed: 0:10:07. Training loss. 0.004343243315815926 Num fake examples 16907 Num true examples 17813\n",
      "  Batch 17,400  of  44,637.    Elapsed: 0:10:09. Training loss. 0.004634136334061623 Num fake examples 16950 Num true examples 17850\n",
      "  Batch 17,440  of  44,637.    Elapsed: 0:10:10. Training loss. 0.006596503779292107 Num fake examples 16990 Num true examples 17890\n",
      "  Batch 17,480  of  44,637.    Elapsed: 0:10:11. Training loss. 0.004535229876637459 Num fake examples 17027 Num true examples 17933\n",
      "  Batch 17,520  of  44,637.    Elapsed: 0:10:13. Training loss. 0.006127519067376852 Num fake examples 17057 Num true examples 17983\n",
      "  Batch 17,560  of  44,637.    Elapsed: 0:10:14. Training loss. 0.007264500018209219 Num fake examples 17103 Num true examples 18017\n",
      "  Batch 17,600  of  44,637.    Elapsed: 0:10:15. Training loss. 0.006231982726603746 Num fake examples 17149 Num true examples 18051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 17,640  of  44,637.    Elapsed: 0:10:17. Training loss. 0.00666718790307641 Num fake examples 17184 Num true examples 18096\n",
      "  Batch 17,680  of  44,637.    Elapsed: 0:10:18. Training loss. 0.00535806268453598 Num fake examples 17220 Num true examples 18140\n",
      "  Batch 17,720  of  44,637.    Elapsed: 0:10:19. Training loss. 0.004941048100590706 Num fake examples 17262 Num true examples 18178\n",
      "  Batch 17,760  of  44,637.    Elapsed: 0:10:21. Training loss. 2.6580073833465576 Num fake examples 17298 Num true examples 18222\n",
      "  Batch 17,800  of  44,637.    Elapsed: 0:10:22. Training loss. 0.005949316080659628 Num fake examples 17329 Num true examples 18271\n",
      "  Batch 17,840  of  44,637.    Elapsed: 0:10:23. Training loss. 0.006494381930679083 Num fake examples 17369 Num true examples 18311\n",
      "  Batch 17,880  of  44,637.    Elapsed: 0:10:25. Training loss. 0.004841960966587067 Num fake examples 17405 Num true examples 18355\n",
      "  Batch 17,920  of  44,637.    Elapsed: 0:10:26. Training loss. 0.0045968592166900635 Num fake examples 17448 Num true examples 18392\n",
      "  Batch 17,960  of  44,637.    Elapsed: 0:10:27. Training loss. 0.003917869180440903 Num fake examples 17482 Num true examples 18438\n",
      "  Batch 18,000  of  44,637.    Elapsed: 0:10:29. Training loss. 0.003354630433022976 Num fake examples 17521 Num true examples 18479\n",
      "  Batch 18,040  of  44,637.    Elapsed: 0:10:30. Training loss. 0.003752052318304777 Num fake examples 17565 Num true examples 18515\n",
      "  Batch 18,080  of  44,637.    Elapsed: 0:10:32. Training loss. 0.004030370153486729 Num fake examples 17598 Num true examples 18562\n",
      "  Batch 18,120  of  44,637.    Elapsed: 0:10:33. Training loss. 0.004643585067242384 Num fake examples 17636 Num true examples 18604\n",
      "  Batch 18,160  of  44,637.    Elapsed: 0:10:34. Training loss. 0.004674992524087429 Num fake examples 17666 Num true examples 18654\n",
      "  Batch 18,200  of  44,637.    Elapsed: 0:10:36. Training loss. 0.004251433536410332 Num fake examples 17713 Num true examples 18687\n",
      "  Batch 18,240  of  44,637.    Elapsed: 0:10:37. Training loss. 0.0044483644887804985 Num fake examples 17757 Num true examples 18723\n",
      "  Batch 18,280  of  44,637.    Elapsed: 0:10:38. Training loss. 0.004002658650279045 Num fake examples 17803 Num true examples 18757\n",
      "  Batch 18,320  of  44,637.    Elapsed: 0:10:40. Training loss. 0.004116808995604515 Num fake examples 17843 Num true examples 18797\n",
      "  Batch 18,360  of  44,637.    Elapsed: 0:10:41. Training loss. 0.003883776720613241 Num fake examples 17888 Num true examples 18832\n",
      "  Batch 18,400  of  44,637.    Elapsed: 0:10:42. Training loss. 0.004561054520308971 Num fake examples 17931 Num true examples 18869\n",
      "  Batch 18,440  of  44,637.    Elapsed: 0:10:44. Training loss. 0.0035646541509777308 Num fake examples 17967 Num true examples 18913\n",
      "  Batch 18,480  of  44,637.    Elapsed: 0:10:45. Training loss. 0.0034992550499737263 Num fake examples 18006 Num true examples 18954\n",
      "  Batch 18,520  of  44,637.    Elapsed: 0:10:47. Training loss. 2.8377127647399902 Num fake examples 18051 Num true examples 18989\n",
      "  Batch 18,560  of  44,637.    Elapsed: 0:10:48. Training loss. 2.8192946910858154 Num fake examples 18087 Num true examples 19033\n",
      "  Batch 18,600  of  44,637.    Elapsed: 0:10:49. Training loss. 2.6104753017425537 Num fake examples 18122 Num true examples 19078\n",
      "  Batch 18,640  of  44,637.    Elapsed: 0:10:51. Training loss. 0.0035942946560680866 Num fake examples 18163 Num true examples 19117\n",
      "  Batch 18,680  of  44,637.    Elapsed: 0:10:52. Training loss. 0.0029937587678432465 Num fake examples 18195 Num true examples 19165\n",
      "  Batch 18,720  of  44,637.    Elapsed: 0:10:53. Training loss. 0.0037752962671220303 Num fake examples 18237 Num true examples 19203\n",
      "  Batch 18,760  of  44,637.    Elapsed: 0:10:55. Training loss. 0.002712010871618986 Num fake examples 18275 Num true examples 19245\n",
      "  Batch 18,800  of  44,637.    Elapsed: 0:10:56. Training loss. 0.0038376960437744856 Num fake examples 18320 Num true examples 19280\n",
      "  Batch 18,840  of  44,637.    Elapsed: 0:10:57. Training loss. 0.003818876575678587 Num fake examples 18366 Num true examples 19314\n",
      "  Batch 18,880  of  44,637.    Elapsed: 0:10:59. Training loss. 0.004279861226677895 Num fake examples 18407 Num true examples 19353\n",
      "  Batch 18,920  of  44,637.    Elapsed: 0:11:00. Training loss. 0.00463523855432868 Num fake examples 18437 Num true examples 19403\n",
      "  Batch 18,960  of  44,637.    Elapsed: 0:11:01. Training loss. 0.0053251152858138084 Num fake examples 18469 Num true examples 19451\n",
      "  Batch 19,000  of  44,637.    Elapsed: 0:11:03. Training loss. 0.004242002964019775 Num fake examples 18517 Num true examples 19483\n",
      "  Batch 19,040  of  44,637.    Elapsed: 0:11:04. Training loss. 0.005827649030834436 Num fake examples 18559 Num true examples 19521\n",
      "  Batch 19,080  of  44,637.    Elapsed: 0:11:06. Training loss. 0.004255805164575577 Num fake examples 18590 Num true examples 19570\n",
      "  Batch 19,120  of  44,637.    Elapsed: 0:11:07. Training loss. 0.004804444499313831 Num fake examples 18622 Num true examples 19618\n",
      "  Batch 19,160  of  44,637.    Elapsed: 0:11:08. Training loss. 0.004147123079746962 Num fake examples 18667 Num true examples 19653\n",
      "  Batch 19,200  of  44,637.    Elapsed: 0:11:10. Training loss. 0.00399539340287447 Num fake examples 18700 Num true examples 19700\n",
      "  Batch 19,240  of  44,637.    Elapsed: 0:11:11. Training loss. 0.003955039661377668 Num fake examples 18737 Num true examples 19743\n",
      "  Batch 19,280  of  44,637.    Elapsed: 0:11:12. Training loss. 0.003872644156217575 Num fake examples 18772 Num true examples 19788\n",
      "  Batch 19,320  of  44,637.    Elapsed: 0:11:14. Training loss. 0.0038828537799417973 Num fake examples 18805 Num true examples 19835\n",
      "  Batch 19,360  of  44,637.    Elapsed: 0:11:15. Training loss. 0.00488509563729167 Num fake examples 18842 Num true examples 19878\n",
      "  Batch 19,400  of  44,637.    Elapsed: 0:11:16. Training loss. 0.004389908164739609 Num fake examples 18883 Num true examples 19917\n",
      "  Batch 19,440  of  44,637.    Elapsed: 0:11:18. Training loss. 0.0038997819647192955 Num fake examples 18918 Num true examples 19962\n",
      "  Batch 19,480  of  44,637.    Elapsed: 0:11:19. Training loss. 0.00298258475959301 Num fake examples 18955 Num true examples 20005\n",
      "  Batch 19,520  of  44,637.    Elapsed: 0:11:20. Training loss. 0.005256702192127705 Num fake examples 18988 Num true examples 20052\n",
      "  Batch 19,560  of  44,637.    Elapsed: 0:11:22. Training loss. 0.004803385119885206 Num fake examples 19028 Num true examples 20092\n",
      "  Batch 19,600  of  44,637.    Elapsed: 0:11:23. Training loss. 0.007042860612273216 Num fake examples 19064 Num true examples 20136\n",
      "  Batch 19,640  of  44,637.    Elapsed: 0:11:25. Training loss. 0.004782655276358128 Num fake examples 19110 Num true examples 20170\n",
      "  Batch 19,680  of  44,637.    Elapsed: 0:11:26. Training loss. 0.004390260204672813 Num fake examples 19150 Num true examples 20210\n",
      "  Batch 19,720  of  44,637.    Elapsed: 0:11:27. Training loss. 0.003769439412280917 Num fake examples 19184 Num true examples 20256\n",
      "  Batch 19,760  of  44,637.    Elapsed: 0:11:29. Training loss. 0.0030208630487322807 Num fake examples 19218 Num true examples 20302\n",
      "  Batch 19,800  of  44,637.    Elapsed: 0:11:30. Training loss. 0.003814616473391652 Num fake examples 19253 Num true examples 20347\n",
      "  Batch 19,840  of  44,637.    Elapsed: 0:11:31. Training loss. 0.003944546449929476 Num fake examples 19295 Num true examples 20385\n",
      "  Batch 19,880  of  44,637.    Elapsed: 0:11:33. Training loss. 0.0034625697880983353 Num fake examples 19338 Num true examples 20422\n",
      "  Batch 19,920  of  44,637.    Elapsed: 0:11:34. Training loss. 0.0029608341865241528 Num fake examples 19378 Num true examples 20462\n",
      "  Batch 19,960  of  44,637.    Elapsed: 0:11:35. Training loss. 0.0037961299531161785 Num fake examples 19414 Num true examples 20506\n",
      "  Batch 20,000  of  44,637.    Elapsed: 0:11:37. Training loss. 0.0029275515116751194 Num fake examples 19457 Num true examples 20543\n",
      "  Batch 20,040  of  44,637.    Elapsed: 0:11:38. Training loss. 0.003911874257028103 Num fake examples 19494 Num true examples 20586\n",
      "  Batch 20,080  of  44,637.    Elapsed: 0:11:39. Training loss. 0.004192160442471504 Num fake examples 19536 Num true examples 20624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 20,120  of  44,637.    Elapsed: 0:11:41. Training loss. 0.003594201523810625 Num fake examples 19576 Num true examples 20664\n",
      "  Batch 20,160  of  44,637.    Elapsed: 0:11:42. Training loss. 2.805429458618164 Num fake examples 19620 Num true examples 20700\n",
      "  Batch 20,200  of  44,637.    Elapsed: 0:11:43. Training loss. 0.0028133837040513754 Num fake examples 19648 Num true examples 20752\n",
      "  Batch 20,240  of  44,637.    Elapsed: 0:11:45. Training loss. 2.9478862285614014 Num fake examples 19688 Num true examples 20792\n",
      "  Batch 20,280  of  44,637.    Elapsed: 0:11:46. Training loss. 0.004856398329138756 Num fake examples 19731 Num true examples 20829\n",
      "  Batch 20,320  of  44,637.    Elapsed: 0:11:48. Training loss. 0.004170049447566271 Num fake examples 19774 Num true examples 20866\n",
      "  Batch 20,360  of  44,637.    Elapsed: 0:11:49. Training loss. 0.004195463843643665 Num fake examples 19816 Num true examples 20904\n",
      "  Batch 20,400  of  44,637.    Elapsed: 0:11:50. Training loss. 0.003741597756743431 Num fake examples 19848 Num true examples 20952\n",
      "  Batch 20,440  of  44,637.    Elapsed: 0:11:52. Training loss. 2.817988872528076 Num fake examples 19891 Num true examples 20989\n",
      "  Batch 20,480  of  44,637.    Elapsed: 0:11:53. Training loss. 0.00402172701433301 Num fake examples 19930 Num true examples 21030\n",
      "  Batch 20,520  of  44,637.    Elapsed: 0:11:54. Training loss. 0.004245090298354626 Num fake examples 19968 Num true examples 21072\n",
      "  Batch 20,560  of  44,637.    Elapsed: 0:11:56. Training loss. 0.004784192889928818 Num fake examples 20000 Num true examples 21120\n",
      "  Batch 20,600  of  44,637.    Elapsed: 0:11:57. Training loss. 2.705075979232788 Num fake examples 20033 Num true examples 21167\n",
      "  Batch 20,640  of  44,637.    Elapsed: 0:11:59. Training loss. 0.005177835933864117 Num fake examples 20076 Num true examples 21204\n",
      "  Batch 20,680  of  44,637.    Elapsed: 0:12:00. Training loss. 0.004318500403314829 Num fake examples 20121 Num true examples 21239\n",
      "  Batch 20,720  of  44,637.    Elapsed: 0:12:01. Training loss. 0.004788779653608799 Num fake examples 20155 Num true examples 21285\n",
      "  Batch 20,760  of  44,637.    Elapsed: 0:12:03. Training loss. 0.00550909573212266 Num fake examples 20193 Num true examples 21327\n",
      "  Batch 20,800  of  44,637.    Elapsed: 0:12:04. Training loss. 0.004158874042332172 Num fake examples 20235 Num true examples 21365\n",
      "  Batch 20,840  of  44,637.    Elapsed: 0:12:05. Training loss. 0.005451628472656012 Num fake examples 20269 Num true examples 21411\n",
      "  Batch 20,880  of  44,637.    Elapsed: 0:12:07. Training loss. 0.004649212118238211 Num fake examples 20310 Num true examples 21450\n",
      "  Batch 20,920  of  44,637.    Elapsed: 0:12:08. Training loss. 0.005544736515730619 Num fake examples 20345 Num true examples 21495\n",
      "  Batch 20,960  of  44,637.    Elapsed: 0:12:10. Training loss. 0.004617119207978249 Num fake examples 20376 Num true examples 21544\n",
      "  Batch 21,000  of  44,637.    Elapsed: 0:12:11. Training loss. 0.005828895606100559 Num fake examples 20407 Num true examples 21593\n",
      "  Batch 21,040  of  44,637.    Elapsed: 0:12:12. Training loss. 0.005904783494770527 Num fake examples 20446 Num true examples 21634\n",
      "  Batch 21,080  of  44,637.    Elapsed: 0:12:14. Training loss. 0.005317806266248226 Num fake examples 20486 Num true examples 21674\n",
      "  Batch 21,120  of  44,637.    Elapsed: 0:12:15. Training loss. 0.004527088720351458 Num fake examples 20530 Num true examples 21710\n",
      "  Batch 21,160  of  44,637.    Elapsed: 0:12:16. Training loss. 2.746110200881958 Num fake examples 20569 Num true examples 21751\n",
      "  Batch 21,200  of  44,637.    Elapsed: 0:12:18. Training loss. 0.004318013787269592 Num fake examples 20609 Num true examples 21791\n",
      "  Batch 21,240  of  44,637.    Elapsed: 0:12:19. Training loss. 2.643772840499878 Num fake examples 20648 Num true examples 21832\n",
      "  Batch 21,280  of  44,637.    Elapsed: 0:12:21. Training loss. 0.004672594368457794 Num fake examples 20681 Num true examples 21879\n",
      "  Batch 21,320  of  44,637.    Elapsed: 0:12:22. Training loss. 0.006393739953637123 Num fake examples 20718 Num true examples 21922\n",
      "  Batch 21,360  of  44,637.    Elapsed: 0:12:23. Training loss. 0.006074489559978247 Num fake examples 20753 Num true examples 21967\n",
      "  Batch 21,400  of  44,637.    Elapsed: 0:12:25. Training loss. 0.00488901836797595 Num fake examples 20786 Num true examples 22014\n",
      "  Batch 21,440  of  44,637.    Elapsed: 0:12:26. Training loss. 0.005503376945853233 Num fake examples 20828 Num true examples 22052\n",
      "  Batch 21,480  of  44,637.    Elapsed: 0:12:27. Training loss. 0.0045849597081542015 Num fake examples 20872 Num true examples 22088\n",
      "  Batch 21,520  of  44,637.    Elapsed: 0:12:29. Training loss. 0.006891518831253052 Num fake examples 20914 Num true examples 22126\n",
      "  Batch 21,560  of  44,637.    Elapsed: 0:12:30. Training loss. 0.005244452506303787 Num fake examples 20951 Num true examples 22169\n",
      "  Batch 21,600  of  44,637.    Elapsed: 0:12:31. Training loss. 0.005134734325110912 Num fake examples 20994 Num true examples 22206\n",
      "  Batch 21,640  of  44,637.    Elapsed: 0:12:33. Training loss. 0.004229416139423847 Num fake examples 21036 Num true examples 22244\n",
      "  Batch 21,680  of  44,637.    Elapsed: 0:12:34. Training loss. 2.5930705070495605 Num fake examples 21067 Num true examples 22293\n",
      "  Batch 21,720  of  44,637.    Elapsed: 0:12:36. Training loss. 0.0049269115552306175 Num fake examples 21111 Num true examples 22329\n",
      "  Batch 21,760  of  44,637.    Elapsed: 0:12:37. Training loss. 0.0046124993823468685 Num fake examples 21148 Num true examples 22372\n",
      "  Batch 21,800  of  44,637.    Elapsed: 0:12:38. Training loss. 0.006050778087228537 Num fake examples 21193 Num true examples 22407\n",
      "  Batch 21,840  of  44,637.    Elapsed: 0:12:40. Training loss. 0.006264142692089081 Num fake examples 21237 Num true examples 22443\n",
      "  Batch 21,880  of  44,637.    Elapsed: 0:12:41. Training loss. 0.006496863439679146 Num fake examples 21284 Num true examples 22476\n",
      "  Batch 21,920  of  44,637.    Elapsed: 0:12:42. Training loss. 0.006302002351731062 Num fake examples 21326 Num true examples 22514\n",
      "  Batch 21,960  of  44,637.    Elapsed: 0:12:44. Training loss. 0.004106135107576847 Num fake examples 21354 Num true examples 22566\n",
      "  Batch 22,000  of  44,637.    Elapsed: 0:12:45. Training loss. 0.005311491433531046 Num fake examples 21392 Num true examples 22608\n",
      "  Batch 22,040  of  44,637.    Elapsed: 0:12:47. Training loss. 0.004944507498294115 Num fake examples 21428 Num true examples 22652\n",
      "  Batch 22,080  of  44,637.    Elapsed: 0:12:48. Training loss. 0.003732437966391444 Num fake examples 21472 Num true examples 22688\n",
      "  Batch 22,120  of  44,637.    Elapsed: 0:12:49. Training loss. 0.0032057370990514755 Num fake examples 21507 Num true examples 22733\n",
      "  Batch 22,160  of  44,637.    Elapsed: 0:12:51. Training loss. 0.0030874605290591717 Num fake examples 21536 Num true examples 22784\n",
      "  Batch 22,200  of  44,637.    Elapsed: 0:12:52. Training loss. 0.004562707617878914 Num fake examples 21574 Num true examples 22826\n",
      "  Batch 22,240  of  44,637.    Elapsed: 0:12:53. Training loss. 0.004069237969815731 Num fake examples 21611 Num true examples 22869\n",
      "  Batch 22,280  of  44,637.    Elapsed: 0:12:55. Training loss. 0.0030853399075567722 Num fake examples 21649 Num true examples 22911\n",
      "  Batch 22,320  of  44,637.    Elapsed: 0:12:56. Training loss. 0.004069225862622261 Num fake examples 21692 Num true examples 22948\n",
      "  Batch 22,360  of  44,637.    Elapsed: 0:12:57. Training loss. 0.0051016900688409805 Num fake examples 21731 Num true examples 22989\n",
      "  Batch 22,400  of  44,637.    Elapsed: 0:12:59. Training loss. 0.00527957733720541 Num fake examples 21763 Num true examples 23037\n",
      "  Batch 22,440  of  44,637.    Elapsed: 0:13:00. Training loss. 0.0042650941759347916 Num fake examples 21799 Num true examples 23081\n",
      "  Batch 22,480  of  44,637.    Elapsed: 0:13:02. Training loss. 2.75665283203125 Num fake examples 21841 Num true examples 23119\n",
      "  Batch 22,520  of  44,637.    Elapsed: 0:13:03. Training loss. 0.004651501774787903 Num fake examples 21881 Num true examples 23159\n",
      "  Batch 22,560  of  44,637.    Elapsed: 0:13:04. Training loss. 0.003722385037690401 Num fake examples 21921 Num true examples 23199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 22,600  of  44,637.    Elapsed: 0:13:06. Training loss. 0.0041538989171385765 Num fake examples 21961 Num true examples 23239\n",
      "  Batch 22,640  of  44,637.    Elapsed: 0:13:07. Training loss. 0.004881605971604586 Num fake examples 21998 Num true examples 23282\n",
      "  Batch 22,680  of  44,637.    Elapsed: 0:13:08. Training loss. 0.00393727608025074 Num fake examples 22038 Num true examples 23322\n",
      "  Batch 22,720  of  44,637.    Elapsed: 0:13:10. Training loss. 0.006764019839465618 Num fake examples 22086 Num true examples 23354\n",
      "  Batch 22,760  of  44,637.    Elapsed: 0:13:11. Training loss. 0.005902543663978577 Num fake examples 22120 Num true examples 23400\n",
      "  Batch 22,800  of  44,637.    Elapsed: 0:13:12. Training loss. 0.005868511740118265 Num fake examples 22163 Num true examples 23437\n",
      "  Batch 22,840  of  44,637.    Elapsed: 0:13:14. Training loss. 0.003869712818413973 Num fake examples 22203 Num true examples 23477\n",
      "  Batch 22,880  of  44,637.    Elapsed: 0:13:15. Training loss. 0.005094388499855995 Num fake examples 22243 Num true examples 23517\n",
      "  Batch 22,920  of  44,637.    Elapsed: 0:13:16. Training loss. 0.004301532171666622 Num fake examples 22279 Num true examples 23561\n",
      "  Batch 22,960  of  44,637.    Elapsed: 0:13:18. Training loss. 0.005308543797582388 Num fake examples 22321 Num true examples 23599\n",
      "  Batch 23,000  of  44,637.    Elapsed: 0:13:19. Training loss. 0.0030231070704758167 Num fake examples 22356 Num true examples 23644\n",
      "  Batch 23,040  of  44,637.    Elapsed: 0:13:21. Training loss. 2.718357801437378 Num fake examples 22395 Num true examples 23685\n",
      "  Batch 23,080  of  44,637.    Elapsed: 0:13:22. Training loss. 0.0035134153440594673 Num fake examples 22434 Num true examples 23726\n",
      "  Batch 23,120  of  44,637.    Elapsed: 0:13:23. Training loss. 0.004310538060963154 Num fake examples 22482 Num true examples 23758\n",
      "  Batch 23,160  of  44,637.    Elapsed: 0:13:25. Training loss. 0.004228021949529648 Num fake examples 22526 Num true examples 23794\n",
      "  Batch 23,200  of  44,637.    Elapsed: 0:13:26. Training loss. 0.0033654957078397274 Num fake examples 22565 Num true examples 23835\n",
      "  Batch 23,240  of  44,637.    Elapsed: 0:13:27. Training loss. 0.0044196005910634995 Num fake examples 22604 Num true examples 23876\n",
      "  Batch 23,280  of  44,637.    Elapsed: 0:13:29. Training loss. 0.0034425556659698486 Num fake examples 22646 Num true examples 23914\n",
      "  Batch 23,320  of  44,637.    Elapsed: 0:13:30. Training loss. 0.003454532939940691 Num fake examples 22687 Num true examples 23953\n",
      "  Batch 23,360  of  44,637.    Elapsed: 0:13:31. Training loss. 0.0036322774831205606 Num fake examples 22719 Num true examples 24001\n",
      "  Batch 23,400  of  44,637.    Elapsed: 0:13:33. Training loss. 0.003327176673337817 Num fake examples 22759 Num true examples 24041\n",
      "  Batch 23,440  of  44,637.    Elapsed: 0:13:34. Training loss. 0.003954839892685413 Num fake examples 22800 Num true examples 24080\n",
      "  Batch 23,480  of  44,637.    Elapsed: 0:13:35. Training loss. 0.0032520308159291744 Num fake examples 22839 Num true examples 24121\n",
      "  Batch 23,520  of  44,637.    Elapsed: 0:13:37. Training loss. 2.8954432010650635 Num fake examples 22883 Num true examples 24157\n",
      "  Batch 23,560  of  44,637.    Elapsed: 0:13:38. Training loss. 0.005175933241844177 Num fake examples 22923 Num true examples 24197\n",
      "  Batch 23,600  of  44,637.    Elapsed: 0:13:40. Training loss. 0.004123586229979992 Num fake examples 22963 Num true examples 24237\n",
      "  Batch 23,640  of  44,637.    Elapsed: 0:13:41. Training loss. 0.004119511693716049 Num fake examples 23006 Num true examples 24274\n",
      "  Batch 23,680  of  44,637.    Elapsed: 0:13:42. Training loss. 0.0038907642010599375 Num fake examples 23056 Num true examples 24304\n",
      "  Batch 23,720  of  44,637.    Elapsed: 0:13:44. Training loss. 0.004403832368552685 Num fake examples 23092 Num true examples 24348\n",
      "  Batch 23,760  of  44,637.    Elapsed: 0:13:45. Training loss. 0.0055635841563344 Num fake examples 23128 Num true examples 24392\n",
      "  Batch 23,800  of  44,637.    Elapsed: 0:13:46. Training loss. 0.006715861149132252 Num fake examples 23165 Num true examples 24435\n",
      "  Batch 23,840  of  44,637.    Elapsed: 0:13:48. Training loss. 0.005697786808013916 Num fake examples 23195 Num true examples 24485\n",
      "  Batch 23,880  of  44,637.    Elapsed: 0:13:49. Training loss. 0.006066619884222746 Num fake examples 23229 Num true examples 24531\n",
      "  Batch 23,920  of  44,637.    Elapsed: 0:13:50. Training loss. 0.006445621605962515 Num fake examples 23264 Num true examples 24576\n",
      "  Batch 23,960  of  44,637.    Elapsed: 0:13:52. Training loss. 0.006177963688969612 Num fake examples 23298 Num true examples 24622\n",
      "  Batch 24,000  of  44,637.    Elapsed: 0:13:53. Training loss. 0.007035071961581707 Num fake examples 23326 Num true examples 24674\n",
      "  Batch 24,040  of  44,637.    Elapsed: 0:13:54. Training loss. 0.006015538237988949 Num fake examples 23363 Num true examples 24717\n",
      "  Batch 24,080  of  44,637.    Elapsed: 0:13:56. Training loss. 0.004471219144761562 Num fake examples 23403 Num true examples 24757\n",
      "  Batch 24,120  of  44,637.    Elapsed: 0:13:57. Training loss. 0.0048671867698431015 Num fake examples 23438 Num true examples 24802\n",
      "  Batch 24,160  of  44,637.    Elapsed: 0:13:59. Training loss. 0.004980851896107197 Num fake examples 23479 Num true examples 24841\n",
      "  Batch 24,200  of  44,637.    Elapsed: 0:14:00. Training loss. 0.0034250786993652582 Num fake examples 23520 Num true examples 24880\n",
      "  Batch 24,240  of  44,637.    Elapsed: 0:14:01. Training loss. 0.003964683040976524 Num fake examples 23558 Num true examples 24922\n",
      "  Batch 24,280  of  44,637.    Elapsed: 0:14:03. Training loss. 0.003990916535258293 Num fake examples 23597 Num true examples 24963\n",
      "  Batch 24,320  of  44,637.    Elapsed: 0:14:04. Training loss. 0.0035976897925138474 Num fake examples 23640 Num true examples 25000\n",
      "  Batch 24,360  of  44,637.    Elapsed: 0:14:05. Training loss. 0.004013348836451769 Num fake examples 23675 Num true examples 25045\n",
      "  Batch 24,400  of  44,637.    Elapsed: 0:14:07. Training loss. 0.0032902732491493225 Num fake examples 23706 Num true examples 25094\n",
      "  Batch 24,440  of  44,637.    Elapsed: 0:14:08. Training loss. 0.003492798423394561 Num fake examples 23742 Num true examples 25138\n",
      "  Batch 24,480  of  44,637.    Elapsed: 0:14:09. Training loss. 0.003300265409052372 Num fake examples 23779 Num true examples 25181\n",
      "  Batch 24,520  of  44,637.    Elapsed: 0:14:11. Training loss. 2.723634958267212 Num fake examples 23824 Num true examples 25216\n",
      "  Batch 24,560  of  44,637.    Elapsed: 0:14:12. Training loss. 0.0034859096631407738 Num fake examples 23872 Num true examples 25248\n",
      "  Batch 24,600  of  44,637.    Elapsed: 0:14:13. Training loss. 0.002972629852592945 Num fake examples 23905 Num true examples 25295\n",
      "  Batch 24,640  of  44,637.    Elapsed: 0:14:15. Training loss. 0.002709710504859686 Num fake examples 23941 Num true examples 25339\n",
      "  Batch 24,680  of  44,637.    Elapsed: 0:14:16. Training loss. 0.002668187953531742 Num fake examples 23989 Num true examples 25371\n",
      "  Batch 24,720  of  44,637.    Elapsed: 0:14:18. Training loss. 0.003273183014243841 Num fake examples 24027 Num true examples 25413\n",
      "  Batch 24,760  of  44,637.    Elapsed: 0:14:19. Training loss. 0.003192722564563155 Num fake examples 24064 Num true examples 25456\n",
      "  Batch 24,800  of  44,637.    Elapsed: 0:14:20. Training loss. 0.0038129326421767473 Num fake examples 24100 Num true examples 25500\n",
      "  Batch 24,840  of  44,637.    Elapsed: 0:14:22. Training loss. 0.003298363648355007 Num fake examples 24141 Num true examples 25539\n",
      "  Batch 24,880  of  44,637.    Elapsed: 0:14:23. Training loss. 0.003193717449903488 Num fake examples 24165 Num true examples 25595\n",
      "  Batch 24,920  of  44,637.    Elapsed: 0:14:24. Training loss. 0.003739274339750409 Num fake examples 24204 Num true examples 25636\n",
      "  Batch 24,960  of  44,637.    Elapsed: 0:14:26. Training loss. 0.00351071753539145 Num fake examples 24239 Num true examples 25681\n",
      "  Batch 25,000  of  44,637.    Elapsed: 0:14:27. Training loss. 0.0032365056686103344 Num fake examples 24289 Num true examples 25711\n",
      "  Batch 25,040  of  44,637.    Elapsed: 0:14:28. Training loss. 0.003571663051843643 Num fake examples 24333 Num true examples 25747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 25,080  of  44,637.    Elapsed: 0:14:30. Training loss. 0.004131588153541088 Num fake examples 24371 Num true examples 25789\n",
      "  Batch 25,120  of  44,637.    Elapsed: 0:14:31. Training loss. 0.0035858270712196827 Num fake examples 24413 Num true examples 25827\n",
      "  Batch 25,160  of  44,637.    Elapsed: 0:14:32. Training loss. 0.003207125701010227 Num fake examples 24453 Num true examples 25867\n",
      "  Batch 25,200  of  44,637.    Elapsed: 0:14:34. Training loss. 0.003502995241433382 Num fake examples 24492 Num true examples 25908\n",
      "  Batch 25,240  of  44,637.    Elapsed: 0:14:35. Training loss. 0.003737292718142271 Num fake examples 24531 Num true examples 25949\n",
      "  Batch 25,280  of  44,637.    Elapsed: 0:14:37. Training loss. 0.003203268861398101 Num fake examples 24578 Num true examples 25982\n",
      "  Batch 25,320  of  44,637.    Elapsed: 0:14:38. Training loss. 2.6874194145202637 Num fake examples 24619 Num true examples 26021\n",
      "  Batch 25,360  of  44,637.    Elapsed: 0:14:39. Training loss. 0.0038406297098845243 Num fake examples 24660 Num true examples 26060\n",
      "  Batch 25,400  of  44,637.    Elapsed: 0:14:41. Training loss. 0.0030590565875172615 Num fake examples 24699 Num true examples 26101\n",
      "  Batch 25,440  of  44,637.    Elapsed: 0:14:42. Training loss. 0.0030783533584326506 Num fake examples 24740 Num true examples 26140\n",
      "  Batch 25,480  of  44,637.    Elapsed: 0:14:43. Training loss. 0.0031002832110971212 Num fake examples 24775 Num true examples 26185\n",
      "  Batch 25,520  of  44,637.    Elapsed: 0:14:45. Training loss. 0.0031581828370690346 Num fake examples 24817 Num true examples 26223\n",
      "  Batch 25,560  of  44,637.    Elapsed: 0:14:46. Training loss. 0.0027171701658517122 Num fake examples 24855 Num true examples 26265\n",
      "  Batch 25,600  of  44,637.    Elapsed: 0:14:47. Training loss. 0.0030432722996920347 Num fake examples 24904 Num true examples 26296\n",
      "  Batch 25,640  of  44,637.    Elapsed: 0:14:49. Training loss. 0.0030090510845184326 Num fake examples 24935 Num true examples 26345\n",
      "  Batch 25,680  of  44,637.    Elapsed: 0:14:50. Training loss. 0.0033887927420437336 Num fake examples 24972 Num true examples 26388\n",
      "  Batch 25,720  of  44,637.    Elapsed: 0:14:52. Training loss. 0.0038079037331044674 Num fake examples 25016 Num true examples 26424\n",
      "  Batch 25,760  of  44,637.    Elapsed: 0:14:53. Training loss. 0.0036082095466554165 Num fake examples 25057 Num true examples 26463\n",
      "  Batch 25,800  of  44,637.    Elapsed: 0:14:54. Training loss. 0.0031349696218967438 Num fake examples 25096 Num true examples 26504\n",
      "  Batch 25,840  of  44,637.    Elapsed: 0:14:56. Training loss. 0.004085000604391098 Num fake examples 25133 Num true examples 26547\n",
      "  Batch 25,880  of  44,637.    Elapsed: 0:14:57. Training loss. 0.004524492658674717 Num fake examples 25168 Num true examples 26592\n",
      "  Batch 25,920  of  44,637.    Elapsed: 0:14:58. Training loss. 0.003931935876607895 Num fake examples 25211 Num true examples 26629\n",
      "  Batch 25,960  of  44,637.    Elapsed: 0:15:00. Training loss. 0.0034553781151771545 Num fake examples 25245 Num true examples 26675\n",
      "  Batch 26,000  of  44,637.    Elapsed: 0:15:01. Training loss. 0.003698923159390688 Num fake examples 25284 Num true examples 26716\n",
      "  Batch 26,040  of  44,637.    Elapsed: 0:15:02. Training loss. 0.005222576204687357 Num fake examples 25317 Num true examples 26763\n",
      "  Batch 26,080  of  44,637.    Elapsed: 0:15:04. Training loss. 0.0047586834989488125 Num fake examples 25365 Num true examples 26795\n",
      "  Batch 26,120  of  44,637.    Elapsed: 0:15:05. Training loss. 2.7965900897979736 Num fake examples 25396 Num true examples 26844\n",
      "  Batch 26,160  of  44,637.    Elapsed: 0:15:06. Training loss. 0.004128466825932264 Num fake examples 25427 Num true examples 26893\n",
      "  Batch 26,200  of  44,637.    Elapsed: 0:15:08. Training loss. 0.004679041914641857 Num fake examples 25469 Num true examples 26931\n",
      "  Batch 26,240  of  44,637.    Elapsed: 0:15:09. Training loss. 0.003841615281999111 Num fake examples 25511 Num true examples 26969\n",
      "  Batch 26,280  of  44,637.    Elapsed: 0:15:11. Training loss. 0.0035754197742789984 Num fake examples 25549 Num true examples 27011\n",
      "  Batch 26,320  of  44,637.    Elapsed: 0:15:12. Training loss. 0.0037253634072840214 Num fake examples 25593 Num true examples 27047\n",
      "  Batch 26,360  of  44,637.    Elapsed: 0:15:13. Training loss. 0.0027459911070764065 Num fake examples 25638 Num true examples 27082\n",
      "  Batch 26,400  of  44,637.    Elapsed: 0:15:15. Training loss. 0.003198954975232482 Num fake examples 25674 Num true examples 27126\n",
      "  Batch 26,440  of  44,637.    Elapsed: 0:15:16. Training loss. 0.0029153102077543736 Num fake examples 25717 Num true examples 27163\n",
      "  Batch 26,480  of  44,637.    Elapsed: 0:15:17. Training loss. 0.002732684835791588 Num fake examples 25756 Num true examples 27204\n",
      "  Batch 26,520  of  44,637.    Elapsed: 0:15:19. Training loss. 0.003321973606944084 Num fake examples 25806 Num true examples 27234\n",
      "  Batch 26,560  of  44,637.    Elapsed: 0:15:20. Training loss. 0.003093207720667124 Num fake examples 25845 Num true examples 27275\n",
      "  Batch 26,600  of  44,637.    Elapsed: 0:15:21. Training loss. 0.0031438940204679966 Num fake examples 25889 Num true examples 27311\n",
      "  Batch 26,640  of  44,637.    Elapsed: 0:15:23. Training loss. 0.0038438457995653152 Num fake examples 25930 Num true examples 27350\n",
      "  Batch 26,680  of  44,637.    Elapsed: 0:15:24. Training loss. 0.0031887092627584934 Num fake examples 25976 Num true examples 27384\n",
      "  Batch 26,720  of  44,637.    Elapsed: 0:15:25. Training loss. 0.003031427040696144 Num fake examples 26017 Num true examples 27423\n",
      "  Batch 26,760  of  44,637.    Elapsed: 0:15:27. Training loss. 0.0031204060651361942 Num fake examples 26053 Num true examples 27467\n",
      "  Batch 26,800  of  44,637.    Elapsed: 0:15:28. Training loss. 0.003887450322508812 Num fake examples 26093 Num true examples 27507\n",
      "  Batch 26,840  of  44,637.    Elapsed: 0:15:29. Training loss. 0.0032293861731886864 Num fake examples 26139 Num true examples 27541\n",
      "  Batch 26,880  of  44,637.    Elapsed: 0:15:31. Training loss. 0.004114579875022173 Num fake examples 26173 Num true examples 27587\n",
      "  Batch 26,920  of  44,637.    Elapsed: 0:15:32. Training loss. 0.003454106394201517 Num fake examples 26211 Num true examples 27629\n",
      "  Batch 26,960  of  44,637.    Elapsed: 0:15:34. Training loss. 0.0033347862772643566 Num fake examples 26254 Num true examples 27666\n",
      "  Batch 27,000  of  44,637.    Elapsed: 0:15:35. Training loss. 0.0038852370344102383 Num fake examples 26293 Num true examples 27707\n",
      "  Batch 27,040  of  44,637.    Elapsed: 0:15:36. Training loss. 0.0038730588275939226 Num fake examples 26338 Num true examples 27742\n",
      "  Batch 27,080  of  44,637.    Elapsed: 0:15:38. Training loss. 0.003298663068562746 Num fake examples 26380 Num true examples 27780\n",
      "  Batch 27,120  of  44,637.    Elapsed: 0:15:39. Training loss. 0.004202072508633137 Num fake examples 26423 Num true examples 27817\n",
      "  Batch 27,160  of  44,637.    Elapsed: 0:15:40. Training loss. 0.003704653587192297 Num fake examples 26460 Num true examples 27860\n",
      "  Batch 27,200  of  44,637.    Elapsed: 0:15:42. Training loss. 0.003823964623734355 Num fake examples 26500 Num true examples 27900\n",
      "  Batch 27,240  of  44,637.    Elapsed: 0:15:43. Training loss. 0.003665646305307746 Num fake examples 26544 Num true examples 27936\n",
      "  Batch 27,280  of  44,637.    Elapsed: 0:15:44. Training loss. 0.0034393751993775368 Num fake examples 26590 Num true examples 27970\n",
      "  Batch 27,320  of  44,637.    Elapsed: 0:15:46. Training loss. 0.00332617643289268 Num fake examples 26626 Num true examples 28014\n",
      "  Batch 27,360  of  44,637.    Elapsed: 0:15:47. Training loss. 0.004028351977467537 Num fake examples 26664 Num true examples 28056\n",
      "  Batch 27,400  of  44,637.    Elapsed: 0:15:49. Training loss. 0.0032703825272619724 Num fake examples 26699 Num true examples 28101\n",
      "  Batch 27,440  of  44,637.    Elapsed: 0:15:50. Training loss. 0.002985333791002631 Num fake examples 26732 Num true examples 28148\n",
      "  Batch 27,480  of  44,637.    Elapsed: 0:15:51. Training loss. 2.914257049560547 Num fake examples 26767 Num true examples 28193\n",
      "  Batch 27,520  of  44,637.    Elapsed: 0:15:53. Training loss. 0.004168433137238026 Num fake examples 26800 Num true examples 28240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 27,560  of  44,637.    Elapsed: 0:15:54. Training loss. 0.00393792986869812 Num fake examples 26848 Num true examples 28272\n",
      "  Batch 27,600  of  44,637.    Elapsed: 0:15:55. Training loss. 0.0036313256714493036 Num fake examples 26889 Num true examples 28311\n",
      "  Batch 27,640  of  44,637.    Elapsed: 0:15:57. Training loss. 0.004383870866149664 Num fake examples 26926 Num true examples 28354\n",
      "  Batch 27,680  of  44,637.    Elapsed: 0:15:58. Training loss. 0.0047114696353673935 Num fake examples 26960 Num true examples 28400\n",
      "  Batch 27,720  of  44,637.    Elapsed: 0:15:59. Training loss. 0.003646712051704526 Num fake examples 26991 Num true examples 28449\n",
      "  Batch 27,760  of  44,637.    Elapsed: 0:16:01. Training loss. 0.00410042516887188 Num fake examples 27029 Num true examples 28491\n",
      "  Batch 27,800  of  44,637.    Elapsed: 0:16:02. Training loss. 0.004108780063688755 Num fake examples 27065 Num true examples 28535\n",
      "  Batch 27,840  of  44,637.    Elapsed: 0:16:04. Training loss. 0.0037874244153499603 Num fake examples 27101 Num true examples 28579\n",
      "  Batch 27,880  of  44,637.    Elapsed: 0:16:05. Training loss. 0.004188520833849907 Num fake examples 27132 Num true examples 28628\n",
      "  Batch 27,920  of  44,637.    Elapsed: 0:16:06. Training loss. 0.003622511401772499 Num fake examples 27175 Num true examples 28665\n",
      "  Batch 27,960  of  44,637.    Elapsed: 0:16:08. Training loss. 0.004034072160720825 Num fake examples 27213 Num true examples 28707\n",
      "  Batch 28,000  of  44,637.    Elapsed: 0:16:09. Training loss. 2.8305106163024902 Num fake examples 27254 Num true examples 28746\n",
      "  Batch 28,040  of  44,637.    Elapsed: 0:16:11. Training loss. 0.003539197612553835 Num fake examples 27285 Num true examples 28795\n",
      "  Batch 28,080  of  44,637.    Elapsed: 0:16:12. Training loss. 0.003942491952329874 Num fake examples 27323 Num true examples 28837\n",
      "  Batch 28,120  of  44,637.    Elapsed: 0:16:13. Training loss. 0.0035769902169704437 Num fake examples 27355 Num true examples 28885\n",
      "  Batch 28,160  of  44,637.    Elapsed: 0:16:15. Training loss. 0.0035775506403297186 Num fake examples 27395 Num true examples 28925\n",
      "  Batch 28,200  of  44,637.    Elapsed: 0:16:16. Training loss. 0.0025444042403250933 Num fake examples 27435 Num true examples 28965\n",
      "  Batch 28,240  of  44,637.    Elapsed: 0:16:17. Training loss. 0.0026329336687922478 Num fake examples 27470 Num true examples 29010\n",
      "  Batch 28,280  of  44,637.    Elapsed: 0:16:19. Training loss. 0.002734629437327385 Num fake examples 27515 Num true examples 29045\n",
      "  Batch 28,320  of  44,637.    Elapsed: 0:16:20. Training loss. 0.0028428174555301666 Num fake examples 27555 Num true examples 29085\n",
      "  Batch 28,360  of  44,637.    Elapsed: 0:16:22. Training loss. 0.0030078792478889227 Num fake examples 27590 Num true examples 29130\n",
      "  Batch 28,400  of  44,637.    Elapsed: 0:16:23. Training loss. 0.003914241679012775 Num fake examples 27628 Num true examples 29172\n",
      "  Batch 28,440  of  44,637.    Elapsed: 0:16:24. Training loss. 0.003671266371384263 Num fake examples 27669 Num true examples 29211\n",
      "  Batch 28,480  of  44,637.    Elapsed: 0:16:26. Training loss. 2.9332034587860107 Num fake examples 27700 Num true examples 29260\n",
      "  Batch 28,520  of  44,637.    Elapsed: 0:16:27. Training loss. 0.004266729578375816 Num fake examples 27733 Num true examples 29307\n",
      "  Batch 28,560  of  44,637.    Elapsed: 0:16:28. Training loss. 0.004040978383272886 Num fake examples 27777 Num true examples 29343\n",
      "  Batch 28,600  of  44,637.    Elapsed: 0:16:30. Training loss. 0.004494718741625547 Num fake examples 27815 Num true examples 29385\n",
      "  Batch 28,640  of  44,637.    Elapsed: 0:16:31. Training loss. 0.003935955930501223 Num fake examples 27852 Num true examples 29428\n",
      "  Batch 28,680  of  44,637.    Elapsed: 0:16:33. Training loss. 0.004239795263856649 Num fake examples 27893 Num true examples 29467\n",
      "  Batch 28,720  of  44,637.    Elapsed: 0:16:34. Training loss. 0.0035458626225590706 Num fake examples 27938 Num true examples 29502\n",
      "  Batch 28,760  of  44,637.    Elapsed: 0:16:35. Training loss. 0.003985369578003883 Num fake examples 27974 Num true examples 29546\n",
      "  Batch 28,800  of  44,637.    Elapsed: 0:16:37. Training loss. 0.0044240206480026245 Num fake examples 28011 Num true examples 29589\n",
      "  Batch 28,840  of  44,637.    Elapsed: 0:16:38. Training loss. 0.004544033668935299 Num fake examples 28049 Num true examples 29631\n",
      "  Batch 28,880  of  44,637.    Elapsed: 0:16:39. Training loss. 0.004771473351866007 Num fake examples 28101 Num true examples 29659\n",
      "  Batch 28,920  of  44,637.    Elapsed: 0:16:41. Training loss. 0.004312883131206036 Num fake examples 28150 Num true examples 29690\n",
      "  Batch 28,960  of  44,637.    Elapsed: 0:16:42. Training loss. 0.0045697796158492565 Num fake examples 28186 Num true examples 29734\n",
      "  Batch 29,000  of  44,637.    Elapsed: 0:16:44. Training loss. 0.00400510523468256 Num fake examples 28228 Num true examples 29772\n",
      "  Batch 29,040  of  44,637.    Elapsed: 0:16:45. Training loss. 0.004678640048950911 Num fake examples 28272 Num true examples 29808\n",
      "  Batch 29,080  of  44,637.    Elapsed: 0:16:46. Training loss. 0.003949763253331184 Num fake examples 28318 Num true examples 29842\n",
      "  Batch 29,120  of  44,637.    Elapsed: 0:16:48. Training loss. 2.6127023696899414 Num fake examples 28359 Num true examples 29881\n",
      "  Batch 29,160  of  44,637.    Elapsed: 0:16:49. Training loss. 0.004324798937886953 Num fake examples 28396 Num true examples 29924\n",
      "  Batch 29,200  of  44,637.    Elapsed: 0:16:50. Training loss. 0.004445469938218594 Num fake examples 28438 Num true examples 29962\n",
      "  Batch 29,240  of  44,637.    Elapsed: 0:16:52. Training loss. 0.003844308201223612 Num fake examples 28474 Num true examples 30006\n",
      "  Batch 29,280  of  44,637.    Elapsed: 0:16:53. Training loss. 0.0038082608953118324 Num fake examples 28518 Num true examples 30042\n",
      "  Batch 29,320  of  44,637.    Elapsed: 0:16:54. Training loss. 2.748014450073242 Num fake examples 28553 Num true examples 30087\n",
      "  Batch 29,360  of  44,637.    Elapsed: 0:16:56. Training loss. 0.004172216635197401 Num fake examples 28591 Num true examples 30129\n",
      "  Batch 29,400  of  44,637.    Elapsed: 0:16:57. Training loss. 0.004933720454573631 Num fake examples 28629 Num true examples 30171\n",
      "  Batch 29,440  of  44,637.    Elapsed: 0:16:59. Training loss. 0.004442732315510511 Num fake examples 28668 Num true examples 30212\n",
      "  Batch 29,480  of  44,637.    Elapsed: 0:17:00. Training loss. 0.004171378444880247 Num fake examples 28704 Num true examples 30256\n",
      "  Batch 29,520  of  44,637.    Elapsed: 0:17:01. Training loss. 2.7509164810180664 Num fake examples 28741 Num true examples 30299\n",
      "  Batch 29,560  of  44,637.    Elapsed: 0:17:03. Training loss. 0.004587320610880852 Num fake examples 28778 Num true examples 30342\n",
      "  Batch 29,600  of  44,637.    Elapsed: 0:17:04. Training loss. 0.004213838838040829 Num fake examples 28820 Num true examples 30380\n",
      "  Batch 29,640  of  44,637.    Elapsed: 0:17:06. Training loss. 0.004998775199055672 Num fake examples 28858 Num true examples 30422\n",
      "  Batch 29,680  of  44,637.    Elapsed: 0:17:07. Training loss. 0.004307388328015804 Num fake examples 28896 Num true examples 30464\n",
      "  Batch 29,720  of  44,637.    Elapsed: 0:17:08. Training loss. 0.0038599150720983744 Num fake examples 28936 Num true examples 30504\n",
      "  Batch 29,760  of  44,637.    Elapsed: 0:17:10. Training loss. 0.0038848803378641605 Num fake examples 28978 Num true examples 30542\n",
      "  Batch 29,800  of  44,637.    Elapsed: 0:17:11. Training loss. 0.004223308991640806 Num fake examples 29016 Num true examples 30584\n",
      "  Batch 29,840  of  44,637.    Elapsed: 0:17:12. Training loss. 0.004367669112980366 Num fake examples 29056 Num true examples 30624\n",
      "  Batch 29,880  of  44,637.    Elapsed: 0:17:14. Training loss. 0.003878937102854252 Num fake examples 29094 Num true examples 30666\n",
      "  Batch 29,920  of  44,637.    Elapsed: 0:17:15. Training loss. 0.0032582907006144524 Num fake examples 29128 Num true examples 30712\n",
      "  Batch 29,960  of  44,637.    Elapsed: 0:17:17. Training loss. 0.0029095280915498734 Num fake examples 29171 Num true examples 30749\n",
      "  Batch 30,000  of  44,637.    Elapsed: 0:17:18. Training loss. 0.0032302720937877893 Num fake examples 29210 Num true examples 30790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 30,040  of  44,637.    Elapsed: 0:17:19. Training loss. 0.0030222320929169655 Num fake examples 29246 Num true examples 30834\n",
      "  Batch 30,080  of  44,637.    Elapsed: 0:17:21. Training loss. 0.003880023490637541 Num fake examples 29284 Num true examples 30876\n",
      "  Batch 30,120  of  44,637.    Elapsed: 0:17:22. Training loss. 0.0032955228816717863 Num fake examples 29328 Num true examples 30912\n",
      "  Batch 30,160  of  44,637.    Elapsed: 0:17:24. Training loss. 0.00290493480861187 Num fake examples 29365 Num true examples 30955\n",
      "  Batch 30,200  of  44,637.    Elapsed: 0:17:25. Training loss. 0.0024547011125832796 Num fake examples 29400 Num true examples 31000\n",
      "  Batch 30,240  of  44,637.    Elapsed: 0:17:26. Training loss. 0.002777255140244961 Num fake examples 29433 Num true examples 31047\n",
      "  Batch 30,280  of  44,637.    Elapsed: 0:17:28. Training loss. 0.0029691560193896294 Num fake examples 29476 Num true examples 31084\n",
      "  Batch 30,320  of  44,637.    Elapsed: 0:17:29. Training loss. 0.002599486615508795 Num fake examples 29522 Num true examples 31118\n",
      "  Batch 30,360  of  44,637.    Elapsed: 0:17:30. Training loss. 0.0025967571418732405 Num fake examples 29559 Num true examples 31161\n",
      "  Batch 30,400  of  44,637.    Elapsed: 0:17:32. Training loss. 0.00274316081777215 Num fake examples 29593 Num true examples 31207\n",
      "  Batch 30,440  of  44,637.    Elapsed: 0:17:33. Training loss. 0.0027543390169739723 Num fake examples 29634 Num true examples 31246\n",
      "  Batch 30,480  of  44,637.    Elapsed: 0:17:35. Training loss. 0.00391251128166914 Num fake examples 29674 Num true examples 31286\n",
      "  Batch 30,520  of  44,637.    Elapsed: 0:17:36. Training loss. 0.002822102513164282 Num fake examples 29713 Num true examples 31327\n",
      "  Batch 30,560  of  44,637.    Elapsed: 0:17:37. Training loss. 0.002898132661357522 Num fake examples 29750 Num true examples 31370\n",
      "  Batch 30,600  of  44,637.    Elapsed: 0:17:39. Training loss. 0.004171803127974272 Num fake examples 29793 Num true examples 31407\n",
      "  Batch 30,640  of  44,637.    Elapsed: 0:17:40. Training loss. 0.0037152734585106373 Num fake examples 29829 Num true examples 31451\n",
      "  Batch 30,680  of  44,637.    Elapsed: 0:17:41. Training loss. 0.003563515143468976 Num fake examples 29871 Num true examples 31489\n",
      "  Batch 30,720  of  44,637.    Elapsed: 0:17:43. Training loss. 0.0038048173300921917 Num fake examples 29907 Num true examples 31533\n",
      "  Batch 30,760  of  44,637.    Elapsed: 0:17:44. Training loss. 0.00317212613299489 Num fake examples 29944 Num true examples 31576\n",
      "  Batch 30,800  of  44,637.    Elapsed: 0:17:46. Training loss. 0.0040504345670342445 Num fake examples 29980 Num true examples 31620\n",
      "  Batch 30,840  of  44,637.    Elapsed: 0:17:47. Training loss. 0.0029606495518237352 Num fake examples 30022 Num true examples 31658\n",
      "  Batch 30,880  of  44,637.    Elapsed: 0:17:48. Training loss. 0.003485469613224268 Num fake examples 30063 Num true examples 31697\n",
      "  Batch 30,920  of  44,637.    Elapsed: 0:17:50. Training loss. 0.0029285666532814503 Num fake examples 30103 Num true examples 31737\n",
      "  Batch 30,960  of  44,637.    Elapsed: 0:17:51. Training loss. 0.0029367979150265455 Num fake examples 30146 Num true examples 31774\n",
      "  Batch 31,000  of  44,637.    Elapsed: 0:17:52. Training loss. 0.0029123194981366396 Num fake examples 30185 Num true examples 31815\n",
      "  Batch 31,040  of  44,637.    Elapsed: 0:17:54. Training loss. 0.0035979163367301226 Num fake examples 30215 Num true examples 31865\n",
      "  Batch 31,080  of  44,637.    Elapsed: 0:17:55. Training loss. 0.0027084732428193092 Num fake examples 30243 Num true examples 31917\n",
      "  Batch 31,120  of  44,637.    Elapsed: 0:17:57. Training loss. 0.0030713898595422506 Num fake examples 30277 Num true examples 31963\n",
      "  Batch 31,160  of  44,637.    Elapsed: 0:17:58. Training loss. 0.003401641733944416 Num fake examples 30315 Num true examples 32005\n",
      "  Batch 31,200  of  44,637.    Elapsed: 0:17:59. Training loss. 0.0037081409245729446 Num fake examples 30358 Num true examples 32042\n",
      "  Batch 31,240  of  44,637.    Elapsed: 0:18:01. Training loss. 0.0033098699059337378 Num fake examples 30408 Num true examples 32072\n",
      "  Batch 31,280  of  44,637.    Elapsed: 0:18:02. Training loss. 0.0031794330570846796 Num fake examples 30446 Num true examples 32114\n",
      "  Batch 31,320  of  44,637.    Elapsed: 0:18:03. Training loss. 0.0030598994344472885 Num fake examples 30486 Num true examples 32154\n",
      "  Batch 31,360  of  44,637.    Elapsed: 0:18:05. Training loss. 0.0029202643781900406 Num fake examples 30520 Num true examples 32200\n",
      "  Batch 31,400  of  44,637.    Elapsed: 0:18:06. Training loss. 0.002672400791198015 Num fake examples 30558 Num true examples 32242\n",
      "  Batch 31,440  of  44,637.    Elapsed: 0:18:08. Training loss. 0.0022121560759842396 Num fake examples 30599 Num true examples 32281\n",
      "  Batch 31,480  of  44,637.    Elapsed: 0:18:09. Training loss. 0.0033663567155599594 Num fake examples 30649 Num true examples 32311\n",
      "  Batch 31,520  of  44,637.    Elapsed: 0:18:10. Training loss. 0.0031753191724419594 Num fake examples 30690 Num true examples 32350\n",
      "  Batch 31,560  of  44,637.    Elapsed: 0:18:12. Training loss. 0.0032385489903390408 Num fake examples 30735 Num true examples 32385\n",
      "  Batch 31,600  of  44,637.    Elapsed: 0:18:13. Training loss. 0.004001533146947622 Num fake examples 30771 Num true examples 32429\n",
      "  Batch 31,640  of  44,637.    Elapsed: 0:18:14. Training loss. 0.004627309739589691 Num fake examples 30806 Num true examples 32474\n",
      "  Batch 31,680  of  44,637.    Elapsed: 0:18:16. Training loss. 2.951766014099121 Num fake examples 30840 Num true examples 32520\n",
      "  Batch 31,720  of  44,637.    Elapsed: 0:18:17. Training loss. 0.004937531426548958 Num fake examples 30884 Num true examples 32556\n",
      "  Batch 31,760  of  44,637.    Elapsed: 0:18:18. Training loss. 0.003862384706735611 Num fake examples 30914 Num true examples 32606\n",
      "  Batch 31,800  of  44,637.    Elapsed: 0:18:20. Training loss. 0.003433752804994583 Num fake examples 30965 Num true examples 32635\n",
      "  Batch 31,840  of  44,637.    Elapsed: 0:18:21. Training loss. 0.0032084202393889427 Num fake examples 31002 Num true examples 32678\n",
      "  Batch 31,880  of  44,637.    Elapsed: 0:18:23. Training loss. 0.0030289057176560163 Num fake examples 31051 Num true examples 32709\n",
      "  Batch 31,920  of  44,637.    Elapsed: 0:18:24. Training loss. 0.003237206256017089 Num fake examples 31089 Num true examples 32751\n",
      "  Batch 31,960  of  44,637.    Elapsed: 0:18:25. Training loss. 0.0030426247976720333 Num fake examples 31127 Num true examples 32793\n",
      "  Batch 32,000  of  44,637.    Elapsed: 0:18:27. Training loss. 0.0029743595514446497 Num fake examples 31168 Num true examples 32832\n",
      "  Batch 32,040  of  44,637.    Elapsed: 0:18:28. Training loss. 0.0032710637897253036 Num fake examples 31209 Num true examples 32871\n",
      "  Batch 32,080  of  44,637.    Elapsed: 0:18:29. Training loss. 0.0035777976736426353 Num fake examples 31241 Num true examples 32919\n",
      "  Batch 32,120  of  44,637.    Elapsed: 0:18:31. Training loss. 0.003969148732721806 Num fake examples 31283 Num true examples 32957\n",
      "  Batch 32,160  of  44,637.    Elapsed: 0:18:32. Training loss. 0.003672331338748336 Num fake examples 31317 Num true examples 33003\n",
      "  Batch 32,200  of  44,637.    Elapsed: 0:18:34. Training loss. 0.0033875168301165104 Num fake examples 31354 Num true examples 33046\n",
      "  Batch 32,240  of  44,637.    Elapsed: 0:18:35. Training loss. 0.003273207927122712 Num fake examples 31397 Num true examples 33083\n",
      "  Batch 32,280  of  44,637.    Elapsed: 0:18:36. Training loss. 0.0031730681657791138 Num fake examples 31436 Num true examples 33124\n",
      "  Batch 32,320  of  44,637.    Elapsed: 0:18:38. Training loss. 0.002945386804640293 Num fake examples 31471 Num true examples 33169\n",
      "  Batch 32,360  of  44,637.    Elapsed: 0:18:39. Training loss. 0.0035193879157304764 Num fake examples 31510 Num true examples 33210\n",
      "  Batch 32,400  of  44,637.    Elapsed: 0:18:40. Training loss. 0.0037750666961073875 Num fake examples 31552 Num true examples 33248\n",
      "  Batch 32,440  of  44,637.    Elapsed: 0:18:42. Training loss. 0.003521449863910675 Num fake examples 31592 Num true examples 33288\n",
      "  Batch 32,480  of  44,637.    Elapsed: 0:18:43. Training loss. 0.0036497279070317745 Num fake examples 31625 Num true examples 33335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 32,520  of  44,637.    Elapsed: 0:18:44. Training loss. 0.0037184078246355057 Num fake examples 31661 Num true examples 33379\n",
      "  Batch 32,560  of  44,637.    Elapsed: 0:18:46. Training loss. 0.0034297904931008816 Num fake examples 31703 Num true examples 33417\n",
      "  Batch 32,600  of  44,637.    Elapsed: 0:18:47. Training loss. 2.840902090072632 Num fake examples 31744 Num true examples 33456\n",
      "  Batch 32,640  of  44,637.    Elapsed: 0:18:48. Training loss. 0.0034844980109483004 Num fake examples 31781 Num true examples 33499\n",
      "  Batch 32,680  of  44,637.    Elapsed: 0:18:50. Training loss. 0.003379338886588812 Num fake examples 31815 Num true examples 33545\n",
      "  Batch 32,720  of  44,637.    Elapsed: 0:18:51. Training loss. 0.0033930581994354725 Num fake examples 31847 Num true examples 33593\n",
      "  Batch 32,760  of  44,637.    Elapsed: 0:18:52. Training loss. 0.004047050140798092 Num fake examples 31882 Num true examples 33638\n",
      "  Batch 32,800  of  44,637.    Elapsed: 0:18:54. Training loss. 0.0037760469131171703 Num fake examples 31923 Num true examples 33677\n",
      "  Batch 32,840  of  44,637.    Elapsed: 0:18:55. Training loss. 0.003962276503443718 Num fake examples 31968 Num true examples 33712\n",
      "  Batch 32,880  of  44,637.    Elapsed: 0:18:57. Training loss. 0.003302781842648983 Num fake examples 32006 Num true examples 33754\n",
      "  Batch 32,920  of  44,637.    Elapsed: 0:18:58. Training loss. 0.002759883413091302 Num fake examples 32051 Num true examples 33789\n",
      "  Batch 32,960  of  44,637.    Elapsed: 0:18:59. Training loss. 0.0030558041762560606 Num fake examples 32086 Num true examples 33834\n",
      "  Batch 33,000  of  44,637.    Elapsed: 0:19:01. Training loss. 0.003041429677978158 Num fake examples 32130 Num true examples 33870\n",
      "  Batch 33,040  of  44,637.    Elapsed: 0:19:02. Training loss. 0.0029310944955796003 Num fake examples 32169 Num true examples 33911\n",
      "  Batch 33,080  of  44,637.    Elapsed: 0:19:04. Training loss. 0.0035383945796638727 Num fake examples 32207 Num true examples 33953\n",
      "  Batch 33,120  of  44,637.    Elapsed: 0:19:05. Training loss. 0.0035738605074584484 Num fake examples 32248 Num true examples 33992\n",
      "  Batch 33,160  of  44,637.    Elapsed: 0:19:06. Training loss. 0.003984139300882816 Num fake examples 32291 Num true examples 34029\n",
      "  Batch 33,200  of  44,637.    Elapsed: 0:19:08. Training loss. 0.0035194563679397106 Num fake examples 32327 Num true examples 34073\n",
      "  Batch 33,240  of  44,637.    Elapsed: 0:19:09. Training loss. 0.0046587372198700905 Num fake examples 32360 Num true examples 34120\n",
      "  Batch 33,280  of  44,637.    Elapsed: 0:19:10. Training loss. 0.004048589151352644 Num fake examples 32393 Num true examples 34167\n",
      "  Batch 33,320  of  44,637.    Elapsed: 0:19:12. Training loss. 0.005351522006094456 Num fake examples 32425 Num true examples 34215\n",
      "  Batch 33,360  of  44,637.    Elapsed: 0:19:13. Training loss. 0.0035493834875524044 Num fake examples 32456 Num true examples 34264\n",
      "  Batch 33,400  of  44,637.    Elapsed: 0:19:15. Training loss. 0.005863368511199951 Num fake examples 32494 Num true examples 34306\n",
      "  Batch 33,440  of  44,637.    Elapsed: 0:19:16. Training loss. 0.0048692552372813225 Num fake examples 32528 Num true examples 34352\n",
      "  Batch 33,480  of  44,637.    Elapsed: 0:19:17. Training loss. 0.004601547960191965 Num fake examples 32568 Num true examples 34392\n",
      "  Batch 33,520  of  44,637.    Elapsed: 0:19:19. Training loss. 0.0030642044730484486 Num fake examples 32605 Num true examples 34435\n",
      "  Batch 33,560  of  44,637.    Elapsed: 0:19:20. Training loss. 0.004238350782543421 Num fake examples 32641 Num true examples 34479\n",
      "  Batch 33,600  of  44,637.    Elapsed: 0:19:21. Training loss. 0.002687673084437847 Num fake examples 32676 Num true examples 34524\n",
      "  Batch 33,640  of  44,637.    Elapsed: 0:19:23. Training loss. 0.0035350555554032326 Num fake examples 32715 Num true examples 34565\n",
      "  Batch 33,680  of  44,637.    Elapsed: 0:19:24. Training loss. 0.002849332056939602 Num fake examples 32751 Num true examples 34609\n",
      "  Batch 33,720  of  44,637.    Elapsed: 0:19:26. Training loss. 0.0027062930166721344 Num fake examples 32785 Num true examples 34655\n",
      "  Batch 33,760  of  44,637.    Elapsed: 0:19:27. Training loss. 0.003183926921337843 Num fake examples 32823 Num true examples 34697\n",
      "  Batch 33,800  of  44,637.    Elapsed: 0:19:28. Training loss. 0.002592687029391527 Num fake examples 32863 Num true examples 34737\n",
      "  Batch 33,840  of  44,637.    Elapsed: 0:19:30. Training loss. 0.002371317707002163 Num fake examples 32904 Num true examples 34776\n",
      "  Batch 33,880  of  44,637.    Elapsed: 0:19:31. Training loss. 0.0023558461107313633 Num fake examples 32939 Num true examples 34821\n",
      "  Batch 33,920  of  44,637.    Elapsed: 0:19:32. Training loss. 0.0025584616232663393 Num fake examples 32984 Num true examples 34856\n",
      "  Batch 33,960  of  44,637.    Elapsed: 0:19:34. Training loss. 0.0021025193855166435 Num fake examples 33013 Num true examples 34907\n",
      "  Batch 34,000  of  44,637.    Elapsed: 0:19:35. Training loss. 0.0018599245231598616 Num fake examples 33054 Num true examples 34946\n",
      "  Batch 34,040  of  44,637.    Elapsed: 0:19:36. Training loss. 0.002623795997351408 Num fake examples 33090 Num true examples 34990\n",
      "  Batch 34,080  of  44,637.    Elapsed: 0:19:38. Training loss. 0.0034030296374112368 Num fake examples 33124 Num true examples 35036\n",
      "  Batch 34,120  of  44,637.    Elapsed: 0:19:39. Training loss. 0.0034374939277768135 Num fake examples 33163 Num true examples 35077\n",
      "  Batch 34,160  of  44,637.    Elapsed: 0:19:40. Training loss. 0.0032641924917697906 Num fake examples 33201 Num true examples 35119\n",
      "  Batch 34,200  of  44,637.    Elapsed: 0:19:42. Training loss. 0.0035868596751242876 Num fake examples 33233 Num true examples 35167\n",
      "  Batch 34,240  of  44,637.    Elapsed: 0:19:43. Training loss. 0.003507982473820448 Num fake examples 33274 Num true examples 35206\n",
      "  Batch 34,280  of  44,637.    Elapsed: 0:19:44. Training loss. 0.0028990868013352156 Num fake examples 33306 Num true examples 35254\n",
      "  Batch 34,320  of  44,637.    Elapsed: 0:19:46. Training loss. 0.0030503992456942797 Num fake examples 33343 Num true examples 35297\n",
      "  Batch 34,360  of  44,637.    Elapsed: 0:19:47. Training loss. 0.0036609533708542585 Num fake examples 33378 Num true examples 35342\n",
      "  Batch 34,400  of  44,637.    Elapsed: 0:19:49. Training loss. 0.0039132460951805115 Num fake examples 33419 Num true examples 35381\n",
      "  Batch 34,440  of  44,637.    Elapsed: 0:19:50. Training loss. 0.004110891837626696 Num fake examples 33461 Num true examples 35419\n",
      "  Batch 34,480  of  44,637.    Elapsed: 0:19:51. Training loss. 0.0037743132561445236 Num fake examples 33507 Num true examples 35453\n",
      "  Batch 34,520  of  44,637.    Elapsed: 0:19:53. Training loss. 0.0036645783111453056 Num fake examples 33541 Num true examples 35499\n",
      "  Batch 34,560  of  44,637.    Elapsed: 0:19:54. Training loss. 0.003607039572671056 Num fake examples 33581 Num true examples 35539\n",
      "  Batch 34,600  of  44,637.    Elapsed: 0:19:55. Training loss. 0.003925312776118517 Num fake examples 33621 Num true examples 35579\n",
      "  Batch 34,640  of  44,637.    Elapsed: 0:19:57. Training loss. 0.004448837134987116 Num fake examples 33656 Num true examples 35624\n",
      "  Batch 34,680  of  44,637.    Elapsed: 0:19:58. Training loss. 0.005244607105851173 Num fake examples 33699 Num true examples 35661\n",
      "  Batch 34,720  of  44,637.    Elapsed: 0:19:59. Training loss. 0.005017396528273821 Num fake examples 33737 Num true examples 35703\n",
      "  Batch 34,760  of  44,637.    Elapsed: 0:20:01. Training loss. 0.004430369008332491 Num fake examples 33772 Num true examples 35748\n",
      "  Batch 34,800  of  44,637.    Elapsed: 0:20:02. Training loss. 2.651021957397461 Num fake examples 33812 Num true examples 35788\n",
      "  Batch 34,840  of  44,637.    Elapsed: 0:20:03. Training loss. 0.004357145167887211 Num fake examples 33842 Num true examples 35838\n",
      "  Batch 34,880  of  44,637.    Elapsed: 0:20:05. Training loss. 0.003935808781534433 Num fake examples 33878 Num true examples 35882\n",
      "  Batch 34,920  of  44,637.    Elapsed: 0:20:06. Training loss. 0.004061903804540634 Num fake examples 33917 Num true examples 35923\n",
      "  Batch 34,960  of  44,637.    Elapsed: 0:20:08. Training loss. 0.004404345061630011 Num fake examples 33957 Num true examples 35963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 35,000  of  44,637.    Elapsed: 0:20:09. Training loss. 0.003488492453470826 Num fake examples 33998 Num true examples 36002\n",
      "  Batch 35,040  of  44,637.    Elapsed: 0:20:10. Training loss. 0.0033848010934889317 Num fake examples 34039 Num true examples 36041\n",
      "  Batch 35,080  of  44,637.    Elapsed: 0:20:12. Training loss. 0.0040693096816539764 Num fake examples 34075 Num true examples 36085\n",
      "  Batch 35,120  of  44,637.    Elapsed: 0:20:13. Training loss. 0.003434994723647833 Num fake examples 34119 Num true examples 36121\n",
      "  Batch 35,160  of  44,637.    Elapsed: 0:20:14. Training loss. 0.0034820889122784138 Num fake examples 34161 Num true examples 36159\n",
      "  Batch 35,200  of  44,637.    Elapsed: 0:20:16. Training loss. 0.0036317072808742523 Num fake examples 34198 Num true examples 36202\n",
      "  Batch 35,240  of  44,637.    Elapsed: 0:20:17. Training loss. 0.0033148033544421196 Num fake examples 34234 Num true examples 36246\n",
      "  Batch 35,280  of  44,637.    Elapsed: 0:20:18. Training loss. 0.0036312947049736977 Num fake examples 34275 Num true examples 36285\n",
      "  Batch 35,320  of  44,637.    Elapsed: 0:20:20. Training loss. 0.003572432789951563 Num fake examples 34314 Num true examples 36326\n",
      "  Batch 35,360  of  44,637.    Elapsed: 0:20:21. Training loss. 0.0033105944748967886 Num fake examples 34350 Num true examples 36370\n",
      "  Batch 35,400  of  44,637.    Elapsed: 0:20:23. Training loss. 0.004786237142980099 Num fake examples 34381 Num true examples 36419\n",
      "  Batch 35,440  of  44,637.    Elapsed: 0:20:24. Training loss. 0.003998572006821632 Num fake examples 34418 Num true examples 36462\n",
      "  Batch 35,480  of  44,637.    Elapsed: 0:20:25. Training loss. 0.003718435065820813 Num fake examples 34454 Num true examples 36506\n",
      "  Batch 35,520  of  44,637.    Elapsed: 0:20:27. Training loss. 0.004002503585070372 Num fake examples 34488 Num true examples 36552\n",
      "  Batch 35,560  of  44,637.    Elapsed: 0:20:28. Training loss. 0.005656955298036337 Num fake examples 34528 Num true examples 36592\n",
      "  Batch 35,600  of  44,637.    Elapsed: 0:20:29. Training loss. 0.004519033245742321 Num fake examples 34563 Num true examples 36637\n",
      "  Batch 35,640  of  44,637.    Elapsed: 0:20:31. Training loss. 0.004813943523913622 Num fake examples 34601 Num true examples 36679\n",
      "  Batch 35,680  of  44,637.    Elapsed: 0:20:32. Training loss. 0.0035700779408216476 Num fake examples 34631 Num true examples 36729\n",
      "  Batch 35,720  of  44,637.    Elapsed: 0:20:33. Training loss. 0.003922276198863983 Num fake examples 34665 Num true examples 36775\n",
      "  Batch 35,760  of  44,637.    Elapsed: 0:20:35. Training loss. 0.004831697791814804 Num fake examples 34706 Num true examples 36814\n",
      "  Batch 35,800  of  44,637.    Elapsed: 0:20:36. Training loss. 2.6873486042022705 Num fake examples 34744 Num true examples 36856\n",
      "  Batch 35,840  of  44,637.    Elapsed: 0:20:37. Training loss. 0.0038126518484205008 Num fake examples 34792 Num true examples 36888\n",
      "  Batch 35,880  of  44,637.    Elapsed: 0:20:39. Training loss. 0.004939652048051357 Num fake examples 34834 Num true examples 36926\n",
      "  Batch 35,920  of  44,637.    Elapsed: 0:20:40. Training loss. 0.003857025410979986 Num fake examples 34873 Num true examples 36967\n",
      "  Batch 35,960  of  44,637.    Elapsed: 0:20:42. Training loss. 0.003270599991083145 Num fake examples 34912 Num true examples 37008\n",
      "  Batch 36,000  of  44,637.    Elapsed: 0:20:43. Training loss. 0.003448922885581851 Num fake examples 34952 Num true examples 37048\n",
      "  Batch 36,040  of  44,637.    Elapsed: 0:20:44. Training loss. 0.003581630066037178 Num fake examples 34989 Num true examples 37091\n",
      "  Batch 36,080  of  44,637.    Elapsed: 0:20:46. Training loss. 0.004233655985444784 Num fake examples 35029 Num true examples 37131\n",
      "  Batch 36,120  of  44,637.    Elapsed: 0:20:47. Training loss. 0.003927768208086491 Num fake examples 35064 Num true examples 37176\n",
      "  Batch 36,160  of  44,637.    Elapsed: 0:20:48. Training loss. 0.003374655731022358 Num fake examples 35109 Num true examples 37211\n",
      "  Batch 36,200  of  44,637.    Elapsed: 0:20:50. Training loss. 0.003621577052399516 Num fake examples 35145 Num true examples 37255\n",
      "  Batch 36,240  of  44,637.    Elapsed: 0:20:51. Training loss. 0.0038242312148213387 Num fake examples 35189 Num true examples 37291\n",
      "  Batch 36,280  of  44,637.    Elapsed: 0:20:52. Training loss. 0.002782363910228014 Num fake examples 35230 Num true examples 37330\n",
      "  Batch 36,320  of  44,637.    Elapsed: 0:20:54. Training loss. 0.003169478615745902 Num fake examples 35264 Num true examples 37376\n",
      "  Batch 36,360  of  44,637.    Elapsed: 0:20:55. Training loss. 0.0033777537755668163 Num fake examples 35300 Num true examples 37420\n",
      "  Batch 36,400  of  44,637.    Elapsed: 0:20:57. Training loss. 0.003380115143954754 Num fake examples 35335 Num true examples 37465\n",
      "  Batch 36,440  of  44,637.    Elapsed: 0:20:58. Training loss. 0.0034976985771209 Num fake examples 35384 Num true examples 37496\n",
      "  Batch 36,480  of  44,637.    Elapsed: 0:20:59. Training loss. 0.0038539746310561895 Num fake examples 35428 Num true examples 37532\n",
      "  Batch 36,520  of  44,637.    Elapsed: 0:21:01. Training loss. 0.0033875000663101673 Num fake examples 35461 Num true examples 37579\n",
      "  Batch 36,560  of  44,637.    Elapsed: 0:21:02. Training loss. 0.0035916322376579046 Num fake examples 35505 Num true examples 37615\n",
      "  Batch 36,600  of  44,637.    Elapsed: 0:21:03. Training loss. 0.0036876839585602283 Num fake examples 35541 Num true examples 37659\n",
      "  Batch 36,640  of  44,637.    Elapsed: 0:21:05. Training loss. 0.003915128763765097 Num fake examples 35576 Num true examples 37704\n",
      "  Batch 36,680  of  44,637.    Elapsed: 0:21:06. Training loss. 0.0035713762044906616 Num fake examples 35620 Num true examples 37740\n",
      "  Batch 36,720  of  44,637.    Elapsed: 0:21:07. Training loss. 0.003383417846634984 Num fake examples 35667 Num true examples 37773\n",
      "  Batch 36,760  of  44,637.    Elapsed: 0:21:09. Training loss. 0.0035802258644253016 Num fake examples 35709 Num true examples 37811\n",
      "  Batch 36,800  of  44,637.    Elapsed: 0:21:10. Training loss. 0.003379369154572487 Num fake examples 35745 Num true examples 37855\n",
      "  Batch 36,840  of  44,637.    Elapsed: 0:21:12. Training loss. 0.003061356022953987 Num fake examples 35781 Num true examples 37899\n",
      "  Batch 36,880  of  44,637.    Elapsed: 0:21:13. Training loss. 0.003376620588824153 Num fake examples 35824 Num true examples 37936\n",
      "  Batch 36,920  of  44,637.    Elapsed: 0:21:14. Training loss. 0.0028344830498099327 Num fake examples 35867 Num true examples 37973\n",
      "  Batch 36,960  of  44,637.    Elapsed: 0:21:16. Training loss. 0.0031710942275822163 Num fake examples 35905 Num true examples 38015\n",
      "  Batch 37,000  of  44,637.    Elapsed: 0:21:17. Training loss. 0.003923653159290552 Num fake examples 35944 Num true examples 38056\n",
      "  Batch 37,040  of  44,637.    Elapsed: 0:21:18. Training loss. 0.00246945908293128 Num fake examples 35977 Num true examples 38103\n",
      "  Batch 37,080  of  44,637.    Elapsed: 0:21:20. Training loss. 0.002973392140120268 Num fake examples 36020 Num true examples 38140\n",
      "  Batch 37,120  of  44,637.    Elapsed: 0:21:21. Training loss. 0.004524361342191696 Num fake examples 36054 Num true examples 38186\n",
      "  Batch 37,160  of  44,637.    Elapsed: 0:21:22. Training loss. 0.00359800411388278 Num fake examples 36089 Num true examples 38231\n",
      "  Batch 37,200  of  44,637.    Elapsed: 0:21:24. Training loss. 0.004246171098202467 Num fake examples 36117 Num true examples 38283\n",
      "  Batch 37,240  of  44,637.    Elapsed: 0:21:25. Training loss. 0.004214506596326828 Num fake examples 36155 Num true examples 38325\n",
      "  Batch 37,280  of  44,637.    Elapsed: 0:21:27. Training loss. 0.004162371624261141 Num fake examples 36197 Num true examples 38363\n",
      "  Batch 37,320  of  44,637.    Elapsed: 0:21:28. Training loss. 0.0036572618409991264 Num fake examples 36234 Num true examples 38406\n",
      "  Batch 37,360  of  44,637.    Elapsed: 0:21:29. Training loss. 0.0036759371869266033 Num fake examples 36272 Num true examples 38448\n",
      "  Batch 37,400  of  44,637.    Elapsed: 0:21:31. Training loss. 0.0028902485501021147 Num fake examples 36310 Num true examples 38490\n",
      "  Batch 37,440  of  44,637.    Elapsed: 0:21:32. Training loss. 0.0031829767394810915 Num fake examples 36346 Num true examples 38534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 37,480  of  44,637.    Elapsed: 0:21:33. Training loss. 0.0028955063316971064 Num fake examples 36380 Num true examples 38580\n",
      "  Batch 37,520  of  44,637.    Elapsed: 0:21:35. Training loss. 0.0036180256865918636 Num fake examples 36423 Num true examples 38617\n",
      "  Batch 37,560  of  44,637.    Elapsed: 0:21:36. Training loss. 0.0018252701265737414 Num fake examples 36463 Num true examples 38657\n",
      "  Batch 37,600  of  44,637.    Elapsed: 0:21:37. Training loss. 0.0029108698945492506 Num fake examples 36500 Num true examples 38700\n",
      "  Batch 37,640  of  44,637.    Elapsed: 0:21:39. Training loss. 0.0028363827150315046 Num fake examples 36544 Num true examples 38736\n",
      "  Batch 37,680  of  44,637.    Elapsed: 0:21:40. Training loss. 0.003225734457373619 Num fake examples 36581 Num true examples 38779\n",
      "  Batch 37,720  of  44,637.    Elapsed: 0:21:41. Training loss. 0.002677041105926037 Num fake examples 36618 Num true examples 38822\n",
      "  Batch 37,760  of  44,637.    Elapsed: 0:21:43. Training loss. 0.0029236190021038055 Num fake examples 36656 Num true examples 38864\n",
      "  Batch 37,800  of  44,637.    Elapsed: 0:21:44. Training loss. 0.002569307805970311 Num fake examples 36692 Num true examples 38908\n",
      "  Batch 37,840  of  44,637.    Elapsed: 0:21:46. Training loss. 0.0029866257682442665 Num fake examples 36735 Num true examples 38945\n",
      "  Batch 37,880  of  44,637.    Elapsed: 0:21:47. Training loss. 0.0028680474497377872 Num fake examples 36771 Num true examples 38989\n",
      "  Batch 37,920  of  44,637.    Elapsed: 0:21:48. Training loss. 0.0023857876658439636 Num fake examples 36811 Num true examples 39029\n",
      "  Batch 37,960  of  44,637.    Elapsed: 0:21:50. Training loss. 0.0034278302919119596 Num fake examples 36854 Num true examples 39066\n",
      "  Batch 38,000  of  44,637.    Elapsed: 0:21:51. Training loss. 0.0031117466278374195 Num fake examples 36890 Num true examples 39110\n",
      "  Batch 38,040  of  44,637.    Elapsed: 0:21:52. Training loss. 2.934415340423584 Num fake examples 36933 Num true examples 39147\n",
      "  Batch 38,080  of  44,637.    Elapsed: 0:21:54. Training loss. 0.002958750119432807 Num fake examples 36970 Num true examples 39190\n",
      "  Batch 38,120  of  44,637.    Elapsed: 0:21:55. Training loss. 0.003342034760862589 Num fake examples 37024 Num true examples 39216\n",
      "  Batch 38,160  of  44,637.    Elapsed: 0:21:56. Training loss. 0.0026954447384923697 Num fake examples 37065 Num true examples 39255\n",
      "  Batch 38,200  of  44,637.    Elapsed: 0:21:58. Training loss. 0.002644591499119997 Num fake examples 37100 Num true examples 39300\n",
      "  Batch 38,240  of  44,637.    Elapsed: 0:21:59. Training loss. 2.8644216060638428 Num fake examples 37135 Num true examples 39345\n",
      "  Batch 38,280  of  44,637.    Elapsed: 0:22:01. Training loss. 0.003624377306550741 Num fake examples 37171 Num true examples 39389\n",
      "  Batch 38,320  of  44,637.    Elapsed: 0:22:02. Training loss. 0.0036795821506530046 Num fake examples 37213 Num true examples 39427\n",
      "  Batch 38,360  of  44,637.    Elapsed: 0:22:03. Training loss. 0.003248346969485283 Num fake examples 37259 Num true examples 39461\n",
      "  Batch 38,400  of  44,637.    Elapsed: 0:22:05. Training loss. 2.7242515087127686 Num fake examples 37292 Num true examples 39508\n",
      "  Batch 38,440  of  44,637.    Elapsed: 0:22:06. Training loss. 0.003856151830404997 Num fake examples 37327 Num true examples 39553\n",
      "  Batch 38,480  of  44,637.    Elapsed: 0:22:07. Training loss. 0.004435122944414616 Num fake examples 37375 Num true examples 39585\n",
      "  Batch 38,520  of  44,637.    Elapsed: 0:22:09. Training loss. 2.731865167617798 Num fake examples 37418 Num true examples 39622\n",
      "  Batch 38,560  of  44,637.    Elapsed: 0:22:10. Training loss. 0.005582419689744711 Num fake examples 37456 Num true examples 39664\n",
      "  Batch 38,600  of  44,637.    Elapsed: 0:22:11. Training loss. 0.004990596789866686 Num fake examples 37494 Num true examples 39706\n",
      "  Batch 38,640  of  44,637.    Elapsed: 0:22:13. Training loss. 0.004145663697272539 Num fake examples 37534 Num true examples 39746\n",
      "  Batch 38,680  of  44,637.    Elapsed: 0:22:14. Training loss. 0.005289424676448107 Num fake examples 37565 Num true examples 39795\n",
      "  Batch 38,720  of  44,637.    Elapsed: 0:22:15. Training loss. 0.00414320221170783 Num fake examples 37606 Num true examples 39834\n",
      "  Batch 38,760  of  44,637.    Elapsed: 0:22:17. Training loss. 0.00425503496080637 Num fake examples 37643 Num true examples 39877\n",
      "  Batch 38,800  of  44,637.    Elapsed: 0:22:18. Training loss. 0.003725273534655571 Num fake examples 37681 Num true examples 39919\n",
      "  Batch 38,840  of  44,637.    Elapsed: 0:22:20. Training loss. 0.00501252431422472 Num fake examples 37712 Num true examples 39968\n",
      "  Batch 38,880  of  44,637.    Elapsed: 0:22:21. Training loss. 0.005319424904882908 Num fake examples 37752 Num true examples 40008\n",
      "  Batch 38,920  of  44,637.    Elapsed: 0:22:22. Training loss. 0.004817096050828695 Num fake examples 37787 Num true examples 40053\n",
      "  Batch 38,960  of  44,637.    Elapsed: 0:22:24. Training loss. 0.005659937858581543 Num fake examples 37827 Num true examples 40093\n",
      "  Batch 39,000  of  44,637.    Elapsed: 0:22:25. Training loss. 0.004502071999013424 Num fake examples 37872 Num true examples 40128\n",
      "  Batch 39,040  of  44,637.    Elapsed: 0:22:26. Training loss. 2.499234199523926 Num fake examples 37908 Num true examples 40172\n",
      "  Batch 39,080  of  44,637.    Elapsed: 0:22:28. Training loss. 2.6655890941619873 Num fake examples 37947 Num true examples 40213\n",
      "  Batch 39,120  of  44,637.    Elapsed: 0:22:29. Training loss. 0.003875146619975567 Num fake examples 37986 Num true examples 40254\n",
      "  Batch 39,160  of  44,637.    Elapsed: 0:22:30. Training loss. 0.004388830624520779 Num fake examples 38028 Num true examples 40292\n",
      "  Batch 39,200  of  44,637.    Elapsed: 0:22:32. Training loss. 0.004041183274239302 Num fake examples 38065 Num true examples 40335\n",
      "  Batch 39,240  of  44,637.    Elapsed: 0:22:33. Training loss. 0.0040877992287278175 Num fake examples 38097 Num true examples 40383\n",
      "  Batch 39,280  of  44,637.    Elapsed: 0:22:35. Training loss. 0.0041323499754071236 Num fake examples 38134 Num true examples 40426\n",
      "  Batch 39,320  of  44,637.    Elapsed: 0:22:36. Training loss. 0.0035088106524199247 Num fake examples 38173 Num true examples 40467\n",
      "  Batch 39,360  of  44,637.    Elapsed: 0:22:37. Training loss. 0.0036601005122065544 Num fake examples 38208 Num true examples 40512\n",
      "  Batch 39,400  of  44,637.    Elapsed: 0:22:39. Training loss. 0.0037475384306162596 Num fake examples 38250 Num true examples 40550\n",
      "  Batch 39,440  of  44,637.    Elapsed: 0:22:40. Training loss. 0.003722089808434248 Num fake examples 38295 Num true examples 40585\n",
      "  Batch 39,480  of  44,637.    Elapsed: 0:22:41. Training loss. 0.0033208359964191914 Num fake examples 38333 Num true examples 40627\n",
      "  Batch 39,520  of  44,637.    Elapsed: 0:22:43. Training loss. 2.8145077228546143 Num fake examples 38359 Num true examples 40681\n",
      "  Batch 39,560  of  44,637.    Elapsed: 0:22:44. Training loss. 0.0033829230815172195 Num fake examples 38399 Num true examples 40721\n",
      "  Batch 39,600  of  44,637.    Elapsed: 0:22:45. Training loss. 0.0043920669704675674 Num fake examples 38447 Num true examples 40753\n",
      "  Batch 39,640  of  44,637.    Elapsed: 0:22:47. Training loss. 0.004433812107890844 Num fake examples 38487 Num true examples 40793\n",
      "  Batch 39,680  of  44,637.    Elapsed: 0:22:48. Training loss. 0.0044127050787210464 Num fake examples 38537 Num true examples 40823\n",
      "  Batch 39,720  of  44,637.    Elapsed: 0:22:50. Training loss. 0.004557131789624691 Num fake examples 38581 Num true examples 40859\n",
      "  Batch 39,760  of  44,637.    Elapsed: 0:22:51. Training loss. 0.00439763069152832 Num fake examples 38624 Num true examples 40896\n",
      "  Batch 39,800  of  44,637.    Elapsed: 0:22:52. Training loss. 0.004486762452870607 Num fake examples 38667 Num true examples 40933\n",
      "  Batch 39,840  of  44,637.    Elapsed: 0:22:54. Training loss. 0.0037693893536925316 Num fake examples 38709 Num true examples 40971\n",
      "  Batch 39,880  of  44,637.    Elapsed: 0:22:55. Training loss. 0.004150667693465948 Num fake examples 38748 Num true examples 41012\n",
      "  Batch 39,920  of  44,637.    Elapsed: 0:22:56. Training loss. 0.004304223228245974 Num fake examples 38782 Num true examples 41058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 39,960  of  44,637.    Elapsed: 0:22:58. Training loss. 0.003981154412031174 Num fake examples 38820 Num true examples 41100\n",
      "  Batch 40,000  of  44,637.    Elapsed: 0:22:59. Training loss. 0.003903804812580347 Num fake examples 38857 Num true examples 41143\n",
      "  Batch 40,040  of  44,637.    Elapsed: 0:23:00. Training loss. 0.003031305270269513 Num fake examples 38895 Num true examples 41185\n",
      "  Batch 40,080  of  44,637.    Elapsed: 0:23:02. Training loss. 0.0035999740939587355 Num fake examples 38938 Num true examples 41222\n",
      "  Batch 40,120  of  44,637.    Elapsed: 0:23:03. Training loss. 0.003043015953153372 Num fake examples 38972 Num true examples 41268\n",
      "  Batch 40,160  of  44,637.    Elapsed: 0:23:04. Training loss. 0.0027947358321398497 Num fake examples 39014 Num true examples 41306\n",
      "  Batch 40,200  of  44,637.    Elapsed: 0:23:06. Training loss. 0.003532904665917158 Num fake examples 39050 Num true examples 41350\n",
      "  Batch 40,240  of  44,637.    Elapsed: 0:23:07. Training loss. 0.004325077868998051 Num fake examples 39087 Num true examples 41393\n",
      "  Batch 40,280  of  44,637.    Elapsed: 0:23:09. Training loss. 0.003968401346355677 Num fake examples 39127 Num true examples 41433\n",
      "  Batch 40,320  of  44,637.    Elapsed: 0:23:10. Training loss. 0.005436609033495188 Num fake examples 39164 Num true examples 41476\n",
      "  Batch 40,360  of  44,637.    Elapsed: 0:23:11. Training loss. 0.005130595061928034 Num fake examples 39207 Num true examples 41513\n",
      "  Batch 40,400  of  44,637.    Elapsed: 0:23:13. Training loss. 0.003998451866209507 Num fake examples 39244 Num true examples 41556\n",
      "  Batch 40,440  of  44,637.    Elapsed: 0:23:14. Training loss. 0.004899531602859497 Num fake examples 39278 Num true examples 41602\n",
      "  Batch 40,480  of  44,637.    Elapsed: 0:23:15. Training loss. 0.0036495330277830362 Num fake examples 39327 Num true examples 41633\n",
      "  Batch 40,520  of  44,637.    Elapsed: 0:23:17. Training loss. 0.0035552599001675844 Num fake examples 39372 Num true examples 41668\n",
      "  Batch 40,560  of  44,637.    Elapsed: 0:23:18. Training loss. 2.9342706203460693 Num fake examples 39425 Num true examples 41695\n",
      "  Batch 40,600  of  44,637.    Elapsed: 0:23:19. Training loss. 0.004049021750688553 Num fake examples 39469 Num true examples 41731\n",
      "  Batch 40,640  of  44,637.    Elapsed: 0:23:21. Training loss. 0.004056550562381744 Num fake examples 39501 Num true examples 41779\n",
      "  Batch 40,680  of  44,637.    Elapsed: 0:23:22. Training loss. 0.0040153623558580875 Num fake examples 39543 Num true examples 41817\n",
      "  Batch 40,720  of  44,637.    Elapsed: 0:23:23. Training loss. 0.004558498505502939 Num fake examples 39579 Num true examples 41861\n",
      "  Batch 40,760  of  44,637.    Elapsed: 0:23:25. Training loss. 0.003943904303014278 Num fake examples 39618 Num true examples 41902\n",
      "  Batch 40,800  of  44,637.    Elapsed: 0:23:26. Training loss. 0.003479043021798134 Num fake examples 39658 Num true examples 41942\n",
      "  Batch 40,840  of  44,637.    Elapsed: 0:23:28. Training loss. 0.005067530553787947 Num fake examples 39697 Num true examples 41983\n",
      "  Batch 40,880  of  44,637.    Elapsed: 0:23:29. Training loss. 0.003775309771299362 Num fake examples 39744 Num true examples 42016\n",
      "  Batch 40,920  of  44,637.    Elapsed: 0:23:30. Training loss. 2.7820475101470947 Num fake examples 39783 Num true examples 42057\n",
      "  Batch 40,960  of  44,637.    Elapsed: 0:23:32. Training loss. 0.003444806206971407 Num fake examples 39816 Num true examples 42104\n",
      "  Batch 41,000  of  44,637.    Elapsed: 0:23:33. Training loss. 0.00395461730659008 Num fake examples 39853 Num true examples 42147\n",
      "  Batch 41,040  of  44,637.    Elapsed: 0:23:34. Training loss. 0.004103613086044788 Num fake examples 39890 Num true examples 42190\n",
      "  Batch 41,080  of  44,637.    Elapsed: 0:23:36. Training loss. 2.776297092437744 Num fake examples 39934 Num true examples 42226\n",
      "  Batch 41,120  of  44,637.    Elapsed: 0:23:37. Training loss. 0.004004180897027254 Num fake examples 39967 Num true examples 42273\n",
      "  Batch 41,160  of  44,637.    Elapsed: 0:23:38. Training loss. 0.003457034705206752 Num fake examples 40006 Num true examples 42314\n",
      "  Batch 41,200  of  44,637.    Elapsed: 0:23:40. Training loss. 0.003672049380838871 Num fake examples 40050 Num true examples 42350\n",
      "  Batch 41,240  of  44,637.    Elapsed: 0:23:41. Training loss. 0.002990506589412689 Num fake examples 40084 Num true examples 42396\n",
      "  Batch 41,280  of  44,637.    Elapsed: 0:23:43. Training loss. 0.0038672746159136295 Num fake examples 40118 Num true examples 42442\n",
      "  Batch 41,320  of  44,637.    Elapsed: 0:23:44. Training loss. 0.0032485686242580414 Num fake examples 40160 Num true examples 42480\n",
      "  Batch 41,360  of  44,637.    Elapsed: 0:23:45. Training loss. 0.004040015395730734 Num fake examples 40191 Num true examples 42529\n",
      "  Batch 41,400  of  44,637.    Elapsed: 0:23:47. Training loss. 0.003702542744576931 Num fake examples 40226 Num true examples 42574\n",
      "  Batch 41,440  of  44,637.    Elapsed: 0:23:48. Training loss. 0.0041074808686971664 Num fake examples 40263 Num true examples 42617\n",
      "  Batch 41,480  of  44,637.    Elapsed: 0:23:49. Training loss. 0.0035976239014416933 Num fake examples 40307 Num true examples 42653\n",
      "  Batch 41,520  of  44,637.    Elapsed: 0:23:51. Training loss. 0.0033509419299662113 Num fake examples 40354 Num true examples 42686\n",
      "  Batch 41,560  of  44,637.    Elapsed: 0:23:52. Training loss. 0.003594639478251338 Num fake examples 40388 Num true examples 42732\n",
      "  Batch 41,600  of  44,637.    Elapsed: 0:23:53. Training loss. 0.004303296562284231 Num fake examples 40421 Num true examples 42779\n",
      "  Batch 41,640  of  44,637.    Elapsed: 0:23:55. Training loss. 0.003168664174154401 Num fake examples 40467 Num true examples 42813\n",
      "  Batch 41,680  of  44,637.    Elapsed: 0:23:56. Training loss. 0.0039278785698115826 Num fake examples 40506 Num true examples 42854\n",
      "  Batch 41,720  of  44,637.    Elapsed: 0:23:57. Training loss. 0.003337541827932 Num fake examples 40540 Num true examples 42900\n",
      "  Batch 41,760  of  44,637.    Elapsed: 0:23:59. Training loss. 0.0031336999963968992 Num fake examples 40584 Num true examples 42936\n",
      "  Batch 41,800  of  44,637.    Elapsed: 0:24:00. Training loss. 0.0035194004885852337 Num fake examples 40630 Num true examples 42970\n",
      "  Batch 41,840  of  44,637.    Elapsed: 0:24:02. Training loss. 0.0040749129839241505 Num fake examples 40666 Num true examples 43014\n",
      "  Batch 41,880  of  44,637.    Elapsed: 0:24:03. Training loss. 0.003585513448342681 Num fake examples 40705 Num true examples 43055\n",
      "  Batch 41,920  of  44,637.    Elapsed: 0:24:04. Training loss. 0.0040344372391700745 Num fake examples 40744 Num true examples 43096\n",
      "  Batch 41,960  of  44,637.    Elapsed: 0:24:06. Training loss. 0.003943996038287878 Num fake examples 40786 Num true examples 43134\n",
      "  Batch 42,000  of  44,637.    Elapsed: 0:24:07. Training loss. 0.003912965301424265 Num fake examples 40827 Num true examples 43173\n",
      "  Batch 42,040  of  44,637.    Elapsed: 0:24:08. Training loss. 0.0035764279309660196 Num fake examples 40859 Num true examples 43221\n",
      "  Batch 42,080  of  44,637.    Elapsed: 0:24:10. Training loss. 0.002912417985498905 Num fake examples 40901 Num true examples 43259\n",
      "  Batch 42,120  of  44,637.    Elapsed: 0:24:11. Training loss. 0.003729880088940263 Num fake examples 40946 Num true examples 43294\n",
      "  Batch 42,160  of  44,637.    Elapsed: 0:24:12. Training loss. 0.004764379933476448 Num fake examples 40981 Num true examples 43339\n",
      "  Batch 42,200  of  44,637.    Elapsed: 0:24:14. Training loss. 0.003077785950154066 Num fake examples 41013 Num true examples 43387\n",
      "  Batch 42,240  of  44,637.    Elapsed: 0:24:15. Training loss. 0.0036463411524891853 Num fake examples 41052 Num true examples 43428\n",
      "  Batch 42,280  of  44,637.    Elapsed: 0:24:16. Training loss. 0.004352849908173084 Num fake examples 41089 Num true examples 43471\n",
      "  Batch 42,320  of  44,637.    Elapsed: 0:24:18. Training loss. 2.7087013721466064 Num fake examples 41125 Num true examples 43515\n",
      "  Batch 42,360  of  44,637.    Elapsed: 0:24:19. Training loss. 0.003884789301082492 Num fake examples 41176 Num true examples 43544\n",
      "  Batch 42,400  of  44,637.    Elapsed: 0:24:21. Training loss. 0.004741585813462734 Num fake examples 41206 Num true examples 43594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 42,440  of  44,637.    Elapsed: 0:24:22. Training loss. 0.004611083772033453 Num fake examples 41257 Num true examples 43623\n",
      "  Batch 42,480  of  44,637.    Elapsed: 0:24:23. Training loss. 0.0038838335312902927 Num fake examples 41301 Num true examples 43659\n",
      "  Batch 42,520  of  44,637.    Elapsed: 0:24:25. Training loss. 0.003912129905074835 Num fake examples 41341 Num true examples 43699\n",
      "  Batch 42,560  of  44,637.    Elapsed: 0:24:26. Training loss. 0.004142396152019501 Num fake examples 41378 Num true examples 43742\n",
      "  Batch 42,600  of  44,637.    Elapsed: 0:24:27. Training loss. 0.003998486790806055 Num fake examples 41417 Num true examples 43783\n",
      "  Batch 42,640  of  44,637.    Elapsed: 0:24:29. Training loss. 0.004154810216277838 Num fake examples 41451 Num true examples 43829\n",
      "  Batch 42,680  of  44,637.    Elapsed: 0:24:30. Training loss. 0.0037347343750298023 Num fake examples 41491 Num true examples 43869\n",
      "  Batch 42,720  of  44,637.    Elapsed: 0:24:32. Training loss. 0.0038685526233166456 Num fake examples 41530 Num true examples 43910\n",
      "  Batch 42,760  of  44,637.    Elapsed: 0:24:33. Training loss. 0.00441286526620388 Num fake examples 41575 Num true examples 43945\n",
      "  Batch 42,800  of  44,637.    Elapsed: 0:24:34. Training loss. 0.004297522362321615 Num fake examples 41622 Num true examples 43978\n",
      "  Batch 42,840  of  44,637.    Elapsed: 0:24:36. Training loss. 0.0036437436938285828 Num fake examples 41663 Num true examples 44017\n",
      "  Batch 42,880  of  44,637.    Elapsed: 0:24:37. Training loss. 0.003565392689779401 Num fake examples 41703 Num true examples 44057\n",
      "  Batch 42,920  of  44,637.    Elapsed: 0:24:38. Training loss. 0.0038042988162487745 Num fake examples 41737 Num true examples 44103\n",
      "  Batch 42,960  of  44,637.    Elapsed: 0:24:40. Training loss. 0.003791489638388157 Num fake examples 41775 Num true examples 44145\n",
      "  Batch 43,000  of  44,637.    Elapsed: 0:24:41. Training loss. 0.003670460544526577 Num fake examples 41810 Num true examples 44190\n",
      "  Batch 43,040  of  44,637.    Elapsed: 0:24:43. Training loss. 0.003509389003738761 Num fake examples 41849 Num true examples 44231\n",
      "  Batch 43,080  of  44,637.    Elapsed: 0:24:44. Training loss. 0.0037692654877901077 Num fake examples 41892 Num true examples 44268\n",
      "  Batch 43,120  of  44,637.    Elapsed: 0:24:45. Training loss. 0.003215568605810404 Num fake examples 41936 Num true examples 44304\n",
      "  Batch 43,160  of  44,637.    Elapsed: 0:24:47. Training loss. 0.003961506765335798 Num fake examples 41974 Num true examples 44346\n",
      "  Batch 43,200  of  44,637.    Elapsed: 0:24:48. Training loss. 0.003426091279834509 Num fake examples 42017 Num true examples 44383\n",
      "  Batch 43,240  of  44,637.    Elapsed: 0:24:49. Training loss. 0.00282279122620821 Num fake examples 42055 Num true examples 44425\n",
      "  Batch 43,280  of  44,637.    Elapsed: 0:24:51. Training loss. 0.0033787244465202093 Num fake examples 42095 Num true examples 44465\n",
      "  Batch 43,320  of  44,637.    Elapsed: 0:24:52. Training loss. 0.0032505416311323643 Num fake examples 42132 Num true examples 44508\n",
      "  Batch 43,360  of  44,637.    Elapsed: 0:24:54. Training loss. 0.003022214397788048 Num fake examples 42169 Num true examples 44551\n",
      "  Batch 43,400  of  44,637.    Elapsed: 0:24:55. Training loss. 0.0028265309520065784 Num fake examples 42198 Num true examples 44602\n",
      "  Batch 43,440  of  44,637.    Elapsed: 0:24:56. Training loss. 0.003316345624625683 Num fake examples 42241 Num true examples 44639\n",
      "  Batch 43,480  of  44,637.    Elapsed: 0:24:58. Training loss. 0.0029624486342072487 Num fake examples 42280 Num true examples 44680\n",
      "  Batch 43,520  of  44,637.    Elapsed: 0:24:59. Training loss. 0.003524276427924633 Num fake examples 42319 Num true examples 44721\n",
      "  Batch 43,560  of  44,637.    Elapsed: 0:25:01. Training loss. 0.0031007889192551374 Num fake examples 42355 Num true examples 44765\n",
      "  Batch 43,600  of  44,637.    Elapsed: 0:25:02. Training loss. 0.0034075588919222355 Num fake examples 42389 Num true examples 44811\n",
      "  Batch 43,640  of  44,637.    Elapsed: 0:25:03. Training loss. 0.0032326369546353817 Num fake examples 42429 Num true examples 44851\n",
      "  Batch 43,680  of  44,637.    Elapsed: 0:25:05. Training loss. 0.003679259680211544 Num fake examples 42462 Num true examples 44898\n",
      "  Batch 43,720  of  44,637.    Elapsed: 0:25:06. Training loss. 0.0029724242631345987 Num fake examples 42507 Num true examples 44933\n",
      "  Batch 43,760  of  44,637.    Elapsed: 0:25:08. Training loss. 0.0030545673798769712 Num fake examples 42542 Num true examples 44978\n",
      "  Batch 43,800  of  44,637.    Elapsed: 0:25:09. Training loss. 0.003497255267575383 Num fake examples 42570 Num true examples 45030\n",
      "  Batch 43,840  of  44,637.    Elapsed: 0:25:10. Training loss. 0.0033329688012599945 Num fake examples 42610 Num true examples 45070\n",
      "  Batch 43,880  of  44,637.    Elapsed: 0:25:12. Training loss. 0.003977817948907614 Num fake examples 42649 Num true examples 45111\n",
      "  Batch 43,920  of  44,637.    Elapsed: 0:25:13. Training loss. 0.0038138278760015965 Num fake examples 42684 Num true examples 45156\n",
      "  Batch 43,960  of  44,637.    Elapsed: 0:25:14. Training loss. 0.003131709061563015 Num fake examples 42727 Num true examples 45193\n",
      "  Batch 44,000  of  44,637.    Elapsed: 0:25:16. Training loss. 0.002926587127149105 Num fake examples 42761 Num true examples 45239\n",
      "  Batch 44,040  of  44,637.    Elapsed: 0:25:17. Training loss. 0.003368015168234706 Num fake examples 42796 Num true examples 45284\n",
      "  Batch 44,080  of  44,637.    Elapsed: 0:25:19. Training loss. 0.0031624441035091877 Num fake examples 42832 Num true examples 45328\n",
      "  Batch 44,120  of  44,637.    Elapsed: 0:25:20. Training loss. 0.003550967900082469 Num fake examples 42868 Num true examples 45372\n",
      "  Batch 44,160  of  44,637.    Elapsed: 0:25:21. Training loss. 0.003174905199557543 Num fake examples 42905 Num true examples 45415\n",
      "  Batch 44,200  of  44,637.    Elapsed: 0:25:23. Training loss. 0.003242054022848606 Num fake examples 42947 Num true examples 45453\n",
      "  Batch 44,240  of  44,637.    Elapsed: 0:25:24. Training loss. 0.0035177634563297033 Num fake examples 42987 Num true examples 45493\n",
      "  Batch 44,280  of  44,637.    Elapsed: 0:25:26. Training loss. 2.920222043991089 Num fake examples 43024 Num true examples 45536\n",
      "  Batch 44,320  of  44,637.    Elapsed: 0:25:27. Training loss. 0.0027553318068385124 Num fake examples 43058 Num true examples 45582\n",
      "  Batch 44,360  of  44,637.    Elapsed: 0:25:28. Training loss. 0.003669560421258211 Num fake examples 43090 Num true examples 45630\n",
      "  Batch 44,400  of  44,637.    Elapsed: 0:25:30. Training loss. 0.0031678935047239065 Num fake examples 43126 Num true examples 45674\n",
      "  Batch 44,440  of  44,637.    Elapsed: 0:25:31. Training loss. 0.002667461521923542 Num fake examples 43166 Num true examples 45714\n",
      "  Batch 44,480  of  44,637.    Elapsed: 0:25:32. Training loss. 0.0026473975740373135 Num fake examples 43205 Num true examples 45755\n",
      "  Batch 44,520  of  44,637.    Elapsed: 0:25:34. Training loss. 0.002607043832540512 Num fake examples 43241 Num true examples 45799\n",
      "  Batch 44,560  of  44,637.    Elapsed: 0:25:35. Training loss. 0.003193617332726717 Num fake examples 43275 Num true examples 45845\n",
      "  Batch 44,600  of  44,637.    Elapsed: 0:25:37. Training loss. 2.7699484825134277 Num fake examples 43314 Num true examples 45886\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epcoh took: 0:25:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "  Validation Loss: 0.19\n",
      "  Validation took: 0:01:54\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of  44,637.    Elapsed: 0:00:01. Training loss. 0.003468782640993595 Num fake examples 33 Num true examples 47\n",
      "  Batch    80  of  44,637.    Elapsed: 0:00:03. Training loss. 0.0033021150156855583 Num fake examples 72 Num true examples 88\n",
      "  Batch   120  of  44,637.    Elapsed: 0:00:04. Training loss. 0.0061979349702596664 Num fake examples 110 Num true examples 130\n",
      "  Batch   160  of  44,637.    Elapsed: 0:00:05. Training loss. 2.879258632659912 Num fake examples 156 Num true examples 164\n",
      "  Batch   200  of  44,637.    Elapsed: 0:00:07. Training loss. 0.00364861311390996 Num fake examples 195 Num true examples 205\n",
      "  Batch   240  of  44,637.    Elapsed: 0:00:08. Training loss. 0.0034166330005973577 Num fake examples 235 Num true examples 245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   280  of  44,637.    Elapsed: 0:00:09. Training loss. 0.003194661345332861 Num fake examples 266 Num true examples 294\n",
      "  Batch   320  of  44,637.    Elapsed: 0:00:11. Training loss. 0.005099664442241192 Num fake examples 306 Num true examples 334\n",
      "  Batch   360  of  44,637.    Elapsed: 0:00:12. Training loss. 0.004661020822823048 Num fake examples 347 Num true examples 373\n",
      "  Batch   400  of  44,637.    Elapsed: 0:00:13. Training loss. 0.0032441269140690565 Num fake examples 378 Num true examples 422\n",
      "  Batch   440  of  44,637.    Elapsed: 0:00:15. Training loss. 2.8121094703674316 Num fake examples 414 Num true examples 466\n",
      "  Batch   480  of  44,637.    Elapsed: 0:00:16. Training loss. 2.758197069168091 Num fake examples 454 Num true examples 506\n",
      "  Batch   520  of  44,637.    Elapsed: 0:00:17. Training loss. 0.004781815689057112 Num fake examples 495 Num true examples 545\n",
      "  Batch   560  of  44,637.    Elapsed: 0:00:19. Training loss. 0.0050916182808578014 Num fake examples 535 Num true examples 585\n",
      "  Batch   600  of  44,637.    Elapsed: 0:00:20. Training loss. 0.005438576452434063 Num fake examples 576 Num true examples 624\n",
      "  Batch   640  of  44,637.    Elapsed: 0:00:21. Training loss. 0.004212973639369011 Num fake examples 616 Num true examples 664\n",
      "  Batch   680  of  44,637.    Elapsed: 0:00:23. Training loss. 0.004578203894197941 Num fake examples 656 Num true examples 704\n",
      "  Batch   720  of  44,637.    Elapsed: 0:00:24. Training loss. 0.004786770325154066 Num fake examples 696 Num true examples 744\n",
      "  Batch   760  of  44,637.    Elapsed: 0:00:25. Training loss. 0.005280274897813797 Num fake examples 734 Num true examples 786\n",
      "  Batch   800  of  44,637.    Elapsed: 0:00:27. Training loss. 0.0044805752113461494 Num fake examples 783 Num true examples 817\n",
      "  Batch   840  of  44,637.    Elapsed: 0:00:28. Training loss. 0.003818419063463807 Num fake examples 817 Num true examples 863\n",
      "  Batch   880  of  44,637.    Elapsed: 0:00:29. Training loss. 0.004252161830663681 Num fake examples 853 Num true examples 907\n",
      "  Batch   920  of  44,637.    Elapsed: 0:00:31. Training loss. 2.681549310684204 Num fake examples 888 Num true examples 952\n",
      "  Batch   960  of  44,637.    Elapsed: 0:00:32. Training loss. 0.004535007290542126 Num fake examples 929 Num true examples 991\n",
      "  Batch 1,000  of  44,637.    Elapsed: 0:00:33. Training loss. 0.0037054913118481636 Num fake examples 965 Num true examples 1035\n",
      "  Batch 1,040  of  44,637.    Elapsed: 0:00:35. Training loss. 0.003871453693136573 Num fake examples 1011 Num true examples 1069\n",
      "  Batch 1,080  of  44,637.    Elapsed: 0:00:36. Training loss. 0.004236644133925438 Num fake examples 1045 Num true examples 1115\n",
      "  Batch 1,120  of  44,637.    Elapsed: 0:00:37. Training loss. 0.0038516498170793056 Num fake examples 1087 Num true examples 1153\n",
      "  Batch 1,160  of  44,637.    Elapsed: 0:00:39. Training loss. 0.004058317746967077 Num fake examples 1125 Num true examples 1195\n",
      "  Batch 1,200  of  44,637.    Elapsed: 0:00:40. Training loss. 0.004185880534350872 Num fake examples 1161 Num true examples 1239\n",
      "  Batch 1,240  of  44,637.    Elapsed: 0:00:41. Training loss. 0.003972116857767105 Num fake examples 1195 Num true examples 1285\n",
      "  Batch 1,280  of  44,637.    Elapsed: 0:00:43. Training loss. 0.0031403691973537207 Num fake examples 1235 Num true examples 1325\n",
      "  Batch 1,320  of  44,637.    Elapsed: 0:00:44. Training loss. 0.002784079872071743 Num fake examples 1270 Num true examples 1370\n",
      "  Batch 1,360  of  44,637.    Elapsed: 0:00:46. Training loss. 0.0038274102844297886 Num fake examples 1315 Num true examples 1405\n",
      "  Batch 1,400  of  44,637.    Elapsed: 0:00:47. Training loss. 0.0032042679376900196 Num fake examples 1350 Num true examples 1450\n",
      "  Batch 1,440  of  44,637.    Elapsed: 0:00:48. Training loss. 0.003993301652371883 Num fake examples 1385 Num true examples 1495\n",
      "  Batch 1,480  of  44,637.    Elapsed: 0:00:50. Training loss. 2.8420987129211426 Num fake examples 1416 Num true examples 1544\n",
      "  Batch 1,520  of  44,637.    Elapsed: 0:00:51. Training loss. 2.7869231700897217 Num fake examples 1461 Num true examples 1579\n",
      "  Batch 1,560  of  44,637.    Elapsed: 0:00:52. Training loss. 0.003358975052833557 Num fake examples 1504 Num true examples 1616\n",
      "  Batch 1,600  of  44,637.    Elapsed: 0:00:54. Training loss. 0.0035952783655375242 Num fake examples 1545 Num true examples 1655\n",
      "  Batch 1,640  of  44,637.    Elapsed: 0:00:55. Training loss. 0.0035686357878148556 Num fake examples 1582 Num true examples 1698\n",
      "  Batch 1,680  of  44,637.    Elapsed: 0:00:56. Training loss. 0.003518615150824189 Num fake examples 1619 Num true examples 1741\n",
      "  Batch 1,720  of  44,637.    Elapsed: 0:00:58. Training loss. 0.0037655741907656193 Num fake examples 1661 Num true examples 1779\n",
      "  Batch 1,760  of  44,637.    Elapsed: 0:00:59. Training loss. 0.0030701325740665197 Num fake examples 1705 Num true examples 1815\n",
      "  Batch 1,800  of  44,637.    Elapsed: 0:01:00. Training loss. 2.8642239570617676 Num fake examples 1745 Num true examples 1855\n",
      "  Batch 1,840  of  44,637.    Elapsed: 0:01:02. Training loss. 0.00383190531283617 Num fake examples 1774 Num true examples 1906\n",
      "  Batch 1,880  of  44,637.    Elapsed: 0:01:03. Training loss. 0.00381569960154593 Num fake examples 1816 Num true examples 1944\n",
      "  Batch 1,920  of  44,637.    Elapsed: 0:01:05. Training loss. 0.004006353206932545 Num fake examples 1855 Num true examples 1985\n",
      "  Batch 1,960  of  44,637.    Elapsed: 0:01:06. Training loss. 0.004875706508755684 Num fake examples 1890 Num true examples 2030\n",
      "  Batch 2,000  of  44,637.    Elapsed: 0:01:07. Training loss. 0.003633714746683836 Num fake examples 1932 Num true examples 2068\n",
      "  Batch 2,040  of  44,637.    Elapsed: 0:01:09. Training loss. 0.0037982661742717028 Num fake examples 1975 Num true examples 2105\n",
      "  Batch 2,080  of  44,637.    Elapsed: 0:01:10. Training loss. 0.0034099884796887636 Num fake examples 2010 Num true examples 2150\n",
      "  Batch 2,120  of  44,637.    Elapsed: 0:01:11. Training loss. 0.003476629965007305 Num fake examples 2056 Num true examples 2184\n",
      "  Batch 2,160  of  44,637.    Elapsed: 0:01:13. Training loss. 0.0039621638134121895 Num fake examples 2097 Num true examples 2223\n",
      "  Batch 2,200  of  44,637.    Elapsed: 0:01:14. Training loss. 0.0034342333674430847 Num fake examples 2135 Num true examples 2265\n",
      "  Batch 2,240  of  44,637.    Elapsed: 0:01:16. Training loss. 0.0032711687963455915 Num fake examples 2170 Num true examples 2310\n",
      "  Batch 2,280  of  44,637.    Elapsed: 0:01:17. Training loss. 0.004705799277871847 Num fake examples 2211 Num true examples 2349\n",
      "  Batch 2,320  of  44,637.    Elapsed: 0:01:18. Training loss. 0.0040504890494048595 Num fake examples 2250 Num true examples 2390\n",
      "  Batch 2,360  of  44,637.    Elapsed: 0:01:20. Training loss. 0.00438121659681201 Num fake examples 2290 Num true examples 2430\n",
      "  Batch 2,400  of  44,637.    Elapsed: 0:01:21. Training loss. 0.0047087473794817924 Num fake examples 2334 Num true examples 2466\n",
      "  Batch 2,440  of  44,637.    Elapsed: 0:01:22. Training loss. 0.003079166868701577 Num fake examples 2371 Num true examples 2509\n",
      "  Batch 2,480  of  44,637.    Elapsed: 0:01:24. Training loss. 0.003582979319617152 Num fake examples 2406 Num true examples 2554\n",
      "  Batch 2,520  of  44,637.    Elapsed: 0:01:25. Training loss. 0.004105048719793558 Num fake examples 2446 Num true examples 2594\n",
      "  Batch 2,560  of  44,637.    Elapsed: 0:01:27. Training loss. 0.0027071861550211906 Num fake examples 2482 Num true examples 2638\n",
      "  Batch 2,600  of  44,637.    Elapsed: 0:01:28. Training loss. 2.7898807525634766 Num fake examples 2516 Num true examples 2684\n",
      "  Batch 2,640  of  44,637.    Elapsed: 0:01:29. Training loss. 0.0037192655727267265 Num fake examples 2546 Num true examples 2734\n",
      "  Batch 2,680  of  44,637.    Elapsed: 0:01:31. Training loss. 2.7153708934783936 Num fake examples 2581 Num true examples 2779\n",
      "  Batch 2,720  of  44,637.    Elapsed: 0:01:32. Training loss. 0.004328686278313398 Num fake examples 2617 Num true examples 2823\n",
      "  Batch 2,760  of  44,637.    Elapsed: 0:01:33. Training loss. 0.004638285376131535 Num fake examples 2653 Num true examples 2867\n",
      "  Batch 2,800  of  44,637.    Elapsed: 0:01:35. Training loss. 0.004307050723582506 Num fake examples 2692 Num true examples 2908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2,840  of  44,637.    Elapsed: 0:01:36. Training loss. 0.00334569183178246 Num fake examples 2723 Num true examples 2957\n",
      "  Batch 2,880  of  44,637.    Elapsed: 0:01:38. Training loss. 2.795090675354004 Num fake examples 2762 Num true examples 2998\n",
      "  Batch 2,920  of  44,637.    Elapsed: 0:01:39. Training loss. 0.004975479561835527 Num fake examples 2808 Num true examples 3032\n",
      "  Batch 2,960  of  44,637.    Elapsed: 0:01:41. Training loss. 0.003844038350507617 Num fake examples 2842 Num true examples 3078\n",
      "  Batch 3,000  of  44,637.    Elapsed: 0:01:42. Training loss. 0.004250291734933853 Num fake examples 2877 Num true examples 3123\n",
      "  Batch 3,040  of  44,637.    Elapsed: 0:01:44. Training loss. 0.003931797109544277 Num fake examples 2913 Num true examples 3167\n",
      "  Batch 3,080  of  44,637.    Elapsed: 0:01:46. Training loss. 0.004272051155567169 Num fake examples 2954 Num true examples 3206\n",
      "  Batch 3,120  of  44,637.    Elapsed: 0:01:47. Training loss. 0.0042154304683208466 Num fake examples 2991 Num true examples 3249\n",
      "  Batch 3,160  of  44,637.    Elapsed: 0:01:49. Training loss. 0.0045690820552408695 Num fake examples 3028 Num true examples 3292\n",
      "  Batch 3,200  of  44,637.    Elapsed: 0:01:51. Training loss. 0.0036719199270009995 Num fake examples 3061 Num true examples 3339\n",
      "  Batch 3,240  of  44,637.    Elapsed: 0:01:52. Training loss. 0.004018999170511961 Num fake examples 3096 Num true examples 3384\n",
      "  Batch 3,280  of  44,637.    Elapsed: 0:01:54. Training loss. 0.0037268903106451035 Num fake examples 3138 Num true examples 3422\n",
      "  Batch 3,320  of  44,637.    Elapsed: 0:01:55. Training loss. 0.004600535146892071 Num fake examples 3184 Num true examples 3456\n",
      "  Batch 3,360  of  44,637.    Elapsed: 0:01:57. Training loss. 0.0051687611266970634 Num fake examples 3225 Num true examples 3495\n",
      "  Batch 3,400  of  44,637.    Elapsed: 0:01:59. Training loss. 0.0047082360833883286 Num fake examples 3262 Num true examples 3538\n",
      "  Batch 3,440  of  44,637.    Elapsed: 0:02:00. Training loss. 2.730750799179077 Num fake examples 3301 Num true examples 3579\n",
      "  Batch 3,480  of  44,637.    Elapsed: 0:02:01. Training loss. 0.003774639219045639 Num fake examples 3340 Num true examples 3620\n",
      "  Batch 3,520  of  44,637.    Elapsed: 0:02:03. Training loss. 0.004484925419092178 Num fake examples 3376 Num true examples 3664\n",
      "  Batch 3,560  of  44,637.    Elapsed: 0:02:04. Training loss. 0.004055701196193695 Num fake examples 3415 Num true examples 3705\n",
      "  Batch 3,600  of  44,637.    Elapsed: 0:02:06. Training loss. 0.0037226881831884384 Num fake examples 3456 Num true examples 3744\n",
      "  Batch 3,640  of  44,637.    Elapsed: 0:02:07. Training loss. 2.651606559753418 Num fake examples 3494 Num true examples 3786\n",
      "  Batch 3,680  of  44,637.    Elapsed: 0:02:09. Training loss. 0.004393583629280329 Num fake examples 3532 Num true examples 3828\n",
      "  Batch 3,720  of  44,637.    Elapsed: 0:02:10. Training loss. 0.0042738704942166805 Num fake examples 3575 Num true examples 3865\n",
      "  Batch 3,760  of  44,637.    Elapsed: 0:02:12. Training loss. 0.0039023999124765396 Num fake examples 3612 Num true examples 3908\n",
      "  Batch 3,800  of  44,637.    Elapsed: 0:02:13. Training loss. 0.0038345069624483585 Num fake examples 3647 Num true examples 3953\n",
      "  Batch 3,840  of  44,637.    Elapsed: 0:02:14. Training loss. 0.0034590959548950195 Num fake examples 3683 Num true examples 3997\n",
      "  Batch 3,880  of  44,637.    Elapsed: 0:02:16. Training loss. 0.0036110880319029093 Num fake examples 3717 Num true examples 4043\n",
      "  Batch 3,920  of  44,637.    Elapsed: 0:02:17. Training loss. 0.003567927982658148 Num fake examples 3757 Num true examples 4083\n",
      "  Batch 3,960  of  44,637.    Elapsed: 0:02:19. Training loss. 0.003495934186503291 Num fake examples 3795 Num true examples 4125\n",
      "  Batch 4,000  of  44,637.    Elapsed: 0:02:20. Training loss. 0.003640863113105297 Num fake examples 3832 Num true examples 4168\n",
      "  Batch 4,040  of  44,637.    Elapsed: 0:02:21. Training loss. 0.0033759893849492073 Num fake examples 3867 Num true examples 4213\n",
      "  Batch 4,080  of  44,637.    Elapsed: 0:02:23. Training loss. 0.004163161851465702 Num fake examples 3908 Num true examples 4252\n",
      "  Batch 4,120  of  44,637.    Elapsed: 0:02:24. Training loss. 0.003416662570089102 Num fake examples 3948 Num true examples 4292\n",
      "  Batch 4,160  of  44,637.    Elapsed: 0:02:25. Training loss. 0.003912989981472492 Num fake examples 3972 Num true examples 4348\n",
      "  Batch 4,200  of  44,637.    Elapsed: 0:02:27. Training loss. 0.0034302908461540937 Num fake examples 4007 Num true examples 4393\n",
      "  Batch 4,240  of  44,637.    Elapsed: 0:02:28. Training loss. 0.003831146052107215 Num fake examples 4048 Num true examples 4432\n",
      "  Batch 4,280  of  44,637.    Elapsed: 0:02:29. Training loss. 0.0047964537516236305 Num fake examples 4081 Num true examples 4479\n",
      "  Batch 4,320  of  44,637.    Elapsed: 0:02:31. Training loss. 0.003776900004595518 Num fake examples 4114 Num true examples 4526\n",
      "  Batch 4,360  of  44,637.    Elapsed: 0:02:32. Training loss. 0.0034526425879448652 Num fake examples 4150 Num true examples 4570\n",
      "  Batch 4,400  of  44,637.    Elapsed: 0:02:34. Training loss. 0.0033811142202466726 Num fake examples 4191 Num true examples 4609\n",
      "  Batch 4,440  of  44,637.    Elapsed: 0:02:35. Training loss. 0.004228905308991671 Num fake examples 4225 Num true examples 4655\n",
      "  Batch 4,480  of  44,637.    Elapsed: 0:02:36. Training loss. 0.0035367088858038187 Num fake examples 4254 Num true examples 4706\n",
      "  Batch 4,520  of  44,637.    Elapsed: 0:02:38. Training loss. 0.0028276536613702774 Num fake examples 4293 Num true examples 4747\n",
      "  Batch 4,560  of  44,637.    Elapsed: 0:02:39. Training loss. 0.0029906579293310642 Num fake examples 4330 Num true examples 4790\n",
      "  Batch 4,600  of  44,637.    Elapsed: 0:02:40. Training loss. 0.0027160276658833027 Num fake examples 4377 Num true examples 4823\n",
      "  Batch 4,640  of  44,637.    Elapsed: 0:02:42. Training loss. 0.003722663503140211 Num fake examples 4411 Num true examples 4869\n",
      "  Batch 4,680  of  44,637.    Elapsed: 0:02:43. Training loss. 0.004647234454751015 Num fake examples 4447 Num true examples 4913\n",
      "  Batch 4,720  of  44,637.    Elapsed: 0:02:45. Training loss. 0.0028812563978135586 Num fake examples 4486 Num true examples 4954\n",
      "  Batch 4,760  of  44,637.    Elapsed: 0:02:46. Training loss. 0.004090261645615101 Num fake examples 4528 Num true examples 4992\n",
      "  Batch 4,800  of  44,637.    Elapsed: 0:02:47. Training loss. 0.004194122739136219 Num fake examples 4567 Num true examples 5033\n",
      "  Batch 4,840  of  44,637.    Elapsed: 0:02:49. Training loss. 0.003869082313030958 Num fake examples 4605 Num true examples 5075\n",
      "  Batch 4,880  of  44,637.    Elapsed: 0:02:50. Training loss. 0.004756281618028879 Num fake examples 4642 Num true examples 5118\n",
      "  Batch 4,920  of  44,637.    Elapsed: 0:02:51. Training loss. 0.004064436070621014 Num fake examples 4684 Num true examples 5156\n",
      "  Batch 4,960  of  44,637.    Elapsed: 0:02:53. Training loss. 0.003893204964697361 Num fake examples 4715 Num true examples 5205\n",
      "  Batch 5,000  of  44,637.    Elapsed: 0:02:54. Training loss. 0.005151277408003807 Num fake examples 4752 Num true examples 5248\n",
      "  Batch 5,040  of  44,637.    Elapsed: 0:02:56. Training loss. 0.003694270271807909 Num fake examples 4785 Num true examples 5295\n",
      "  Batch 5,080  of  44,637.    Elapsed: 0:02:57. Training loss. 0.00373541540466249 Num fake examples 4826 Num true examples 5334\n",
      "  Batch 5,120  of  44,637.    Elapsed: 0:02:58. Training loss. 0.003775817109271884 Num fake examples 4857 Num true examples 5383\n",
      "  Batch 5,160  of  44,637.    Elapsed: 0:03:00. Training loss. 0.0031907344236969948 Num fake examples 4892 Num true examples 5428\n",
      "  Batch 5,200  of  44,637.    Elapsed: 0:03:01. Training loss. 0.004276926629245281 Num fake examples 4940 Num true examples 5460\n",
      "  Batch 5,240  of  44,637.    Elapsed: 0:03:03. Training loss. 0.003776735160499811 Num fake examples 4987 Num true examples 5493\n",
      "  Batch 5,280  of  44,637.    Elapsed: 0:03:04. Training loss. 0.0033461693674325943 Num fake examples 5028 Num true examples 5532\n",
      "  Batch 5,320  of  44,637.    Elapsed: 0:03:05. Training loss. 0.003734864993020892 Num fake examples 5068 Num true examples 5572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,360  of  44,637.    Elapsed: 0:03:07. Training loss. 0.004083877429366112 Num fake examples 5105 Num true examples 5615\n",
      "  Batch 5,400  of  44,637.    Elapsed: 0:03:08. Training loss. 0.0035773231647908688 Num fake examples 5145 Num true examples 5655\n",
      "  Batch 5,440  of  44,637.    Elapsed: 0:03:09. Training loss. 0.003989210352301598 Num fake examples 5186 Num true examples 5694\n",
      "  Batch 5,480  of  44,637.    Elapsed: 0:03:11. Training loss. 0.004380741622298956 Num fake examples 5228 Num true examples 5732\n",
      "  Batch 5,520  of  44,637.    Elapsed: 0:03:12. Training loss. 0.003974552731961012 Num fake examples 5272 Num true examples 5768\n",
      "  Batch 5,560  of  44,637.    Elapsed: 0:03:13. Training loss. 0.003258579410612583 Num fake examples 5311 Num true examples 5809\n",
      "  Batch 5,600  of  44,637.    Elapsed: 0:03:15. Training loss. 0.003856887575238943 Num fake examples 5352 Num true examples 5848\n",
      "  Batch 5,640  of  44,637.    Elapsed: 0:03:16. Training loss. 0.004083923064172268 Num fake examples 5398 Num true examples 5882\n",
      "  Batch 5,680  of  44,637.    Elapsed: 0:03:18. Training loss. 0.0035824934020638466 Num fake examples 5436 Num true examples 5924\n",
      "  Batch 5,720  of  44,637.    Elapsed: 0:03:19. Training loss. 0.003832499496638775 Num fake examples 5475 Num true examples 5965\n",
      "  Batch 5,760  of  44,637.    Elapsed: 0:03:20. Training loss. 0.003430432640016079 Num fake examples 5508 Num true examples 6012\n",
      "  Batch 5,800  of  44,637.    Elapsed: 0:03:22. Training loss. 0.0038330741226673126 Num fake examples 5545 Num true examples 6055\n",
      "  Batch 5,840  of  44,637.    Elapsed: 0:03:23. Training loss. 0.003099502297118306 Num fake examples 5571 Num true examples 6109\n",
      "  Batch 5,880  of  44,637.    Elapsed: 0:03:24. Training loss. 0.00305578694678843 Num fake examples 5612 Num true examples 6148\n",
      "  Batch 5,920  of  44,637.    Elapsed: 0:03:26. Training loss. 0.0029670840594917536 Num fake examples 5645 Num true examples 6195\n",
      "  Batch 5,960  of  44,637.    Elapsed: 0:03:27. Training loss. 0.0028977643232792616 Num fake examples 5681 Num true examples 6239\n",
      "  Batch 6,000  of  44,637.    Elapsed: 0:03:29. Training loss. 0.004016133025288582 Num fake examples 5724 Num true examples 6276\n",
      "  Batch 6,040  of  44,637.    Elapsed: 0:03:30. Training loss. 0.0031257006339728832 Num fake examples 5764 Num true examples 6316\n",
      "  Batch 6,080  of  44,637.    Elapsed: 0:03:31. Training loss. 0.003211180679500103 Num fake examples 5802 Num true examples 6358\n",
      "  Batch 6,120  of  44,637.    Elapsed: 0:03:33. Training loss. 0.002590940799564123 Num fake examples 5840 Num true examples 6400\n",
      "  Batch 6,160  of  44,637.    Elapsed: 0:03:34. Training loss. 0.002756909467279911 Num fake examples 5881 Num true examples 6439\n",
      "  Batch 6,200  of  44,637.    Elapsed: 0:03:36. Training loss. 0.002729465253651142 Num fake examples 5920 Num true examples 6480\n",
      "  Batch 6,240  of  44,637.    Elapsed: 0:03:37. Training loss. 0.0029237321577966213 Num fake examples 5953 Num true examples 6527\n",
      "  Batch 6,280  of  44,637.    Elapsed: 0:03:38. Training loss. 0.0027670739218592644 Num fake examples 5999 Num true examples 6561\n",
      "  Batch 6,320  of  44,637.    Elapsed: 0:03:40. Training loss. 0.002585104200989008 Num fake examples 6040 Num true examples 6600\n",
      "  Batch 6,360  of  44,637.    Elapsed: 0:03:41. Training loss. 2.9012415409088135 Num fake examples 6077 Num true examples 6643\n",
      "  Batch 6,400  of  44,637.    Elapsed: 0:03:42. Training loss. 0.0032418416813015938 Num fake examples 6121 Num true examples 6679\n",
      "  Batch 6,440  of  44,637.    Elapsed: 0:03:44. Training loss. 0.002907950896769762 Num fake examples 6156 Num true examples 6724\n",
      "  Batch 6,480  of  44,637.    Elapsed: 0:03:45. Training loss. 0.0035638276021927595 Num fake examples 6195 Num true examples 6765\n",
      "  Batch 6,520  of  44,637.    Elapsed: 0:03:47. Training loss. 0.003309574443846941 Num fake examples 6236 Num true examples 6804\n",
      "  Batch 6,560  of  44,637.    Elapsed: 0:03:48. Training loss. 0.00277914572507143 Num fake examples 6282 Num true examples 6838\n",
      "  Batch 6,600  of  44,637.    Elapsed: 0:03:49. Training loss. 0.003662919392809272 Num fake examples 6317 Num true examples 6883\n",
      "  Batch 6,640  of  44,637.    Elapsed: 0:03:51. Training loss. 0.0030808376614004374 Num fake examples 6356 Num true examples 6924\n",
      "  Batch 6,680  of  44,637.    Elapsed: 0:03:52. Training loss. 0.003516246099025011 Num fake examples 6398 Num true examples 6962\n",
      "  Batch 6,720  of  44,637.    Elapsed: 0:03:53. Training loss. 0.0034333220683038235 Num fake examples 6428 Num true examples 7012\n",
      "  Batch 6,760  of  44,637.    Elapsed: 0:03:55. Training loss. 0.0030935676768422127 Num fake examples 6460 Num true examples 7060\n",
      "  Batch 6,800  of  44,637.    Elapsed: 0:03:56. Training loss. 0.0036648260429501534 Num fake examples 6489 Num true examples 7111\n",
      "  Batch 6,840  of  44,637.    Elapsed: 0:03:58. Training loss. 0.00318553252145648 Num fake examples 6531 Num true examples 7149\n",
      "  Batch 6,880  of  44,637.    Elapsed: 0:03:59. Training loss. 0.0030742548406124115 Num fake examples 6564 Num true examples 7196\n",
      "  Batch 6,920  of  44,637.    Elapsed: 0:04:00. Training loss. 0.0035160647239536047 Num fake examples 6613 Num true examples 7227\n",
      "  Batch 6,960  of  44,637.    Elapsed: 0:04:02. Training loss. 0.002777733141556382 Num fake examples 6653 Num true examples 7267\n",
      "  Batch 7,000  of  44,637.    Elapsed: 0:04:03. Training loss. 0.0034279641695320606 Num fake examples 6696 Num true examples 7304\n",
      "  Batch 7,040  of  44,637.    Elapsed: 0:04:04. Training loss. 0.002895743353292346 Num fake examples 6733 Num true examples 7347\n",
      "  Batch 7,080  of  44,637.    Elapsed: 0:04:06. Training loss. 0.0029214327223598957 Num fake examples 6771 Num true examples 7389\n",
      "  Batch 7,120  of  44,637.    Elapsed: 0:04:07. Training loss. 0.003824995132163167 Num fake examples 6811 Num true examples 7429\n",
      "  Batch 7,160  of  44,637.    Elapsed: 0:04:09. Training loss. 0.0032503418624401093 Num fake examples 6858 Num true examples 7462\n",
      "  Batch 7,200  of  44,637.    Elapsed: 0:04:10. Training loss. 0.0036552271340042353 Num fake examples 6891 Num true examples 7509\n",
      "  Batch 7,240  of  44,637.    Elapsed: 0:04:11. Training loss. 0.0031937151215970516 Num fake examples 6929 Num true examples 7551\n",
      "  Batch 7,280  of  44,637.    Elapsed: 0:04:13. Training loss. 0.0033326137345284224 Num fake examples 6964 Num true examples 7596\n",
      "  Batch 7,320  of  44,637.    Elapsed: 0:04:14. Training loss. 2.9192616939544678 Num fake examples 7000 Num true examples 7640\n",
      "  Batch 7,360  of  44,637.    Elapsed: 0:04:15. Training loss. 0.0036561633460223675 Num fake examples 7045 Num true examples 7675\n",
      "  Batch 7,400  of  44,637.    Elapsed: 0:04:17. Training loss. 0.0033405921421945095 Num fake examples 7084 Num true examples 7716\n",
      "  Batch 7,440  of  44,637.    Elapsed: 0:04:18. Training loss. 2.8660967350006104 Num fake examples 7117 Num true examples 7763\n",
      "  Batch 7,480  of  44,637.    Elapsed: 0:04:20. Training loss. 0.003007601946592331 Num fake examples 7156 Num true examples 7804\n",
      "  Batch 7,520  of  44,637.    Elapsed: 0:04:21. Training loss. 0.0036054535303264856 Num fake examples 7195 Num true examples 7845\n",
      "  Batch 7,560  of  44,637.    Elapsed: 0:04:22. Training loss. 0.003769224975258112 Num fake examples 7230 Num true examples 7890\n",
      "  Batch 7,600  of  44,637.    Elapsed: 0:04:24. Training loss. 0.003373487386852503 Num fake examples 7261 Num true examples 7939\n",
      "  Batch 7,640  of  44,637.    Elapsed: 0:04:25. Training loss. 0.003627275349572301 Num fake examples 7299 Num true examples 7981\n",
      "  Batch 7,680  of  44,637.    Elapsed: 0:04:26. Training loss. 0.0036867260932922363 Num fake examples 7341 Num true examples 8019\n",
      "  Batch 7,720  of  44,637.    Elapsed: 0:04:28. Training loss. 0.0027990720700472593 Num fake examples 7381 Num true examples 8059\n",
      "  Batch 7,760  of  44,637.    Elapsed: 0:04:29. Training loss. 0.003749205032363534 Num fake examples 7421 Num true examples 8099\n",
      "  Batch 7,800  of  44,637.    Elapsed: 0:04:31. Training loss. 0.00341191329061985 Num fake examples 7455 Num true examples 8145\n",
      "  Batch 7,840  of  44,637.    Elapsed: 0:04:32. Training loss. 0.0038221103604882956 Num fake examples 7499 Num true examples 8181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 7,880  of  44,637.    Elapsed: 0:04:33. Training loss. 0.004208666738122702 Num fake examples 7537 Num true examples 8223\n",
      "  Batch 7,920  of  44,637.    Elapsed: 0:04:35. Training loss. 0.003236562479287386 Num fake examples 7567 Num true examples 8273\n",
      "  Batch 7,960  of  44,637.    Elapsed: 0:04:36. Training loss. 0.003239500103518367 Num fake examples 7605 Num true examples 8315\n",
      "  Batch 8,000  of  44,637.    Elapsed: 0:04:37. Training loss. 0.004199416376650333 Num fake examples 7648 Num true examples 8352\n",
      "  Batch 8,040  of  44,637.    Elapsed: 0:04:39. Training loss. 0.0034196830820292234 Num fake examples 7677 Num true examples 8403\n",
      "  Batch 8,080  of  44,637.    Elapsed: 0:04:40. Training loss. 0.00335519528016448 Num fake examples 7722 Num true examples 8438\n",
      "  Batch 8,120  of  44,637.    Elapsed: 0:04:42. Training loss. 0.00406300276517868 Num fake examples 7762 Num true examples 8478\n",
      "  Batch 8,160  of  44,637.    Elapsed: 0:04:43. Training loss. 0.003345048986375332 Num fake examples 7800 Num true examples 8520\n",
      "  Batch 8,200  of  44,637.    Elapsed: 0:04:44. Training loss. 0.003765174187719822 Num fake examples 7843 Num true examples 8557\n",
      "  Batch 8,240  of  44,637.    Elapsed: 0:04:46. Training loss. 0.0039034588262438774 Num fake examples 7881 Num true examples 8599\n",
      "  Batch 8,280  of  44,637.    Elapsed: 0:04:47. Training loss. 0.003995377570390701 Num fake examples 7917 Num true examples 8643\n",
      "  Batch 8,320  of  44,637.    Elapsed: 0:04:48. Training loss. 0.0041359663009643555 Num fake examples 7953 Num true examples 8687\n",
      "  Batch 8,360  of  44,637.    Elapsed: 0:04:50. Training loss. 0.0036474335938692093 Num fake examples 7998 Num true examples 8722\n",
      "  Batch 8,400  of  44,637.    Elapsed: 0:04:51. Training loss. 0.003268061438575387 Num fake examples 8029 Num true examples 8771\n",
      "  Batch 8,440  of  44,637.    Elapsed: 0:04:52. Training loss. 0.004079772625118494 Num fake examples 8073 Num true examples 8807\n",
      "  Batch 8,480  of  44,637.    Elapsed: 0:04:54. Training loss. 0.0037162834778428078 Num fake examples 8113 Num true examples 8847\n",
      "  Batch 8,520  of  44,637.    Elapsed: 0:04:55. Training loss. 0.003364700125530362 Num fake examples 8149 Num true examples 8891\n",
      "  Batch 8,560  of  44,637.    Elapsed: 0:04:56. Training loss. 2.921778917312622 Num fake examples 8182 Num true examples 8938\n",
      "  Batch 8,600  of  44,637.    Elapsed: 0:04:58. Training loss. 0.0039594704285264015 Num fake examples 8222 Num true examples 8978\n",
      "  Batch 8,640  of  44,637.    Elapsed: 0:04:59. Training loss. 0.0038702196907252073 Num fake examples 8263 Num true examples 9017\n",
      "  Batch 8,680  of  44,637.    Elapsed: 0:05:01. Training loss. 0.0038663248997181654 Num fake examples 8306 Num true examples 9054\n",
      "  Batch 8,720  of  44,637.    Elapsed: 0:05:02. Training loss. 0.0034185689873993397 Num fake examples 8340 Num true examples 9100\n",
      "  Batch 8,760  of  44,637.    Elapsed: 0:05:03. Training loss. 0.003048740327358246 Num fake examples 8385 Num true examples 9135\n",
      "  Batch 8,800  of  44,637.    Elapsed: 0:05:05. Training loss. 0.0033885613083839417 Num fake examples 8424 Num true examples 9176\n",
      "  Batch 8,840  of  44,637.    Elapsed: 0:05:06. Training loss. 0.003372699022293091 Num fake examples 8462 Num true examples 9218\n",
      "  Batch 8,880  of  44,637.    Elapsed: 0:05:07. Training loss. 0.0033781221136450768 Num fake examples 8505 Num true examples 9255\n",
      "  Batch 8,920  of  44,637.    Elapsed: 0:05:09. Training loss. 0.002889652969315648 Num fake examples 8541 Num true examples 9299\n",
      "  Batch 8,960  of  44,637.    Elapsed: 0:05:10. Training loss. 0.003453577868640423 Num fake examples 8577 Num true examples 9343\n",
      "  Batch 9,000  of  44,637.    Elapsed: 0:05:11. Training loss. 0.005211522802710533 Num fake examples 8621 Num true examples 9379\n",
      "  Batch 9,040  of  44,637.    Elapsed: 0:05:13. Training loss. 2.7554619312286377 Num fake examples 8659 Num true examples 9421\n",
      "  Batch 9,080  of  44,637.    Elapsed: 0:05:14. Training loss. 0.004044565372169018 Num fake examples 8700 Num true examples 9460\n",
      "  Batch 9,120  of  44,637.    Elapsed: 0:05:16. Training loss. 0.0035715820267796516 Num fake examples 8742 Num true examples 9498\n",
      "  Batch 9,160  of  44,637.    Elapsed: 0:05:17. Training loss. 0.005352689418941736 Num fake examples 8783 Num true examples 9537\n",
      "  Batch 9,200  of  44,637.    Elapsed: 0:05:18. Training loss. 0.0036851244512945414 Num fake examples 8837 Num true examples 9563\n",
      "  Batch 9,240  of  44,637.    Elapsed: 0:05:20. Training loss. 0.004301009234040976 Num fake examples 8873 Num true examples 9607\n",
      "  Batch 9,280  of  44,637.    Elapsed: 0:05:21. Training loss. 0.003973274491727352 Num fake examples 8909 Num true examples 9651\n",
      "  Batch 9,320  of  44,637.    Elapsed: 0:05:22. Training loss. 0.0038070338778197765 Num fake examples 8946 Num true examples 9694\n",
      "  Batch 9,360  of  44,637.    Elapsed: 0:05:24. Training loss. 0.0033092997036874294 Num fake examples 8981 Num true examples 9739\n",
      "  Batch 9,400  of  44,637.    Elapsed: 0:05:25. Training loss. 0.003627509344369173 Num fake examples 9021 Num true examples 9779\n",
      "  Batch 9,440  of  44,637.    Elapsed: 0:05:26. Training loss. 0.003484280314296484 Num fake examples 9059 Num true examples 9821\n",
      "  Batch 9,480  of  44,637.    Elapsed: 0:05:28. Training loss. 0.003618614748120308 Num fake examples 9101 Num true examples 9859\n",
      "  Batch 9,520  of  44,637.    Elapsed: 0:05:29. Training loss. 0.003234205534681678 Num fake examples 9141 Num true examples 9899\n",
      "  Batch 9,560  of  44,637.    Elapsed: 0:05:30. Training loss. 0.0034173864405602217 Num fake examples 9187 Num true examples 9933\n",
      "  Batch 9,600  of  44,637.    Elapsed: 0:05:32. Training loss. 0.003227963810786605 Num fake examples 9227 Num true examples 9973\n",
      "  Batch 9,640  of  44,637.    Elapsed: 0:05:33. Training loss. 2.845095634460449 Num fake examples 9261 Num true examples 10019\n",
      "  Batch 9,680  of  44,637.    Elapsed: 0:05:35. Training loss. 2.7672455310821533 Num fake examples 9299 Num true examples 10061\n",
      "  Batch 9,720  of  44,637.    Elapsed: 0:05:36. Training loss. 0.003662433009594679 Num fake examples 9333 Num true examples 10107\n",
      "  Batch 9,760  of  44,637.    Elapsed: 0:05:37. Training loss. 0.0032919817604124546 Num fake examples 9368 Num true examples 10152\n",
      "  Batch 9,800  of  44,637.    Elapsed: 0:05:39. Training loss. 0.003254420356824994 Num fake examples 9406 Num true examples 10194\n",
      "  Batch 9,840  of  44,637.    Elapsed: 0:05:40. Training loss. 0.004142181947827339 Num fake examples 9446 Num true examples 10234\n",
      "  Batch 9,880  of  44,637.    Elapsed: 0:05:41. Training loss. 0.004975111223757267 Num fake examples 9487 Num true examples 10273\n",
      "  Batch 9,920  of  44,637.    Elapsed: 0:05:43. Training loss. 2.660109043121338 Num fake examples 9525 Num true examples 10315\n",
      "  Batch 9,960  of  44,637.    Elapsed: 0:05:44. Training loss. 0.004233299754559994 Num fake examples 9569 Num true examples 10351\n",
      "  Batch 10,000  of  44,637.    Elapsed: 0:05:45. Training loss. 0.0040797083638608456 Num fake examples 9610 Num true examples 10390\n",
      "  Batch 10,040  of  44,637.    Elapsed: 0:05:47. Training loss. 0.003204128472134471 Num fake examples 9644 Num true examples 10436\n",
      "  Batch 10,080  of  44,637.    Elapsed: 0:05:48. Training loss. 0.003911356441676617 Num fake examples 9685 Num true examples 10475\n",
      "  Batch 10,120  of  44,637.    Elapsed: 0:05:50. Training loss. 0.0030393069609999657 Num fake examples 9723 Num true examples 10517\n",
      "  Batch 10,160  of  44,637.    Elapsed: 0:05:51. Training loss. 0.002835575258359313 Num fake examples 9763 Num true examples 10557\n",
      "  Batch 10,200  of  44,637.    Elapsed: 0:05:52. Training loss. 0.0038702706806361675 Num fake examples 9794 Num true examples 10606\n",
      "  Batch 10,240  of  44,637.    Elapsed: 0:05:54. Training loss. 0.003086366690695286 Num fake examples 9836 Num true examples 10644\n",
      "  Batch 10,280  of  44,637.    Elapsed: 0:05:55. Training loss. 0.003182095941156149 Num fake examples 9876 Num true examples 10684\n",
      "  Batch 10,320  of  44,637.    Elapsed: 0:05:56. Training loss. 0.0033414552453905344 Num fake examples 9917 Num true examples 10723\n",
      "  Batch 10,360  of  44,637.    Elapsed: 0:05:58. Training loss. 0.0034248563461005688 Num fake examples 9951 Num true examples 10769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10,400  of  44,637.    Elapsed: 0:05:59. Training loss. 0.003256595227867365 Num fake examples 9983 Num true examples 10817\n",
      "  Batch 10,440  of  44,637.    Elapsed: 0:06:01. Training loss. 0.0035906180273741484 Num fake examples 10026 Num true examples 10854\n",
      "  Batch 10,480  of  44,637.    Elapsed: 0:06:02. Training loss. 0.0031511960551142693 Num fake examples 10066 Num true examples 10894\n",
      "  Batch 10,520  of  44,637.    Elapsed: 0:06:03. Training loss. 2.9343814849853516 Num fake examples 10098 Num true examples 10942\n",
      "  Batch 10,560  of  44,637.    Elapsed: 0:06:05. Training loss. 0.003142360132187605 Num fake examples 10132 Num true examples 10988\n",
      "  Batch 10,600  of  44,637.    Elapsed: 0:06:06. Training loss. 0.002978648291900754 Num fake examples 10174 Num true examples 11026\n",
      "  Batch 10,640  of  44,637.    Elapsed: 0:06:08. Training loss. 0.002387796062976122 Num fake examples 10217 Num true examples 11063\n",
      "  Batch 10,680  of  44,637.    Elapsed: 0:06:09. Training loss. 0.0026386333629488945 Num fake examples 10259 Num true examples 11101\n",
      "  Batch 10,720  of  44,637.    Elapsed: 0:06:10. Training loss. 0.0018418715335428715 Num fake examples 10286 Num true examples 11154\n",
      "  Batch 10,760  of  44,637.    Elapsed: 0:06:12. Training loss. 0.002057210076600313 Num fake examples 10324 Num true examples 11196\n",
      "  Batch 10,800  of  44,637.    Elapsed: 0:06:13. Training loss. 0.002363978885114193 Num fake examples 10366 Num true examples 11234\n",
      "  Batch 10,840  of  44,637.    Elapsed: 0:06:15. Training loss. 0.00256528751924634 Num fake examples 10407 Num true examples 11273\n",
      "  Batch 10,880  of  44,637.    Elapsed: 0:06:16. Training loss. 0.0017948286840692163 Num fake examples 10438 Num true examples 11322\n",
      "  Batch 10,920  of  44,637.    Elapsed: 0:06:17. Training loss. 0.002387933898717165 Num fake examples 10480 Num true examples 11360\n",
      "  Batch 10,960  of  44,637.    Elapsed: 0:06:19. Training loss. 0.002106464933604002 Num fake examples 10524 Num true examples 11396\n",
      "  Batch 11,000  of  44,637.    Elapsed: 0:06:20. Training loss. 0.002038815524429083 Num fake examples 10564 Num true examples 11436\n",
      "  Batch 11,040  of  44,637.    Elapsed: 0:06:21. Training loss. 0.0027757035568356514 Num fake examples 10604 Num true examples 11476\n",
      "  Batch 11,080  of  44,637.    Elapsed: 0:06:23. Training loss. 0.00283373286947608 Num fake examples 10630 Num true examples 11530\n",
      "  Batch 11,120  of  44,637.    Elapsed: 0:06:24. Training loss. 0.0031690250616520643 Num fake examples 10671 Num true examples 11569\n",
      "  Batch 11,160  of  44,637.    Elapsed: 0:06:26. Training loss. 0.0018821160774677992 Num fake examples 10699 Num true examples 11621\n",
      "  Batch 11,200  of  44,637.    Elapsed: 0:06:27. Training loss. 0.0027049286291003227 Num fake examples 10744 Num true examples 11656\n",
      "  Batch 11,240  of  44,637.    Elapsed: 0:06:28. Training loss. 0.0029159211553633213 Num fake examples 10788 Num true examples 11692\n",
      "  Batch 11,280  of  44,637.    Elapsed: 0:06:30. Training loss. 0.0035703929606825113 Num fake examples 10829 Num true examples 11731\n",
      "  Batch 11,320  of  44,637.    Elapsed: 0:06:31. Training loss. 0.003770387265831232 Num fake examples 10868 Num true examples 11772\n",
      "  Batch 11,360  of  44,637.    Elapsed: 0:06:33. Training loss. 0.0037684855051338673 Num fake examples 10907 Num true examples 11813\n",
      "  Batch 11,400  of  44,637.    Elapsed: 0:06:34. Training loss. 0.0032992891501635313 Num fake examples 10945 Num true examples 11855\n",
      "  Batch 11,440  of  44,637.    Elapsed: 0:06:35. Training loss. 0.003061736933887005 Num fake examples 10981 Num true examples 11899\n",
      "  Batch 11,480  of  44,637.    Elapsed: 0:06:37. Training loss. 0.0032011880539357662 Num fake examples 11017 Num true examples 11943\n",
      "  Batch 11,520  of  44,637.    Elapsed: 0:06:38. Training loss. 2.8574843406677246 Num fake examples 11062 Num true examples 11978\n",
      "  Batch 11,560  of  44,637.    Elapsed: 0:06:39. Training loss. 0.0035262545570731163 Num fake examples 11102 Num true examples 12018\n",
      "  Batch 11,600  of  44,637.    Elapsed: 0:06:41. Training loss. 0.0032295070122927427 Num fake examples 11140 Num true examples 12060\n",
      "  Batch 11,640  of  44,637.    Elapsed: 0:06:42. Training loss. 0.0028514766599982977 Num fake examples 11177 Num true examples 12103\n",
      "  Batch 11,680  of  44,637.    Elapsed: 0:06:43. Training loss. 0.003068429185077548 Num fake examples 11219 Num true examples 12141\n",
      "  Batch 11,720  of  44,637.    Elapsed: 0:06:45. Training loss. 0.0032682237215340137 Num fake examples 11256 Num true examples 12184\n",
      "  Batch 11,760  of  44,637.    Elapsed: 0:06:46. Training loss. 2.8859987258911133 Num fake examples 11298 Num true examples 12222\n",
      "  Batch 11,800  of  44,637.    Elapsed: 0:06:48. Training loss. 0.003625418059527874 Num fake examples 11331 Num true examples 12269\n",
      "  Batch 11,840  of  44,637.    Elapsed: 0:06:49. Training loss. 0.003194109071046114 Num fake examples 11364 Num true examples 12316\n",
      "  Batch 11,880  of  44,637.    Elapsed: 0:06:50. Training loss. 0.0035305949859321117 Num fake examples 11411 Num true examples 12349\n",
      "  Batch 11,920  of  44,637.    Elapsed: 0:06:52. Training loss. 2.987976312637329 Num fake examples 11446 Num true examples 12394\n",
      "  Batch 11,960  of  44,637.    Elapsed: 0:06:53. Training loss. 0.0027412171475589275 Num fake examples 11494 Num true examples 12426\n",
      "  Batch 12,000  of  44,637.    Elapsed: 0:06:54. Training loss. 0.0026640943251550198 Num fake examples 11530 Num true examples 12470\n",
      "  Batch 12,040  of  44,637.    Elapsed: 0:06:56. Training loss. 0.003535593394190073 Num fake examples 11568 Num true examples 12512\n",
      "  Batch 12,080  of  44,637.    Elapsed: 0:06:57. Training loss. 0.003755614859983325 Num fake examples 11607 Num true examples 12553\n",
      "  Batch 12,120  of  44,637.    Elapsed: 0:06:58. Training loss. 0.0040718200616538525 Num fake examples 11646 Num true examples 12594\n",
      "  Batch 12,160  of  44,637.    Elapsed: 0:07:00. Training loss. 0.004175353329628706 Num fake examples 11684 Num true examples 12636\n",
      "  Batch 12,200  of  44,637.    Elapsed: 0:07:01. Training loss. 0.0044824350625276566 Num fake examples 11726 Num true examples 12674\n",
      "  Batch 12,240  of  44,637.    Elapsed: 0:07:03. Training loss. 0.003802328836172819 Num fake examples 11763 Num true examples 12717\n",
      "  Batch 12,280  of  44,637.    Elapsed: 0:07:04. Training loss. 0.003553981427103281 Num fake examples 11802 Num true examples 12758\n",
      "  Batch 12,320  of  44,637.    Elapsed: 0:07:05. Training loss. 0.0036483174189925194 Num fake examples 11841 Num true examples 12799\n",
      "  Batch 12,360  of  44,637.    Elapsed: 0:07:07. Training loss. 0.0032794284634292126 Num fake examples 11886 Num true examples 12834\n",
      "  Batch 12,400  of  44,637.    Elapsed: 0:07:08. Training loss. 0.003164425492286682 Num fake examples 11922 Num true examples 12878\n",
      "  Batch 12,440  of  44,637.    Elapsed: 0:07:09. Training loss. 0.0034857098944485188 Num fake examples 11960 Num true examples 12920\n",
      "  Batch 12,480  of  44,637.    Elapsed: 0:07:11. Training loss. 2.892585277557373 Num fake examples 11993 Num true examples 12967\n",
      "  Batch 12,520  of  44,637.    Elapsed: 0:07:12. Training loss. 2.773772716522217 Num fake examples 12030 Num true examples 13010\n",
      "  Batch 12,560  of  44,637.    Elapsed: 0:07:13. Training loss. 0.0040717045776546 Num fake examples 12069 Num true examples 13051\n",
      "  Batch 12,600  of  44,637.    Elapsed: 0:07:15. Training loss. 0.0036088840570300817 Num fake examples 12103 Num true examples 13097\n",
      "  Batch 12,640  of  44,637.    Elapsed: 0:07:16. Training loss. 0.003806980326771736 Num fake examples 12141 Num true examples 13139\n",
      "  Batch 12,680  of  44,637.    Elapsed: 0:07:17. Training loss. 0.0039766752161085606 Num fake examples 12174 Num true examples 13186\n",
      "  Batch 12,720  of  44,637.    Elapsed: 0:07:19. Training loss. 0.003944125957787037 Num fake examples 12217 Num true examples 13223\n",
      "  Batch 12,760  of  44,637.    Elapsed: 0:07:20. Training loss. 0.003536395262926817 Num fake examples 12249 Num true examples 13271\n",
      "  Batch 12,800  of  44,637.    Elapsed: 0:07:22. Training loss. 0.0035572764463722706 Num fake examples 12289 Num true examples 13311\n",
      "  Batch 12,840  of  44,637.    Elapsed: 0:07:23. Training loss. 0.004020045977085829 Num fake examples 12323 Num true examples 13357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 12,880  of  44,637.    Elapsed: 0:07:24. Training loss. 0.0037236043717712164 Num fake examples 12359 Num true examples 13401\n",
      "  Batch 12,920  of  44,637.    Elapsed: 0:07:26. Training loss. 2.819333076477051 Num fake examples 12397 Num true examples 13443\n",
      "  Batch 12,960  of  44,637.    Elapsed: 0:07:27. Training loss. 2.8255908489227295 Num fake examples 12430 Num true examples 13490\n",
      "  Batch 13,000  of  44,637.    Elapsed: 0:07:28. Training loss. 0.0038886258844286203 Num fake examples 12463 Num true examples 13537\n",
      "  Batch 13,040  of  44,637.    Elapsed: 0:07:30. Training loss. 0.004371829330921173 Num fake examples 12504 Num true examples 13576\n",
      "  Batch 13,080  of  44,637.    Elapsed: 0:07:31. Training loss. 0.003238759469240904 Num fake examples 12546 Num true examples 13614\n",
      "  Batch 13,120  of  44,637.    Elapsed: 0:07:32. Training loss. 0.004018479026854038 Num fake examples 12588 Num true examples 13652\n",
      "  Batch 13,160  of  44,637.    Elapsed: 0:07:34. Training loss. 0.0035323488991707563 Num fake examples 12627 Num true examples 13693\n",
      "  Batch 13,200  of  44,637.    Elapsed: 0:07:35. Training loss. 0.0032589188776910305 Num fake examples 12670 Num true examples 13730\n",
      "  Batch 13,240  of  44,637.    Elapsed: 0:07:37. Training loss. 0.003895742818713188 Num fake examples 12712 Num true examples 13768\n",
      "  Batch 13,280  of  44,637.    Elapsed: 0:07:38. Training loss. 0.0033160089515149593 Num fake examples 12749 Num true examples 13811\n",
      "  Batch 13,320  of  44,637.    Elapsed: 0:07:39. Training loss. 0.0033732797019183636 Num fake examples 12787 Num true examples 13853\n",
      "  Batch 13,360  of  44,637.    Elapsed: 0:07:41. Training loss. 0.0030683972872793674 Num fake examples 12826 Num true examples 13894\n",
      "  Batch 13,400  of  44,637.    Elapsed: 0:07:42. Training loss. 0.003404087619856 Num fake examples 12859 Num true examples 13941\n",
      "  Batch 13,440  of  44,637.    Elapsed: 0:07:43. Training loss. 0.0038689218927174807 Num fake examples 12889 Num true examples 13991\n",
      "  Batch 13,480  of  44,637.    Elapsed: 0:07:45. Training loss. 0.00338273448869586 Num fake examples 12931 Num true examples 14029\n",
      "  Batch 13,520  of  44,637.    Elapsed: 0:07:46. Training loss. 0.003421698696911335 Num fake examples 12973 Num true examples 14067\n",
      "  Batch 13,560  of  44,637.    Elapsed: 0:07:47. Training loss. 0.0029811011627316475 Num fake examples 13004 Num true examples 14116\n",
      "  Batch 13,600  of  44,637.    Elapsed: 0:07:49. Training loss. 0.0029095583595335484 Num fake examples 13039 Num true examples 14161\n",
      "  Batch 13,640  of  44,637.    Elapsed: 0:07:50. Training loss. 0.0036230222322046757 Num fake examples 13079 Num true examples 14201\n",
      "  Batch 13,680  of  44,637.    Elapsed: 0:07:52. Training loss. 0.003405739553272724 Num fake examples 13115 Num true examples 14245\n",
      "  Batch 13,720  of  44,637.    Elapsed: 0:07:53. Training loss. 0.0036834650672972202 Num fake examples 13157 Num true examples 14283\n",
      "  Batch 13,760  of  44,637.    Elapsed: 0:07:54. Training loss. 0.003675830317661166 Num fake examples 13201 Num true examples 14319\n",
      "  Batch 13,800  of  44,637.    Elapsed: 0:07:56. Training loss. 0.0031037202570587397 Num fake examples 13239 Num true examples 14361\n",
      "  Batch 13,840  of  44,637.    Elapsed: 0:07:57. Training loss. 0.003365845652297139 Num fake examples 13288 Num true examples 14392\n",
      "  Batch 13,880  of  44,637.    Elapsed: 0:07:58. Training loss. 2.980844259262085 Num fake examples 13325 Num true examples 14435\n",
      "  Batch 13,920  of  44,637.    Elapsed: 0:08:00. Training loss. 0.0037820066791027784 Num fake examples 13365 Num true examples 14475\n",
      "  Batch 13,960  of  44,637.    Elapsed: 0:08:01. Training loss. 0.0032198107801377773 Num fake examples 13408 Num true examples 14512\n",
      "  Batch 14,000  of  44,637.    Elapsed: 0:08:02. Training loss. 0.003034006804227829 Num fake examples 13450 Num true examples 14550\n",
      "  Batch 14,040  of  44,637.    Elapsed: 0:08:04. Training loss. 0.0032472354359924793 Num fake examples 13489 Num true examples 14591\n",
      "  Batch 14,080  of  44,637.    Elapsed: 0:08:05. Training loss. 0.003718988038599491 Num fake examples 13532 Num true examples 14628\n",
      "  Batch 14,120  of  44,637.    Elapsed: 0:08:06. Training loss. 0.0033227549865841866 Num fake examples 13572 Num true examples 14668\n",
      "  Batch 14,160  of  44,637.    Elapsed: 0:08:08. Training loss. 2.7702319622039795 Num fake examples 13609 Num true examples 14711\n",
      "  Batch 14,200  of  44,637.    Elapsed: 0:08:09. Training loss. 0.003196988720446825 Num fake examples 13646 Num true examples 14754\n",
      "  Batch 14,240  of  44,637.    Elapsed: 0:08:11. Training loss. 0.0037795663811266422 Num fake examples 13686 Num true examples 14794\n",
      "  Batch 14,280  of  44,637.    Elapsed: 0:08:12. Training loss. 0.003399861976504326 Num fake examples 13720 Num true examples 14840\n",
      "  Batch 14,320  of  44,637.    Elapsed: 0:08:13. Training loss. 0.0033109732903540134 Num fake examples 13766 Num true examples 14874\n",
      "  Batch 14,360  of  44,637.    Elapsed: 0:08:15. Training loss. 0.0032454440370202065 Num fake examples 13801 Num true examples 14919\n",
      "  Batch 14,400  of  44,637.    Elapsed: 0:08:16. Training loss. 0.003737649880349636 Num fake examples 13838 Num true examples 14962\n",
      "  Batch 14,440  of  44,637.    Elapsed: 0:08:17. Training loss. 0.004187057260423899 Num fake examples 13884 Num true examples 14996\n",
      "  Batch 14,480  of  44,637.    Elapsed: 0:08:19. Training loss. 0.003523538587614894 Num fake examples 13922 Num true examples 15038\n",
      "  Batch 14,520  of  44,637.    Elapsed: 0:08:20. Training loss. 0.0034037400037050247 Num fake examples 13964 Num true examples 15076\n",
      "  Batch 14,560  of  44,637.    Elapsed: 0:08:21. Training loss. 0.0039136637933552265 Num fake examples 14003 Num true examples 15117\n",
      "  Batch 14,600  of  44,637.    Elapsed: 0:08:23. Training loss. 0.0038934212643653154 Num fake examples 14039 Num true examples 15161\n",
      "  Batch 14,640  of  44,637.    Elapsed: 0:08:24. Training loss. 0.003141964552924037 Num fake examples 14080 Num true examples 15200\n",
      "  Batch 14,680  of  44,637.    Elapsed: 0:08:26. Training loss. 0.004071645438671112 Num fake examples 14118 Num true examples 15242\n",
      "  Batch 14,720  of  44,637.    Elapsed: 0:08:27. Training loss. 0.003788555506616831 Num fake examples 14157 Num true examples 15283\n",
      "  Batch 14,760  of  44,637.    Elapsed: 0:08:28. Training loss. 0.0036698977928608656 Num fake examples 14197 Num true examples 15323\n",
      "  Batch 14,800  of  44,637.    Elapsed: 0:08:30. Training loss. 0.003979627974331379 Num fake examples 14231 Num true examples 15369\n",
      "  Batch 14,840  of  44,637.    Elapsed: 0:08:31. Training loss. 0.003860998898744583 Num fake examples 14269 Num true examples 15411\n",
      "  Batch 14,880  of  44,637.    Elapsed: 0:08:32. Training loss. 0.0036081031430512667 Num fake examples 14310 Num true examples 15450\n",
      "  Batch 14,920  of  44,637.    Elapsed: 0:08:34. Training loss. 0.0032647489570081234 Num fake examples 14345 Num true examples 15495\n",
      "  Batch 14,960  of  44,637.    Elapsed: 0:08:35. Training loss. 2.8277106285095215 Num fake examples 14376 Num true examples 15544\n",
      "  Batch 15,000  of  44,637.    Elapsed: 0:08:36. Training loss. 0.00463714636862278 Num fake examples 14416 Num true examples 15584\n",
      "  Batch 15,040  of  44,637.    Elapsed: 0:08:38. Training loss. 0.004164477810263634 Num fake examples 14454 Num true examples 15626\n",
      "  Batch 15,080  of  44,637.    Elapsed: 0:08:39. Training loss. 0.0032449779100716114 Num fake examples 14496 Num true examples 15664\n",
      "  Batch 15,120  of  44,637.    Elapsed: 0:08:41. Training loss. 0.003066070843487978 Num fake examples 14530 Num true examples 15710\n",
      "  Batch 15,160  of  44,637.    Elapsed: 0:08:42. Training loss. 0.0032314741984009743 Num fake examples 14575 Num true examples 15745\n",
      "  Batch 15,200  of  44,637.    Elapsed: 0:08:43. Training loss. 0.0026806709356606007 Num fake examples 14606 Num true examples 15794\n",
      "  Batch 15,240  of  44,637.    Elapsed: 0:08:45. Training loss. 0.003336498513817787 Num fake examples 14645 Num true examples 15835\n",
      "  Batch 15,280  of  44,637.    Elapsed: 0:08:46. Training loss. 0.004471948370337486 Num fake examples 14682 Num true examples 15878\n",
      "  Batch 15,320  of  44,637.    Elapsed: 0:08:47. Training loss. 0.003351382678374648 Num fake examples 14713 Num true examples 15927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15,360  of  44,637.    Elapsed: 0:08:49. Training loss. 0.0032967247534543276 Num fake examples 14757 Num true examples 15963\n",
      "  Batch 15,400  of  44,637.    Elapsed: 0:08:50. Training loss. 0.003687222022563219 Num fake examples 14792 Num true examples 16008\n",
      "  Batch 15,440  of  44,637.    Elapsed: 0:08:52. Training loss. 0.0033118375577032566 Num fake examples 14835 Num true examples 16045\n",
      "  Batch 15,480  of  44,637.    Elapsed: 0:08:53. Training loss. 2.784710645675659 Num fake examples 14876 Num true examples 16084\n",
      "  Batch 15,520  of  44,637.    Elapsed: 0:08:54. Training loss. 0.0047057196497917175 Num fake examples 14905 Num true examples 16135\n",
      "  Batch 15,560  of  44,637.    Elapsed: 0:08:56. Training loss. 0.0028054227586835623 Num fake examples 14938 Num true examples 16182\n",
      "  Batch 15,600  of  44,637.    Elapsed: 0:08:57. Training loss. 0.00322955590672791 Num fake examples 14971 Num true examples 16229\n",
      "  Batch 15,640  of  44,637.    Elapsed: 0:08:58. Training loss. 0.0036700814962387085 Num fake examples 15015 Num true examples 16265\n",
      "  Batch 15,680  of  44,637.    Elapsed: 0:09:00. Training loss. 0.003346481593325734 Num fake examples 15051 Num true examples 16309\n",
      "  Batch 15,720  of  44,637.    Elapsed: 0:09:01. Training loss. 0.0042108227498829365 Num fake examples 15089 Num true examples 16351\n",
      "  Batch 15,760  of  44,637.    Elapsed: 0:09:03. Training loss. 2.9626009464263916 Num fake examples 15129 Num true examples 16391\n",
      "  Batch 15,800  of  44,637.    Elapsed: 0:09:04. Training loss. 0.0032167211174964905 Num fake examples 15164 Num true examples 16436\n",
      "  Batch 15,840  of  44,637.    Elapsed: 0:09:05. Training loss. 0.0035416651517152786 Num fake examples 15202 Num true examples 16478\n",
      "  Batch 15,880  of  44,637.    Elapsed: 0:09:07. Training loss. 2.7119603157043457 Num fake examples 15233 Num true examples 16527\n",
      "  Batch 15,920  of  44,637.    Elapsed: 0:09:08. Training loss. 0.004638839513063431 Num fake examples 15271 Num true examples 16569\n",
      "  Batch 15,960  of  44,637.    Elapsed: 0:09:09. Training loss. 2.73565411567688 Num fake examples 15311 Num true examples 16609\n",
      "  Batch 16,000  of  44,637.    Elapsed: 0:09:11. Training loss. 0.002787256147712469 Num fake examples 15353 Num true examples 16647\n",
      "  Batch 16,040  of  44,637.    Elapsed: 0:09:12. Training loss. 0.0035788456443697214 Num fake examples 15389 Num true examples 16691\n",
      "  Batch 16,080  of  44,637.    Elapsed: 0:09:14. Training loss. 0.003055235370993614 Num fake examples 15428 Num true examples 16732\n",
      "  Batch 16,120  of  44,637.    Elapsed: 0:09:15. Training loss. 0.0035650660283863544 Num fake examples 15474 Num true examples 16766\n",
      "  Batch 16,160  of  44,637.    Elapsed: 0:09:16. Training loss. 0.0031854475382715464 Num fake examples 15513 Num true examples 16807\n",
      "  Batch 16,200  of  44,637.    Elapsed: 0:09:18. Training loss. 0.0033729039132595062 Num fake examples 15559 Num true examples 16841\n",
      "  Batch 16,240  of  44,637.    Elapsed: 0:09:19. Training loss. 0.0028399985749274492 Num fake examples 15592 Num true examples 16888\n",
      "  Batch 16,280  of  44,637.    Elapsed: 0:09:20. Training loss. 0.003270705696195364 Num fake examples 15632 Num true examples 16928\n",
      "  Batch 16,320  of  44,637.    Elapsed: 0:09:22. Training loss. 2.8756439685821533 Num fake examples 15673 Num true examples 16967\n",
      "  Batch 16,360  of  44,637.    Elapsed: 0:09:23. Training loss. 0.002912766532972455 Num fake examples 15711 Num true examples 17009\n",
      "  Batch 16,400  of  44,637.    Elapsed: 0:09:25. Training loss. 0.003733564168214798 Num fake examples 15751 Num true examples 17049\n",
      "  Batch 16,440  of  44,637.    Elapsed: 0:09:26. Training loss. 2.9406237602233887 Num fake examples 15790 Num true examples 17090\n",
      "  Batch 16,480  of  44,637.    Elapsed: 0:09:27. Training loss. 0.004434893373399973 Num fake examples 15833 Num true examples 17127\n",
      "  Batch 16,520  of  44,637.    Elapsed: 0:09:29. Training loss. 0.004664617124944925 Num fake examples 15874 Num true examples 17166\n",
      "  Batch 16,560  of  44,637.    Elapsed: 0:09:30. Training loss. 0.0040106563828885555 Num fake examples 15911 Num true examples 17209\n",
      "  Batch 16,600  of  44,637.    Elapsed: 0:09:31. Training loss. 0.0037151705473661423 Num fake examples 15943 Num true examples 17257\n",
      "  Batch 16,640  of  44,637.    Elapsed: 0:09:33. Training loss. 0.004205435048788786 Num fake examples 15971 Num true examples 17309\n",
      "  Batch 16,680  of  44,637.    Elapsed: 0:09:34. Training loss. 0.0043293386697769165 Num fake examples 16011 Num true examples 17349\n",
      "  Batch 16,720  of  44,637.    Elapsed: 0:09:36. Training loss. 0.004092271439731121 Num fake examples 16061 Num true examples 17379\n",
      "  Batch 16,760  of  44,637.    Elapsed: 0:09:37. Training loss. 2.877575635910034 Num fake examples 16105 Num true examples 17415\n",
      "  Batch 16,800  of  44,637.    Elapsed: 0:09:38. Training loss. 0.003614465706050396 Num fake examples 16149 Num true examples 17451\n",
      "  Batch 16,840  of  44,637.    Elapsed: 0:09:40. Training loss. 0.0035643181763589382 Num fake examples 16188 Num true examples 17492\n",
      "  Batch 16,880  of  44,637.    Elapsed: 0:09:41. Training loss. 0.003561414545401931 Num fake examples 16229 Num true examples 17531\n",
      "  Batch 16,920  of  44,637.    Elapsed: 0:09:42. Training loss. 0.003766194451600313 Num fake examples 16269 Num true examples 17571\n",
      "  Batch 16,960  of  44,637.    Elapsed: 0:09:44. Training loss. 0.003819122677668929 Num fake examples 16311 Num true examples 17609\n",
      "  Batch 17,000  of  44,637.    Elapsed: 0:09:45. Training loss. 0.003024073550477624 Num fake examples 16350 Num true examples 17650\n",
      "  Batch 17,040  of  44,637.    Elapsed: 0:09:47. Training loss. 0.00403043907135725 Num fake examples 16391 Num true examples 17689\n",
      "  Batch 17,080  of  44,637.    Elapsed: 0:09:48. Training loss. 0.00345559511333704 Num fake examples 16428 Num true examples 17732\n",
      "  Batch 17,120  of  44,637.    Elapsed: 0:09:49. Training loss. 0.0033193433191627264 Num fake examples 16472 Num true examples 17768\n",
      "  Batch 17,160  of  44,637.    Elapsed: 0:09:51. Training loss. 2.9718713760375977 Num fake examples 16511 Num true examples 17809\n",
      "  Batch 17,200  of  44,637.    Elapsed: 0:09:52. Training loss. 0.003198329359292984 Num fake examples 16557 Num true examples 17843\n",
      "  Batch 17,240  of  44,637.    Elapsed: 0:09:54. Training loss. 0.0031957621686160564 Num fake examples 16605 Num true examples 17875\n",
      "  Batch 17,280  of  44,637.    Elapsed: 0:09:55. Training loss. 0.0037760622799396515 Num fake examples 16639 Num true examples 17921\n",
      "  Batch 17,320  of  44,637.    Elapsed: 0:09:56. Training loss. 0.0035160579718649387 Num fake examples 16675 Num true examples 17965\n",
      "  Batch 17,360  of  44,637.    Elapsed: 0:09:58. Training loss. 0.0033156261779367924 Num fake examples 16719 Num true examples 18001\n",
      "  Batch 17,400  of  44,637.    Elapsed: 0:09:59. Training loss. 0.003348839469254017 Num fake examples 16757 Num true examples 18043\n",
      "  Batch 17,440  of  44,637.    Elapsed: 0:10:00. Training loss. 0.0033539361320436 Num fake examples 16786 Num true examples 18094\n",
      "  Batch 17,480  of  44,637.    Elapsed: 0:10:02. Training loss. 0.0026445549447089434 Num fake examples 16823 Num true examples 18137\n",
      "  Batch 17,520  of  44,637.    Elapsed: 0:10:03. Training loss. 0.0029880404472351074 Num fake examples 16862 Num true examples 18178\n",
      "  Batch 17,560  of  44,637.    Elapsed: 0:10:05. Training loss. 0.003623812459409237 Num fake examples 16909 Num true examples 18211\n",
      "  Batch 17,600  of  44,637.    Elapsed: 0:10:06. Training loss. 0.0030191955156624317 Num fake examples 16952 Num true examples 18248\n",
      "  Batch 17,640  of  44,637.    Elapsed: 0:10:07. Training loss. 0.003208755049854517 Num fake examples 16992 Num true examples 18288\n",
      "  Batch 17,680  of  44,637.    Elapsed: 0:10:09. Training loss. 0.002657596254721284 Num fake examples 17035 Num true examples 18325\n",
      "  Batch 17,720  of  44,637.    Elapsed: 0:10:10. Training loss. 0.002890408504754305 Num fake examples 17082 Num true examples 18358\n",
      "  Batch 17,760  of  44,637.    Elapsed: 0:10:12. Training loss. 0.0027001264970749617 Num fake examples 17117 Num true examples 18403\n",
      "  Batch 17,800  of  44,637.    Elapsed: 0:10:13. Training loss. 0.0027988506481051445 Num fake examples 17151 Num true examples 18449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 17,840  of  44,637.    Elapsed: 0:10:14. Training loss. 0.0029057541396468878 Num fake examples 17194 Num true examples 18486\n",
      "  Batch 17,880  of  44,637.    Elapsed: 0:10:16. Training loss. 0.0032019484788179398 Num fake examples 17236 Num true examples 18524\n",
      "  Batch 17,920  of  44,637.    Elapsed: 0:10:17. Training loss. 0.0033928793855011463 Num fake examples 17271 Num true examples 18569\n",
      "  Batch 17,960  of  44,637.    Elapsed: 0:10:18. Training loss. 0.0030052755028009415 Num fake examples 17313 Num true examples 18607\n",
      "  Batch 18,000  of  44,637.    Elapsed: 0:10:20. Training loss. 0.0031182568054646254 Num fake examples 17347 Num true examples 18653\n",
      "  Batch 18,040  of  44,637.    Elapsed: 0:10:21. Training loss. 0.0032156435772776604 Num fake examples 17385 Num true examples 18695\n",
      "  Batch 18,080  of  44,637.    Elapsed: 0:10:23. Training loss. 0.0034844032488763332 Num fake examples 17423 Num true examples 18737\n",
      "  Batch 18,120  of  44,637.    Elapsed: 0:10:24. Training loss. 0.003692502621561289 Num fake examples 17469 Num true examples 18771\n",
      "  Batch 18,160  of  44,637.    Elapsed: 0:10:25. Training loss. 0.004059162922203541 Num fake examples 17508 Num true examples 18812\n",
      "  Batch 18,200  of  44,637.    Elapsed: 0:10:27. Training loss. 2.7900686264038086 Num fake examples 17542 Num true examples 18858\n",
      "  Batch 18,240  of  44,637.    Elapsed: 0:10:28. Training loss. 0.003716413164511323 Num fake examples 17579 Num true examples 18901\n",
      "  Batch 18,280  of  44,637.    Elapsed: 0:10:29. Training loss. 0.004104974679648876 Num fake examples 17621 Num true examples 18939\n",
      "  Batch 18,320  of  44,637.    Elapsed: 0:10:31. Training loss. 2.7936534881591797 Num fake examples 17662 Num true examples 18978\n",
      "  Batch 18,360  of  44,637.    Elapsed: 0:10:32. Training loss. 0.003242962062358856 Num fake examples 17707 Num true examples 19013\n",
      "  Batch 18,400  of  44,637.    Elapsed: 0:10:34. Training loss. 0.003366833785548806 Num fake examples 17747 Num true examples 19053\n",
      "  Batch 18,440  of  44,637.    Elapsed: 0:10:35. Training loss. 0.003926003351807594 Num fake examples 17788 Num true examples 19092\n",
      "  Batch 18,480  of  44,637.    Elapsed: 0:10:36. Training loss. 0.003067268989980221 Num fake examples 17822 Num true examples 19138\n",
      "  Batch 18,520  of  44,637.    Elapsed: 0:10:38. Training loss. 0.0029896199703216553 Num fake examples 17861 Num true examples 19179\n",
      "  Batch 18,560  of  44,637.    Elapsed: 0:10:39. Training loss. 0.0035673058591783047 Num fake examples 17902 Num true examples 19218\n",
      "  Batch 18,600  of  44,637.    Elapsed: 0:10:40. Training loss. 0.003060337621718645 Num fake examples 17936 Num true examples 19264\n",
      "  Batch 18,640  of  44,637.    Elapsed: 0:10:42. Training loss. 0.0036394954659044743 Num fake examples 17968 Num true examples 19312\n",
      "  Batch 18,680  of  44,637.    Elapsed: 0:10:43. Training loss. 0.0026490280870348215 Num fake examples 18009 Num true examples 19351\n",
      "  Batch 18,720  of  44,637.    Elapsed: 0:10:44. Training loss. 0.0033178580924868584 Num fake examples 18042 Num true examples 19398\n",
      "  Batch 18,760  of  44,637.    Elapsed: 0:10:46. Training loss. 0.0027670462150126696 Num fake examples 18073 Num true examples 19447\n",
      "  Batch 18,800  of  44,637.    Elapsed: 0:10:47. Training loss. 0.0030909983906894922 Num fake examples 18112 Num true examples 19488\n",
      "  Batch 18,840  of  44,637.    Elapsed: 0:10:49. Training loss. 0.003227011766284704 Num fake examples 18146 Num true examples 19534\n",
      "  Batch 18,880  of  44,637.    Elapsed: 0:10:50. Training loss. 0.002964394399896264 Num fake examples 18181 Num true examples 19579\n",
      "  Batch 18,920  of  44,637.    Elapsed: 0:10:51. Training loss. 0.003107598749920726 Num fake examples 18217 Num true examples 19623\n",
      "  Batch 18,960  of  44,637.    Elapsed: 0:10:53. Training loss. 0.003242796054109931 Num fake examples 18254 Num true examples 19666\n",
      "  Batch 19,000  of  44,637.    Elapsed: 0:10:54. Training loss. 0.0028030648827552795 Num fake examples 18291 Num true examples 19709\n",
      "  Batch 19,040  of  44,637.    Elapsed: 0:10:55. Training loss. 0.0032942823600023985 Num fake examples 18329 Num true examples 19751\n",
      "  Batch 19,080  of  44,637.    Elapsed: 0:10:57. Training loss. 0.003295493545010686 Num fake examples 18370 Num true examples 19790\n",
      "  Batch 19,120  of  44,637.    Elapsed: 0:10:58. Training loss. 0.003142929170280695 Num fake examples 18411 Num true examples 19829\n",
      "  Batch 19,160  of  44,637.    Elapsed: 0:11:00. Training loss. 0.003457244485616684 Num fake examples 18453 Num true examples 19867\n",
      "  Batch 19,200  of  44,637.    Elapsed: 0:11:01. Training loss. 0.0032297219149768353 Num fake examples 18498 Num true examples 19902\n",
      "  Batch 19,240  of  44,637.    Elapsed: 0:11:02. Training loss. 0.0037381662987172604 Num fake examples 18536 Num true examples 19944\n",
      "  Batch 19,280  of  44,637.    Elapsed: 0:11:04. Training loss. 0.0035317162983119488 Num fake examples 18573 Num true examples 19987\n",
      "  Batch 19,320  of  44,637.    Elapsed: 0:11:05. Training loss. 0.0038182768039405346 Num fake examples 18617 Num true examples 20023\n",
      "  Batch 19,360  of  44,637.    Elapsed: 0:11:06. Training loss. 0.003565562190487981 Num fake examples 18658 Num true examples 20062\n",
      "  Batch 19,400  of  44,637.    Elapsed: 0:11:08. Training loss. 0.003686468582600355 Num fake examples 18691 Num true examples 20109\n",
      "  Batch 19,440  of  44,637.    Elapsed: 0:11:09. Training loss. 0.0033817484509199858 Num fake examples 18728 Num true examples 20152\n",
      "  Batch 19,480  of  44,637.    Elapsed: 0:11:10. Training loss. 0.002957042073830962 Num fake examples 18770 Num true examples 20190\n",
      "  Batch 19,520  of  44,637.    Elapsed: 0:11:12. Training loss. 0.0027316121850162745 Num fake examples 18810 Num true examples 20230\n",
      "  Batch 19,560  of  44,637.    Elapsed: 0:11:13. Training loss. 2.88316011428833 Num fake examples 18848 Num true examples 20272\n",
      "  Batch 19,600  of  44,637.    Elapsed: 0:11:15. Training loss. 0.0030725612305104733 Num fake examples 18893 Num true examples 20307\n",
      "  Batch 19,640  of  44,637.    Elapsed: 0:11:16. Training loss. 0.0033656926825642586 Num fake examples 18925 Num true examples 20355\n",
      "  Batch 19,680  of  44,637.    Elapsed: 0:11:17. Training loss. 0.0032204831950366497 Num fake examples 18967 Num true examples 20393\n",
      "  Batch 19,720  of  44,637.    Elapsed: 0:11:19. Training loss. 0.0033948596101254225 Num fake examples 19007 Num true examples 20433\n",
      "  Batch 19,760  of  44,637.    Elapsed: 0:11:20. Training loss. 0.0034654492046684027 Num fake examples 19043 Num true examples 20477\n",
      "  Batch 19,800  of  44,637.    Elapsed: 0:11:22. Training loss. 0.0036174533888697624 Num fake examples 19083 Num true examples 20517\n",
      "  Batch 19,840  of  44,637.    Elapsed: 0:11:23. Training loss. 0.004230090882629156 Num fake examples 19125 Num true examples 20555\n",
      "  Batch 19,880  of  44,637.    Elapsed: 0:11:24. Training loss. 0.0038775450084358454 Num fake examples 19162 Num true examples 20598\n",
      "  Batch 19,920  of  44,637.    Elapsed: 0:11:26. Training loss. 0.002939381403848529 Num fake examples 19198 Num true examples 20642\n",
      "  Batch 19,960  of  44,637.    Elapsed: 0:11:27. Training loss. 0.0027294792234897614 Num fake examples 19243 Num true examples 20677\n",
      "  Batch 20,000  of  44,637.    Elapsed: 0:11:28. Training loss. 0.003598421113565564 Num fake examples 19284 Num true examples 20716\n",
      "  Batch 20,040  of  44,637.    Elapsed: 0:11:30. Training loss. 0.004295554012060165 Num fake examples 19322 Num true examples 20758\n",
      "  Batch 20,080  of  44,637.    Elapsed: 0:11:31. Training loss. 0.003555022180080414 Num fake examples 19356 Num true examples 20804\n",
      "  Batch 20,120  of  44,637.    Elapsed: 0:11:33. Training loss. 0.003497440367937088 Num fake examples 19396 Num true examples 20844\n",
      "  Batch 20,160  of  44,637.    Elapsed: 0:11:34. Training loss. 0.003644762560725212 Num fake examples 19437 Num true examples 20883\n",
      "  Batch 20,200  of  44,637.    Elapsed: 0:11:35. Training loss. 0.003530083689838648 Num fake examples 19473 Num true examples 20927\n",
      "  Batch 20,240  of  44,637.    Elapsed: 0:11:37. Training loss. 0.002831918653100729 Num fake examples 19512 Num true examples 20968\n",
      "  Batch 20,280  of  44,637.    Elapsed: 0:11:38. Training loss. 0.0034545341040939093 Num fake examples 19556 Num true examples 21004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 20,320  of  44,637.    Elapsed: 0:11:39. Training loss. 0.003369659185409546 Num fake examples 19595 Num true examples 21045\n",
      "  Batch 20,360  of  44,637.    Elapsed: 0:11:41. Training loss. 0.0031556319445371628 Num fake examples 19633 Num true examples 21087\n",
      "  Batch 20,400  of  44,637.    Elapsed: 0:11:42. Training loss. 0.0036646185908466578 Num fake examples 19678 Num true examples 21122\n",
      "  Batch 20,440  of  44,637.    Elapsed: 0:11:43. Training loss. 0.00365788908675313 Num fake examples 19715 Num true examples 21165\n",
      "  Batch 20,480  of  44,637.    Elapsed: 0:11:45. Training loss. 0.0036042919382452965 Num fake examples 19759 Num true examples 21201\n",
      "  Batch 20,520  of  44,637.    Elapsed: 0:11:46. Training loss. 0.00303816725499928 Num fake examples 19794 Num true examples 21246\n",
      "  Batch 20,560  of  44,637.    Elapsed: 0:11:48. Training loss. 0.003919185139238834 Num fake examples 19832 Num true examples 21288\n",
      "  Batch 20,600  of  44,637.    Elapsed: 0:11:49. Training loss. 0.0030318591743707657 Num fake examples 19869 Num true examples 21331\n",
      "  Batch 20,640  of  44,637.    Elapsed: 0:11:50. Training loss. 0.0038251241203397512 Num fake examples 19900 Num true examples 21380\n",
      "  Batch 20,680  of  44,637.    Elapsed: 0:11:52. Training loss. 0.003886651713401079 Num fake examples 19932 Num true examples 21428\n",
      "  Batch 20,720  of  44,637.    Elapsed: 0:11:53. Training loss. 0.004261897876858711 Num fake examples 19964 Num true examples 21476\n",
      "  Batch 20,760  of  44,637.    Elapsed: 0:11:54. Training loss. 0.003900751005858183 Num fake examples 20007 Num true examples 21513\n",
      "  Batch 20,800  of  44,637.    Elapsed: 0:11:56. Training loss. 0.002862735651433468 Num fake examples 20049 Num true examples 21551\n",
      "  Batch 20,840  of  44,637.    Elapsed: 0:11:57. Training loss. 0.00406188378110528 Num fake examples 20078 Num true examples 21602\n",
      "  Batch 20,880  of  44,637.    Elapsed: 0:11:58. Training loss. 0.004586057737469673 Num fake examples 20115 Num true examples 21645\n",
      "  Batch 20,920  of  44,637.    Elapsed: 0:12:00. Training loss. 0.003168718656525016 Num fake examples 20160 Num true examples 21680\n",
      "  Batch 20,960  of  44,637.    Elapsed: 0:12:01. Training loss. 0.0035626692697405815 Num fake examples 20192 Num true examples 21728\n",
      "  Batch 21,000  of  44,637.    Elapsed: 0:12:02. Training loss. 0.003630508668720722 Num fake examples 20228 Num true examples 21772\n",
      "  Batch 21,040  of  44,637.    Elapsed: 0:12:04. Training loss. 0.0034518111497163773 Num fake examples 20265 Num true examples 21815\n",
      "  Batch 21,080  of  44,637.    Elapsed: 0:12:05. Training loss. 0.003016697708517313 Num fake examples 20307 Num true examples 21853\n",
      "  Batch 21,120  of  44,637.    Elapsed: 0:12:06. Training loss. 0.003588300896808505 Num fake examples 20342 Num true examples 21898\n",
      "  Batch 21,160  of  44,637.    Elapsed: 0:12:08. Training loss. 0.003324741031974554 Num fake examples 20378 Num true examples 21942\n",
      "  Batch 21,200  of  44,637.    Elapsed: 0:12:09. Training loss. 2.876068592071533 Num fake examples 20416 Num true examples 21984\n",
      "  Batch 21,240  of  44,637.    Elapsed: 0:12:11. Training loss. 0.003457709215581417 Num fake examples 20456 Num true examples 22024\n",
      "  Batch 21,280  of  44,637.    Elapsed: 0:12:12. Training loss. 0.003028159961104393 Num fake examples 20497 Num true examples 22063\n",
      "  Batch 21,320  of  44,637.    Elapsed: 0:12:13. Training loss. 0.0035893311724066734 Num fake examples 20524 Num true examples 22116\n",
      "  Batch 21,360  of  44,637.    Elapsed: 0:12:15. Training loss. 0.003304933663457632 Num fake examples 20564 Num true examples 22156\n",
      "  Batch 21,400  of  44,637.    Elapsed: 0:12:16. Training loss. 0.0034062322229146957 Num fake examples 20607 Num true examples 22193\n",
      "  Batch 21,440  of  44,637.    Elapsed: 0:12:17. Training loss. 0.0033573592081665993 Num fake examples 20643 Num true examples 22237\n",
      "  Batch 21,480  of  44,637.    Elapsed: 0:12:19. Training loss. 0.002999212360009551 Num fake examples 20682 Num true examples 22278\n",
      "  Batch 21,520  of  44,637.    Elapsed: 0:12:20. Training loss. 0.0036997825372964144 Num fake examples 20725 Num true examples 22315\n",
      "  Batch 21,560  of  44,637.    Elapsed: 0:12:22. Training loss. 0.0028485888615250587 Num fake examples 20760 Num true examples 22360\n",
      "  Batch 21,600  of  44,637.    Elapsed: 0:12:23. Training loss. 0.0030270619317889214 Num fake examples 20793 Num true examples 22407\n",
      "  Batch 21,640  of  44,637.    Elapsed: 0:12:24. Training loss. 0.003360559232532978 Num fake examples 20832 Num true examples 22448\n",
      "  Batch 21,680  of  44,637.    Elapsed: 0:12:26. Training loss. 0.0033809561282396317 Num fake examples 20870 Num true examples 22490\n",
      "  Batch 21,720  of  44,637.    Elapsed: 0:12:27. Training loss. 0.0032874993048608303 Num fake examples 20909 Num true examples 22531\n",
      "  Batch 21,760  of  44,637.    Elapsed: 0:12:28. Training loss. 2.874169111251831 Num fake examples 20948 Num true examples 22572\n",
      "  Batch 21,800  of  44,637.    Elapsed: 0:12:30. Training loss. 0.003356324043124914 Num fake examples 20991 Num true examples 22609\n",
      "  Batch 21,840  of  44,637.    Elapsed: 0:12:31. Training loss. 0.0037198876962065697 Num fake examples 21024 Num true examples 22656\n",
      "  Batch 21,880  of  44,637.    Elapsed: 0:12:33. Training loss. 0.0028652690816670656 Num fake examples 21061 Num true examples 22699\n",
      "  Batch 21,920  of  44,637.    Elapsed: 0:12:34. Training loss. 0.0030418038368225098 Num fake examples 21101 Num true examples 22739\n",
      "  Batch 21,960  of  44,637.    Elapsed: 0:12:35. Training loss. 0.003777044825255871 Num fake examples 21134 Num true examples 22786\n",
      "  Batch 22,000  of  44,637.    Elapsed: 0:12:37. Training loss. 0.0032163485884666443 Num fake examples 21171 Num true examples 22829\n",
      "  Batch 22,040  of  44,637.    Elapsed: 0:12:38. Training loss. 0.003184383735060692 Num fake examples 21212 Num true examples 22868\n",
      "  Batch 22,080  of  44,637.    Elapsed: 0:12:39. Training loss. 0.0035236203111708164 Num fake examples 21250 Num true examples 22910\n",
      "  Batch 22,120  of  44,637.    Elapsed: 0:12:41. Training loss. 0.002889751922339201 Num fake examples 21280 Num true examples 22960\n",
      "  Batch 22,160  of  44,637.    Elapsed: 0:12:42. Training loss. 0.003268653294071555 Num fake examples 21321 Num true examples 22999\n",
      "  Batch 22,200  of  44,637.    Elapsed: 0:12:44. Training loss. 0.0032241889275610447 Num fake examples 21368 Num true examples 23032\n",
      "  Batch 22,240  of  44,637.    Elapsed: 0:12:45. Training loss. 0.00324777839705348 Num fake examples 21408 Num true examples 23072\n",
      "  Batch 22,280  of  44,637.    Elapsed: 0:12:46. Training loss. 2.8388473987579346 Num fake examples 21448 Num true examples 23112\n",
      "  Batch 22,320  of  44,637.    Elapsed: 0:12:48. Training loss. 0.0033732762094587088 Num fake examples 21483 Num true examples 23157\n",
      "  Batch 22,360  of  44,637.    Elapsed: 0:12:49. Training loss. 0.003279666882008314 Num fake examples 21528 Num true examples 23192\n",
      "  Batch 22,400  of  44,637.    Elapsed: 0:12:50. Training loss. 0.0036234953440725803 Num fake examples 21568 Num true examples 23232\n",
      "  Batch 22,440  of  44,637.    Elapsed: 0:12:52. Training loss. 0.003038983326405287 Num fake examples 21608 Num true examples 23272\n",
      "  Batch 22,480  of  44,637.    Elapsed: 0:12:53. Training loss. 0.0032496796920895576 Num fake examples 21645 Num true examples 23315\n",
      "  Batch 22,520  of  44,637.    Elapsed: 0:12:55. Training loss. 0.0033352416940033436 Num fake examples 21681 Num true examples 23359\n",
      "  Batch 22,560  of  44,637.    Elapsed: 0:12:56. Training loss. 0.003675140207633376 Num fake examples 21721 Num true examples 23399\n",
      "  Batch 22,600  of  44,637.    Elapsed: 0:12:57. Training loss. 0.0033488853368908167 Num fake examples 21758 Num true examples 23442\n",
      "  Batch 22,640  of  44,637.    Elapsed: 0:12:59. Training loss. 0.003795403754338622 Num fake examples 21801 Num true examples 23479\n",
      "  Batch 22,680  of  44,637.    Elapsed: 0:13:00. Training loss. 0.0040416419506073 Num fake examples 21845 Num true examples 23515\n",
      "  Batch 22,720  of  44,637.    Elapsed: 0:13:01. Training loss. 0.004005745984613895 Num fake examples 21884 Num true examples 23556\n",
      "  Batch 22,760  of  44,637.    Elapsed: 0:13:03. Training loss. 0.004033219534903765 Num fake examples 21924 Num true examples 23596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 22,800  of  44,637.    Elapsed: 0:13:04. Training loss. 0.003170476760715246 Num fake examples 21958 Num true examples 23642\n",
      "  Batch 22,840  of  44,637.    Elapsed: 0:13:05. Training loss. 0.0034276177175343037 Num fake examples 22002 Num true examples 23678\n",
      "  Batch 22,880  of  44,637.    Elapsed: 0:13:07. Training loss. 0.0036233551800251007 Num fake examples 22054 Num true examples 23706\n",
      "  Batch 22,920  of  44,637.    Elapsed: 0:13:08. Training loss. 0.003666588570922613 Num fake examples 22093 Num true examples 23747\n",
      "  Batch 22,960  of  44,637.    Elapsed: 0:13:09. Training loss. 0.0030130897648632526 Num fake examples 22136 Num true examples 23784\n",
      "  Batch 23,000  of  44,637.    Elapsed: 0:13:11. Training loss. 0.004235174972563982 Num fake examples 22175 Num true examples 23825\n",
      "  Batch 23,040  of  44,637.    Elapsed: 0:13:12. Training loss. 0.0032149350736290216 Num fake examples 22216 Num true examples 23864\n",
      "  Batch 23,080  of  44,637.    Elapsed: 0:13:14. Training loss. 0.0031286163721233606 Num fake examples 22254 Num true examples 23906\n",
      "  Batch 23,120  of  44,637.    Elapsed: 0:13:15. Training loss. 0.0037292405031621456 Num fake examples 22291 Num true examples 23949\n",
      "  Batch 23,160  of  44,637.    Elapsed: 0:13:16. Training loss. 0.003194127231836319 Num fake examples 22331 Num true examples 23989\n",
      "  Batch 23,200  of  44,637.    Elapsed: 0:13:18. Training loss. 0.0033466701861470938 Num fake examples 22368 Num true examples 24032\n",
      "  Batch 23,240  of  44,637.    Elapsed: 0:13:19. Training loss. 0.003800660837441683 Num fake examples 22404 Num true examples 24076\n",
      "  Batch 23,280  of  44,637.    Elapsed: 0:13:20. Training loss. 0.003791874973103404 Num fake examples 22436 Num true examples 24124\n",
      "  Batch 23,320  of  44,637.    Elapsed: 0:13:22. Training loss. 0.0028959326446056366 Num fake examples 22472 Num true examples 24168\n",
      "  Batch 23,360  of  44,637.    Elapsed: 0:13:23. Training loss. 0.0034699798561632633 Num fake examples 22514 Num true examples 24206\n",
      "  Batch 23,400  of  44,637.    Elapsed: 0:13:24. Training loss. 0.003129425458610058 Num fake examples 22556 Num true examples 24244\n",
      "  Batch 23,440  of  44,637.    Elapsed: 0:13:26. Training loss. 0.0034569944255053997 Num fake examples 22594 Num true examples 24286\n",
      "  Batch 23,480  of  44,637.    Elapsed: 0:13:27. Training loss. 0.0034175082109868526 Num fake examples 22637 Num true examples 24323\n",
      "  Batch 23,520  of  44,637.    Elapsed: 0:13:29. Training loss. 0.0036237819585949183 Num fake examples 22677 Num true examples 24363\n",
      "  Batch 23,560  of  44,637.    Elapsed: 0:13:30. Training loss. 0.0030548139475286007 Num fake examples 22721 Num true examples 24399\n",
      "  Batch 23,600  of  44,637.    Elapsed: 0:13:31. Training loss. 0.003023742698132992 Num fake examples 22759 Num true examples 24441\n",
      "  Batch 23,640  of  44,637.    Elapsed: 0:13:33. Training loss. 0.0036572497338056564 Num fake examples 22792 Num true examples 24488\n",
      "  Batch 23,680  of  44,637.    Elapsed: 0:13:34. Training loss. 0.0034204269759356976 Num fake examples 22834 Num true examples 24526\n",
      "  Batch 23,720  of  44,637.    Elapsed: 0:13:35. Training loss. 0.003033443121239543 Num fake examples 22877 Num true examples 24563\n",
      "  Batch 23,760  of  44,637.    Elapsed: 0:13:37. Training loss. 0.0033273687586188316 Num fake examples 22918 Num true examples 24602\n",
      "  Batch 23,800  of  44,637.    Elapsed: 0:13:38. Training loss. 2.866206407546997 Num fake examples 22954 Num true examples 24646\n",
      "  Batch 23,840  of  44,637.    Elapsed: 0:13:39. Training loss. 0.003443349851295352 Num fake examples 22995 Num true examples 24685\n",
      "  Batch 23,880  of  44,637.    Elapsed: 0:13:41. Training loss. 0.0038923334795981646 Num fake examples 23033 Num true examples 24727\n",
      "  Batch 23,920  of  44,637.    Elapsed: 0:13:42. Training loss. 0.004072612151503563 Num fake examples 23077 Num true examples 24763\n",
      "  Batch 23,960  of  44,637.    Elapsed: 0:13:43. Training loss. 0.0031554880551993847 Num fake examples 23108 Num true examples 24812\n",
      "  Batch 24,000  of  44,637.    Elapsed: 0:13:45. Training loss. 0.0036132382228970528 Num fake examples 23145 Num true examples 24855\n",
      "  Batch 24,040  of  44,637.    Elapsed: 0:13:46. Training loss. 0.0035173557698726654 Num fake examples 23179 Num true examples 24901\n",
      "  Batch 24,080  of  44,637.    Elapsed: 0:13:48. Training loss. 0.003322177566587925 Num fake examples 23209 Num true examples 24951\n",
      "  Batch 24,120  of  44,637.    Elapsed: 0:13:49. Training loss. 0.0033099199645221233 Num fake examples 23249 Num true examples 24991\n",
      "  Batch 24,160  of  44,637.    Elapsed: 0:13:50. Training loss. 0.0033990428782999516 Num fake examples 23287 Num true examples 25033\n",
      "  Batch 24,200  of  44,637.    Elapsed: 0:13:52. Training loss. 0.003278126707300544 Num fake examples 23322 Num true examples 25078\n",
      "  Batch 24,240  of  44,637.    Elapsed: 0:13:53. Training loss. 0.0038637486286461353 Num fake examples 23357 Num true examples 25123\n",
      "  Batch 24,280  of  44,637.    Elapsed: 0:13:54. Training loss. 0.0031523010693490505 Num fake examples 23395 Num true examples 25165\n",
      "  Batch 24,320  of  44,637.    Elapsed: 0:13:56. Training loss. 0.003616305533796549 Num fake examples 23431 Num true examples 25209\n",
      "  Batch 24,360  of  44,637.    Elapsed: 0:13:57. Training loss. 0.003007249440997839 Num fake examples 23467 Num true examples 25253\n",
      "  Batch 24,400  of  44,637.    Elapsed: 0:13:58. Training loss. 0.0030727346893399954 Num fake examples 23505 Num true examples 25295\n",
      "  Batch 24,440  of  44,637.    Elapsed: 0:14:00. Training loss. 0.003047169651836157 Num fake examples 23542 Num true examples 25338\n",
      "  Batch 24,480  of  44,637.    Elapsed: 0:14:01. Training loss. 0.0030012372881174088 Num fake examples 23581 Num true examples 25379\n",
      "  Batch 24,520  of  44,637.    Elapsed: 0:14:02. Training loss. 0.0030167517252266407 Num fake examples 23623 Num true examples 25417\n",
      "  Batch 24,560  of  44,637.    Elapsed: 0:14:04. Training loss. 0.0032770230900496244 Num fake examples 23661 Num true examples 25459\n",
      "  Batch 24,600  of  44,637.    Elapsed: 0:14:05. Training loss. 0.003390562953427434 Num fake examples 23699 Num true examples 25501\n",
      "  Batch 24,640  of  44,637.    Elapsed: 0:14:07. Training loss. 0.0030792313627898693 Num fake examples 23734 Num true examples 25546\n",
      "  Batch 24,680  of  44,637.    Elapsed: 0:14:08. Training loss. 0.0032422603107988834 Num fake examples 23766 Num true examples 25594\n",
      "  Batch 24,720  of  44,637.    Elapsed: 0:14:09. Training loss. 0.003926596604287624 Num fake examples 23809 Num true examples 25631\n",
      "  Batch 24,760  of  44,637.    Elapsed: 0:14:11. Training loss. 0.00304134888574481 Num fake examples 23844 Num true examples 25676\n",
      "  Batch 24,800  of  44,637.    Elapsed: 0:14:12. Training loss. 0.003930412232875824 Num fake examples 23879 Num true examples 25721\n",
      "  Batch 24,840  of  44,637.    Elapsed: 0:14:13. Training loss. 0.003315676935017109 Num fake examples 23911 Num true examples 25769\n",
      "  Batch 24,880  of  44,637.    Elapsed: 0:14:15. Training loss. 0.0033087776973843575 Num fake examples 23955 Num true examples 25805\n",
      "  Batch 24,920  of  44,637.    Elapsed: 0:14:16. Training loss. 0.0029759653843939304 Num fake examples 23996 Num true examples 25844\n",
      "  Batch 24,960  of  44,637.    Elapsed: 0:14:17. Training loss. 0.0033040400594472885 Num fake examples 24035 Num true examples 25885\n",
      "  Batch 25,000  of  44,637.    Elapsed: 0:14:19. Training loss. 0.0029866881668567657 Num fake examples 24080 Num true examples 25920\n",
      "  Batch 25,040  of  44,637.    Elapsed: 0:14:20. Training loss. 2.766995906829834 Num fake examples 24118 Num true examples 25962\n",
      "  Batch 25,080  of  44,637.    Elapsed: 0:14:22. Training loss. 0.002989533357322216 Num fake examples 24159 Num true examples 26001\n",
      "  Batch 25,120  of  44,637.    Elapsed: 0:14:23. Training loss. 0.003187826368957758 Num fake examples 24196 Num true examples 26044\n",
      "  Batch 25,160  of  44,637.    Elapsed: 0:14:24. Training loss. 0.003181036561727524 Num fake examples 24235 Num true examples 26085\n",
      "  Batch 25,200  of  44,637.    Elapsed: 0:14:26. Training loss. 0.002951399888843298 Num fake examples 24271 Num true examples 26129\n",
      "  Batch 25,240  of  44,637.    Elapsed: 0:14:27. Training loss. 0.0035185401793569326 Num fake examples 24310 Num true examples 26170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 25,280  of  44,637.    Elapsed: 0:14:28. Training loss. 0.0031998001504689455 Num fake examples 24346 Num true examples 26214\n",
      "  Batch 25,320  of  44,637.    Elapsed: 0:14:30. Training loss. 0.0033661765046417713 Num fake examples 24383 Num true examples 26257\n",
      "  Batch 25,360  of  44,637.    Elapsed: 0:14:31. Training loss. 0.00436544930562377 Num fake examples 24417 Num true examples 26303\n",
      "  Batch 25,400  of  44,637.    Elapsed: 0:14:33. Training loss. 0.0034050371032208204 Num fake examples 24456 Num true examples 26344\n",
      "  Batch 25,440  of  44,637.    Elapsed: 0:14:34. Training loss. 0.0037542995996773243 Num fake examples 24503 Num true examples 26377\n",
      "  Batch 25,480  of  44,637.    Elapsed: 0:14:35. Training loss. 0.0032737883739173412 Num fake examples 24538 Num true examples 26422\n",
      "  Batch 25,520  of  44,637.    Elapsed: 0:14:37. Training loss. 0.00436962116509676 Num fake examples 24582 Num true examples 26458\n",
      "  Batch 25,560  of  44,637.    Elapsed: 0:14:38. Training loss. 0.00315223541110754 Num fake examples 24620 Num true examples 26500\n",
      "  Batch 25,600  of  44,637.    Elapsed: 0:14:39. Training loss. 0.003896903246641159 Num fake examples 24664 Num true examples 26536\n",
      "  Batch 25,640  of  44,637.    Elapsed: 0:14:41. Training loss. 0.0034071847330778837 Num fake examples 24705 Num true examples 26575\n",
      "  Batch 25,680  of  44,637.    Elapsed: 0:14:42. Training loss. 0.004267359152436256 Num fake examples 24746 Num true examples 26614\n",
      "  Batch 25,720  of  44,637.    Elapsed: 0:14:43. Training loss. 0.003083996009081602 Num fake examples 24795 Num true examples 26645\n",
      "  Batch 25,760  of  44,637.    Elapsed: 0:14:45. Training loss. 0.004192403517663479 Num fake examples 24837 Num true examples 26683\n",
      "  Batch 25,800  of  44,637.    Elapsed: 0:14:46. Training loss. 0.0034593206364661455 Num fake examples 24883 Num true examples 26717\n",
      "  Batch 25,840  of  44,637.    Elapsed: 0:14:47. Training loss. 0.003958841320127249 Num fake examples 24927 Num true examples 26753\n",
      "  Batch 25,880  of  44,637.    Elapsed: 0:14:49. Training loss. 0.003571135923266411 Num fake examples 24965 Num true examples 26795\n",
      "  Batch 25,920  of  44,637.    Elapsed: 0:14:50. Training loss. 0.003371716011315584 Num fake examples 25007 Num true examples 26833\n",
      "  Batch 25,960  of  44,637.    Elapsed: 0:14:52. Training loss. 0.0040154848247766495 Num fake examples 25043 Num true examples 26877\n",
      "  Batch 26,000  of  44,637.    Elapsed: 0:14:53. Training loss. 0.0037342230789363384 Num fake examples 25088 Num true examples 26912\n",
      "  Batch 26,040  of  44,637.    Elapsed: 0:14:54. Training loss. 0.00339094502851367 Num fake examples 25119 Num true examples 26961\n",
      "  Batch 26,080  of  44,637.    Elapsed: 0:14:56. Training loss. 0.0033362735994160175 Num fake examples 25151 Num true examples 27009\n",
      "  Batch 26,120  of  44,637.    Elapsed: 0:14:57. Training loss. 0.003279091091826558 Num fake examples 25191 Num true examples 27049\n",
      "  Batch 26,160  of  44,637.    Elapsed: 0:14:58. Training loss. 0.0030902561265975237 Num fake examples 25232 Num true examples 27088\n",
      "  Batch 26,200  of  44,637.    Elapsed: 0:15:00. Training loss. 0.0035227963235229254 Num fake examples 25265 Num true examples 27135\n",
      "  Batch 26,240  of  44,637.    Elapsed: 0:15:01. Training loss. 2.8982350826263428 Num fake examples 25310 Num true examples 27170\n",
      "  Batch 26,280  of  44,637.    Elapsed: 0:15:02. Training loss. 0.003277221694588661 Num fake examples 25343 Num true examples 27217\n",
      "  Batch 26,320  of  44,637.    Elapsed: 0:15:04. Training loss. 0.0034181447699666023 Num fake examples 25382 Num true examples 27258\n",
      "  Batch 26,360  of  44,637.    Elapsed: 0:15:05. Training loss. 0.0030441011767834425 Num fake examples 25426 Num true examples 27294\n",
      "  Batch 26,400  of  44,637.    Elapsed: 0:15:07. Training loss. 0.0036415504291653633 Num fake examples 25472 Num true examples 27328\n",
      "  Batch 26,440  of  44,637.    Elapsed: 0:15:08. Training loss. 2.829059600830078 Num fake examples 25513 Num true examples 27367\n",
      "  Batch 26,480  of  44,637.    Elapsed: 0:15:09. Training loss. 0.003741251537576318 Num fake examples 25554 Num true examples 27406\n",
      "  Batch 26,520  of  44,637.    Elapsed: 0:15:11. Training loss. 0.003537764074280858 Num fake examples 25595 Num true examples 27445\n",
      "  Batch 26,560  of  44,637.    Elapsed: 0:15:12. Training loss. 0.0049890330992639065 Num fake examples 25622 Num true examples 27498\n",
      "  Batch 26,600  of  44,637.    Elapsed: 0:15:13. Training loss. 0.0037653183098882437 Num fake examples 25663 Num true examples 27537\n",
      "  Batch 26,640  of  44,637.    Elapsed: 0:15:15. Training loss. 0.004389398265630007 Num fake examples 25697 Num true examples 27583\n",
      "  Batch 26,680  of  44,637.    Elapsed: 0:15:16. Training loss. 0.003399801906198263 Num fake examples 25733 Num true examples 27627\n",
      "  Batch 26,720  of  44,637.    Elapsed: 0:15:17. Training loss. 0.0035145182628184557 Num fake examples 25770 Num true examples 27670\n",
      "  Batch 26,760  of  44,637.    Elapsed: 0:15:19. Training loss. 0.003731654491275549 Num fake examples 25807 Num true examples 27713\n",
      "  Batch 26,800  of  44,637.    Elapsed: 0:15:20. Training loss. 0.0036344602704048157 Num fake examples 25847 Num true examples 27753\n",
      "  Batch 26,840  of  44,637.    Elapsed: 0:15:22. Training loss. 0.0036592986434698105 Num fake examples 25882 Num true examples 27798\n",
      "  Batch 26,880  of  44,637.    Elapsed: 0:15:23. Training loss. 0.003251613350585103 Num fake examples 25915 Num true examples 27845\n",
      "  Batch 26,920  of  44,637.    Elapsed: 0:15:24. Training loss. 0.0036431760527193546 Num fake examples 25956 Num true examples 27884\n",
      "  Batch 26,960  of  44,637.    Elapsed: 0:15:26. Training loss. 0.003883305937051773 Num fake examples 25995 Num true examples 27925\n",
      "  Batch 27,000  of  44,637.    Elapsed: 0:15:27. Training loss. 0.003375209867954254 Num fake examples 26028 Num true examples 27972\n",
      "  Batch 27,040  of  44,637.    Elapsed: 0:15:28. Training loss. 0.004230262711644173 Num fake examples 26064 Num true examples 28016\n",
      "  Batch 27,080  of  44,637.    Elapsed: 0:15:30. Training loss. 0.004020960535854101 Num fake examples 26104 Num true examples 28056\n",
      "  Batch 27,120  of  44,637.    Elapsed: 0:15:31. Training loss. 0.0033633687999099493 Num fake examples 26138 Num true examples 28102\n",
      "  Batch 27,160  of  44,637.    Elapsed: 0:15:32. Training loss. 0.004020127467811108 Num fake examples 26178 Num true examples 28142\n",
      "  Batch 27,200  of  44,637.    Elapsed: 0:15:34. Training loss. 0.0033402247354388237 Num fake examples 26215 Num true examples 28185\n",
      "  Batch 27,240  of  44,637.    Elapsed: 0:15:35. Training loss. 0.004262961912900209 Num fake examples 26255 Num true examples 28225\n",
      "  Batch 27,280  of  44,637.    Elapsed: 0:15:37. Training loss. 0.0032234778627753258 Num fake examples 26295 Num true examples 28265\n",
      "  Batch 27,320  of  44,637.    Elapsed: 0:15:38. Training loss. 0.003558760043233633 Num fake examples 26329 Num true examples 28311\n",
      "  Batch 27,360  of  44,637.    Elapsed: 0:15:39. Training loss. 0.0036040390841662884 Num fake examples 26369 Num true examples 28351\n",
      "  Batch 27,400  of  44,637.    Elapsed: 0:15:41. Training loss. 0.0036927550099790096 Num fake examples 26413 Num true examples 28387\n",
      "  Batch 27,440  of  44,637.    Elapsed: 0:15:42. Training loss. 0.003458095248788595 Num fake examples 26443 Num true examples 28437\n",
      "  Batch 27,480  of  44,637.    Elapsed: 0:15:43. Training loss. 0.003321315860375762 Num fake examples 26487 Num true examples 28473\n",
      "  Batch 27,520  of  44,637.    Elapsed: 0:15:45. Training loss. 0.003677441505715251 Num fake examples 26524 Num true examples 28516\n",
      "  Batch 27,560  of  44,637.    Elapsed: 0:15:46. Training loss. 0.0035549586173146963 Num fake examples 26563 Num true examples 28557\n",
      "  Batch 27,600  of  44,637.    Elapsed: 0:15:47. Training loss. 0.0033905603922903538 Num fake examples 26600 Num true examples 28600\n",
      "  Batch 27,640  of  44,637.    Elapsed: 0:15:49. Training loss. 0.0034211345482617617 Num fake examples 26640 Num true examples 28640\n",
      "  Batch 27,680  of  44,637.    Elapsed: 0:15:50. Training loss. 0.003578542498871684 Num fake examples 26680 Num true examples 28680\n",
      "  Batch 27,720  of  44,637.    Elapsed: 0:15:52. Training loss. 0.002955246949568391 Num fake examples 26714 Num true examples 28726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 27,760  of  44,637.    Elapsed: 0:15:53. Training loss. 0.002967757172882557 Num fake examples 26761 Num true examples 28759\n",
      "  Batch 27,800  of  44,637.    Elapsed: 0:15:54. Training loss. 0.0033457756508141756 Num fake examples 26807 Num true examples 28793\n",
      "  Batch 27,840  of  44,637.    Elapsed: 0:15:56. Training loss. 0.0034067381639033556 Num fake examples 26847 Num true examples 28833\n",
      "  Batch 27,880  of  44,637.    Elapsed: 0:15:57. Training loss. 0.004680235404521227 Num fake examples 26880 Num true examples 28880\n",
      "  Batch 27,920  of  44,637.    Elapsed: 0:15:58. Training loss. 0.004026732873171568 Num fake examples 26914 Num true examples 28926\n",
      "  Batch 27,960  of  44,637.    Elapsed: 0:16:00. Training loss. 0.004091443493962288 Num fake examples 26958 Num true examples 28962\n",
      "  Batch 28,000  of  44,637.    Elapsed: 0:16:01. Training loss. 0.0031488374806940556 Num fake examples 26997 Num true examples 29003\n",
      "  Batch 28,040  of  44,637.    Elapsed: 0:16:02. Training loss. 0.003711177036166191 Num fake examples 27035 Num true examples 29045\n",
      "  Batch 28,080  of  44,637.    Elapsed: 0:16:04. Training loss. 0.0035393191501498222 Num fake examples 27071 Num true examples 29089\n",
      "  Batch 28,120  of  44,637.    Elapsed: 0:16:05. Training loss. 0.0033478031400591135 Num fake examples 27105 Num true examples 29135\n",
      "  Batch 28,160  of  44,637.    Elapsed: 0:16:06. Training loss. 0.0035816046874970198 Num fake examples 27144 Num true examples 29176\n",
      "  Batch 28,200  of  44,637.    Elapsed: 0:16:08. Training loss. 0.0034263781271874905 Num fake examples 27184 Num true examples 29216\n",
      "  Batch 28,240  of  44,637.    Elapsed: 0:16:09. Training loss. 2.777082681655884 Num fake examples 27230 Num true examples 29250\n",
      "  Batch 28,280  of  44,637.    Elapsed: 0:16:10. Training loss. 0.004053929820656776 Num fake examples 27274 Num true examples 29286\n",
      "  Batch 28,320  of  44,637.    Elapsed: 0:16:12. Training loss. 0.0037373658269643784 Num fake examples 27308 Num true examples 29332\n",
      "  Batch 28,360  of  44,637.    Elapsed: 0:16:13. Training loss. 0.0038735689595341682 Num fake examples 27349 Num true examples 29371\n",
      "  Batch 28,400  of  44,637.    Elapsed: 0:16:15. Training loss. 0.004243458621203899 Num fake examples 27388 Num true examples 29412\n",
      "  Batch 28,440  of  44,637.    Elapsed: 0:16:16. Training loss. 2.835806369781494 Num fake examples 27436 Num true examples 29444\n",
      "  Batch 28,480  of  44,637.    Elapsed: 0:16:17. Training loss. 0.00452637393027544 Num fake examples 27475 Num true examples 29485\n",
      "  Batch 28,520  of  44,637.    Elapsed: 0:16:19. Training loss. 0.003555385861545801 Num fake examples 27511 Num true examples 29529\n",
      "  Batch 28,560  of  44,637.    Elapsed: 0:16:20. Training loss. 0.004009503871202469 Num fake examples 27543 Num true examples 29577\n",
      "  Batch 28,600  of  44,637.    Elapsed: 0:16:21. Training loss. 0.004399903118610382 Num fake examples 27578 Num true examples 29622\n",
      "  Batch 28,640  of  44,637.    Elapsed: 0:16:23. Training loss. 0.004826077260077 Num fake examples 27619 Num true examples 29661\n",
      "  Batch 28,680  of  44,637.    Elapsed: 0:16:24. Training loss. 0.003573911264538765 Num fake examples 27657 Num true examples 29703\n",
      "  Batch 28,720  of  44,637.    Elapsed: 0:16:26. Training loss. 0.0038218919653445482 Num fake examples 27693 Num true examples 29747\n",
      "  Batch 28,760  of  44,637.    Elapsed: 0:16:27. Training loss. 0.003756046760827303 Num fake examples 27737 Num true examples 29783\n",
      "  Batch 28,800  of  44,637.    Elapsed: 0:16:28. Training loss. 0.0040970295667648315 Num fake examples 27776 Num true examples 29824\n",
      "  Batch 28,840  of  44,637.    Elapsed: 0:16:30. Training loss. 0.004356497898697853 Num fake examples 27809 Num true examples 29871\n",
      "  Batch 28,880  of  44,637.    Elapsed: 0:16:31. Training loss. 0.003794237272813916 Num fake examples 27852 Num true examples 29908\n",
      "  Batch 28,920  of  44,637.    Elapsed: 0:16:32. Training loss. 0.00414933729916811 Num fake examples 27891 Num true examples 29949\n",
      "  Batch 28,960  of  44,637.    Elapsed: 0:16:34. Training loss. 0.003965076990425587 Num fake examples 27925 Num true examples 29995\n",
      "  Batch 29,000  of  44,637.    Elapsed: 0:16:35. Training loss. 0.003409815952181816 Num fake examples 27957 Num true examples 30043\n",
      "  Batch 29,040  of  44,637.    Elapsed: 0:16:36. Training loss. 0.003729925025254488 Num fake examples 27999 Num true examples 30081\n",
      "  Batch 29,080  of  44,637.    Elapsed: 0:16:38. Training loss. 0.0038519136141985655 Num fake examples 28042 Num true examples 30118\n",
      "  Batch 29,120  of  44,637.    Elapsed: 0:16:39. Training loss. 0.004418796859681606 Num fake examples 28084 Num true examples 30156\n",
      "  Batch 29,160  of  44,637.    Elapsed: 0:16:40. Training loss. 0.0032481886446475983 Num fake examples 28124 Num true examples 30196\n",
      "  Batch 29,200  of  44,637.    Elapsed: 0:16:42. Training loss. 0.0032816212624311447 Num fake examples 28155 Num true examples 30245\n",
      "  Batch 29,240  of  44,637.    Elapsed: 0:16:43. Training loss. 0.0035494198091328144 Num fake examples 28199 Num true examples 30281\n",
      "  Batch 29,280  of  44,637.    Elapsed: 0:16:45. Training loss. 0.003568339394405484 Num fake examples 28243 Num true examples 30317\n",
      "  Batch 29,320  of  44,637.    Elapsed: 0:16:46. Training loss. 2.7830569744110107 Num fake examples 28285 Num true examples 30355\n",
      "  Batch 29,360  of  44,637.    Elapsed: 0:16:47. Training loss. 0.0031578456982970238 Num fake examples 28323 Num true examples 30397\n",
      "  Batch 29,400  of  44,637.    Elapsed: 0:16:49. Training loss. 0.003354910295456648 Num fake examples 28364 Num true examples 30436\n",
      "  Batch 29,440  of  44,637.    Elapsed: 0:16:50. Training loss. 0.0038289446383714676 Num fake examples 28393 Num true examples 30487\n",
      "  Batch 29,480  of  44,637.    Elapsed: 0:16:51. Training loss. 0.004118430893868208 Num fake examples 28437 Num true examples 30523\n",
      "  Batch 29,520  of  44,637.    Elapsed: 0:16:53. Training loss. 0.004233368206769228 Num fake examples 28481 Num true examples 30559\n",
      "  Batch 29,560  of  44,637.    Elapsed: 0:16:54. Training loss. 0.003563471371307969 Num fake examples 28525 Num true examples 30595\n",
      "  Batch 29,600  of  44,637.    Elapsed: 0:16:55. Training loss. 0.003655199194326997 Num fake examples 28559 Num true examples 30641\n",
      "  Batch 29,640  of  44,637.    Elapsed: 0:16:57. Training loss. 0.0039194305427372456 Num fake examples 28588 Num true examples 30692\n",
      "  Batch 29,680  of  44,637.    Elapsed: 0:16:58. Training loss. 0.0037623154930770397 Num fake examples 28623 Num true examples 30737\n",
      "  Batch 29,720  of  44,637.    Elapsed: 0:16:59. Training loss. 0.003333131317049265 Num fake examples 28662 Num true examples 30778\n",
      "  Batch 29,760  of  44,637.    Elapsed: 0:17:01. Training loss. 2.8001201152801514 Num fake examples 28703 Num true examples 30817\n",
      "  Batch 29,800  of  44,637.    Elapsed: 0:17:02. Training loss. 0.004651366733014584 Num fake examples 28736 Num true examples 30864\n",
      "  Batch 29,840  of  44,637.    Elapsed: 0:17:04. Training loss. 0.00424692640081048 Num fake examples 28767 Num true examples 30913\n",
      "  Batch 29,880  of  44,637.    Elapsed: 0:17:05. Training loss. 0.004587715491652489 Num fake examples 28799 Num true examples 30961\n",
      "  Batch 29,920  of  44,637.    Elapsed: 0:17:06. Training loss. 0.004097880329936743 Num fake examples 28831 Num true examples 31009\n",
      "  Batch 29,960  of  44,637.    Elapsed: 0:17:08. Training loss. 0.004342266358435154 Num fake examples 28870 Num true examples 31050\n",
      "  Batch 30,000  of  44,637.    Elapsed: 0:17:09. Training loss. 0.00428627897053957 Num fake examples 28914 Num true examples 31086\n",
      "  Batch 30,040  of  44,637.    Elapsed: 0:17:10. Training loss. 0.003886068006977439 Num fake examples 28945 Num true examples 31135\n",
      "  Batch 30,080  of  44,637.    Elapsed: 0:17:12. Training loss. 0.0038314172998070717 Num fake examples 28991 Num true examples 31169\n",
      "  Batch 30,120  of  44,637.    Elapsed: 0:17:13. Training loss. 2.692471504211426 Num fake examples 29021 Num true examples 31219\n",
      "  Batch 30,160  of  44,637.    Elapsed: 0:17:14. Training loss. 0.004034886136651039 Num fake examples 29058 Num true examples 31262\n",
      "  Batch 30,200  of  44,637.    Elapsed: 0:17:16. Training loss. 0.005097574554383755 Num fake examples 29096 Num true examples 31304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 30,240  of  44,637.    Elapsed: 0:17:17. Training loss. 0.004342446103692055 Num fake examples 29136 Num true examples 31344\n",
      "  Batch 30,280  of  44,637.    Elapsed: 0:17:18. Training loss. 0.004009315278381109 Num fake examples 29167 Num true examples 31393\n",
      "  Batch 30,320  of  44,637.    Elapsed: 0:17:20. Training loss. 0.004092744551599026 Num fake examples 29212 Num true examples 31428\n",
      "  Batch 30,360  of  44,637.    Elapsed: 0:17:21. Training loss. 0.004458365496248007 Num fake examples 29259 Num true examples 31461\n",
      "  Batch 30,400  of  44,637.    Elapsed: 0:17:23. Training loss. 0.003906805533915758 Num fake examples 29306 Num true examples 31494\n",
      "  Batch 30,440  of  44,637.    Elapsed: 0:17:24. Training loss. 0.003909691236913204 Num fake examples 29340 Num true examples 31540\n",
      "  Batch 30,480  of  44,637.    Elapsed: 0:17:25. Training loss. 0.004401272162795067 Num fake examples 29382 Num true examples 31578\n",
      "  Batch 30,520  of  44,637.    Elapsed: 0:17:27. Training loss. 0.003482324769720435 Num fake examples 29419 Num true examples 31621\n",
      "  Batch 30,560  of  44,637.    Elapsed: 0:17:28. Training loss. 0.002952879760414362 Num fake examples 29451 Num true examples 31669\n",
      "  Batch 30,600  of  44,637.    Elapsed: 0:17:29. Training loss. 0.003994561266154051 Num fake examples 29486 Num true examples 31714\n",
      "  Batch 30,640  of  44,637.    Elapsed: 0:17:31. Training loss. 0.0039011272601783276 Num fake examples 29521 Num true examples 31759\n",
      "  Batch 30,680  of  44,637.    Elapsed: 0:17:32. Training loss. 0.003765018889680505 Num fake examples 29558 Num true examples 31802\n",
      "  Batch 30,720  of  44,637.    Elapsed: 0:17:33. Training loss. 0.004788785241544247 Num fake examples 29602 Num true examples 31838\n",
      "  Batch 30,760  of  44,637.    Elapsed: 0:17:35. Training loss. 0.0037946472875773907 Num fake examples 29642 Num true examples 31878\n",
      "  Batch 30,800  of  44,637.    Elapsed: 0:17:36. Training loss. 0.0040537090972065926 Num fake examples 29682 Num true examples 31918\n",
      "  Batch 30,840  of  44,637.    Elapsed: 0:17:37. Training loss. 2.9457576274871826 Num fake examples 29719 Num true examples 31961\n",
      "  Batch 30,880  of  44,637.    Elapsed: 0:17:39. Training loss. 0.0035985694266855717 Num fake examples 29772 Num true examples 31988\n",
      "  Batch 30,920  of  44,637.    Elapsed: 0:17:40. Training loss. 0.0032168012112379074 Num fake examples 29806 Num true examples 32034\n",
      "  Batch 30,960  of  44,637.    Elapsed: 0:17:42. Training loss. 0.003322542179375887 Num fake examples 29840 Num true examples 32080\n",
      "  Batch 31,000  of  44,637.    Elapsed: 0:17:43. Training loss. 0.0036153371911495924 Num fake examples 29878 Num true examples 32122\n",
      "  Batch 31,040  of  44,637.    Elapsed: 0:17:44. Training loss. 2.8259341716766357 Num fake examples 29923 Num true examples 32157\n",
      "  Batch 31,080  of  44,637.    Elapsed: 0:17:46. Training loss. 0.0034927790984511375 Num fake examples 29957 Num true examples 32203\n",
      "  Batch 31,120  of  44,637.    Elapsed: 0:17:47. Training loss. 0.0033862153068184853 Num fake examples 29994 Num true examples 32246\n",
      "  Batch 31,160  of  44,637.    Elapsed: 0:17:48. Training loss. 0.00435710558667779 Num fake examples 30038 Num true examples 32282\n",
      "  Batch 31,200  of  44,637.    Elapsed: 0:17:50. Training loss. 2.848029851913452 Num fake examples 30072 Num true examples 32328\n",
      "  Batch 31,240  of  44,637.    Elapsed: 0:17:51. Training loss. 0.003697173437103629 Num fake examples 30114 Num true examples 32366\n",
      "  Batch 31,280  of  44,637.    Elapsed: 0:17:52. Training loss. 0.0033278162591159344 Num fake examples 30149 Num true examples 32411\n",
      "  Batch 31,320  of  44,637.    Elapsed: 0:17:54. Training loss. 0.003306727623566985 Num fake examples 30184 Num true examples 32456\n",
      "  Batch 31,360  of  44,637.    Elapsed: 0:17:55. Training loss. 0.004382488317787647 Num fake examples 30221 Num true examples 32499\n",
      "  Batch 31,400  of  44,637.    Elapsed: 0:17:56. Training loss. 0.003308334155008197 Num fake examples 30263 Num true examples 32537\n",
      "  Batch 31,440  of  44,637.    Elapsed: 0:17:58. Training loss. 0.00303934165276587 Num fake examples 30302 Num true examples 32578\n",
      "  Batch 31,480  of  44,637.    Elapsed: 0:17:59. Training loss. 0.004390016198158264 Num fake examples 30344 Num true examples 32616\n",
      "  Batch 31,520  of  44,637.    Elapsed: 0:18:01. Training loss. 0.003280933480709791 Num fake examples 30393 Num true examples 32647\n",
      "  Batch 31,560  of  44,637.    Elapsed: 0:18:02. Training loss. 2.9236955642700195 Num fake examples 30429 Num true examples 32691\n",
      "  Batch 31,600  of  44,637.    Elapsed: 0:18:03. Training loss. 0.004055206198245287 Num fake examples 30467 Num true examples 32733\n",
      "  Batch 31,640  of  44,637.    Elapsed: 0:18:05. Training loss. 0.003384258830919862 Num fake examples 30511 Num true examples 32769\n",
      "  Batch 31,680  of  44,637.    Elapsed: 0:18:06. Training loss. 0.004242565017193556 Num fake examples 30546 Num true examples 32814\n",
      "  Batch 31,720  of  44,637.    Elapsed: 0:18:07. Training loss. 0.003293066518381238 Num fake examples 30585 Num true examples 32855\n",
      "  Batch 31,760  of  44,637.    Elapsed: 0:18:09. Training loss. 0.004090427421033382 Num fake examples 30626 Num true examples 32894\n",
      "  Batch 31,800  of  44,637.    Elapsed: 0:18:10. Training loss. 0.004425977356731892 Num fake examples 30667 Num true examples 32933\n",
      "  Batch 31,840  of  44,637.    Elapsed: 0:18:11. Training loss. 0.0040186308324337006 Num fake examples 30710 Num true examples 32970\n",
      "  Batch 31,880  of  44,637.    Elapsed: 0:18:13. Training loss. 0.004687550477683544 Num fake examples 30755 Num true examples 33005\n",
      "  Batch 31,920  of  44,637.    Elapsed: 0:18:14. Training loss. 0.0041660042479634285 Num fake examples 30795 Num true examples 33045\n",
      "  Batch 31,960  of  44,637.    Elapsed: 0:18:15. Training loss. 0.004269260913133621 Num fake examples 30835 Num true examples 33085\n",
      "  Batch 32,000  of  44,637.    Elapsed: 0:18:17. Training loss. 2.885716199874878 Num fake examples 30874 Num true examples 33126\n",
      "  Batch 32,040  of  44,637.    Elapsed: 0:18:18. Training loss. 0.0035304827615618706 Num fake examples 30913 Num true examples 33167\n",
      "  Batch 32,080  of  44,637.    Elapsed: 0:18:20. Training loss. 0.0038989337626844645 Num fake examples 30948 Num true examples 33212\n",
      "  Batch 32,120  of  44,637.    Elapsed: 0:18:21. Training loss. 0.003338790964335203 Num fake examples 30987 Num true examples 33253\n",
      "  Batch 32,160  of  44,637.    Elapsed: 0:18:22. Training loss. 0.0040945871733129025 Num fake examples 31035 Num true examples 33285\n",
      "  Batch 32,200  of  44,637.    Elapsed: 0:18:24. Training loss. 0.004110346548259258 Num fake examples 31080 Num true examples 33320\n",
      "  Batch 32,240  of  44,637.    Elapsed: 0:18:25. Training loss. 0.0032452684827148914 Num fake examples 31120 Num true examples 33360\n",
      "  Batch 32,280  of  44,637.    Elapsed: 0:18:26. Training loss. 0.0033036088570952415 Num fake examples 31156 Num true examples 33404\n",
      "  Batch 32,320  of  44,637.    Elapsed: 0:18:28. Training loss. 0.003643382340669632 Num fake examples 31199 Num true examples 33441\n",
      "  Batch 32,360  of  44,637.    Elapsed: 0:18:29. Training loss. 0.0033745819237083197 Num fake examples 31246 Num true examples 33474\n",
      "  Batch 32,400  of  44,637.    Elapsed: 0:18:30. Training loss. 0.003697999520227313 Num fake examples 31287 Num true examples 33513\n",
      "  Batch 32,440  of  44,637.    Elapsed: 0:18:32. Training loss. 0.0030659809708595276 Num fake examples 31332 Num true examples 33548\n",
      "  Batch 32,480  of  44,637.    Elapsed: 0:18:33. Training loss. 0.004167189821600914 Num fake examples 31364 Num true examples 33596\n",
      "  Batch 32,520  of  44,637.    Elapsed: 0:18:34. Training loss. 0.004152428824454546 Num fake examples 31401 Num true examples 33639\n",
      "  Batch 32,560  of  44,637.    Elapsed: 0:18:36. Training loss. 0.004175066016614437 Num fake examples 31431 Num true examples 33689\n",
      "  Batch 32,600  of  44,637.    Elapsed: 0:18:37. Training loss. 0.0036124091129750013 Num fake examples 31460 Num true examples 33740\n",
      "  Batch 32,640  of  44,637.    Elapsed: 0:18:39. Training loss. 0.003497289028018713 Num fake examples 31496 Num true examples 33784\n",
      "  Batch 32,680  of  44,637.    Elapsed: 0:18:40. Training loss. 0.0035017223563045263 Num fake examples 31532 Num true examples 33828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 32,720  of  44,637.    Elapsed: 0:18:41. Training loss. 0.0034747174941003323 Num fake examples 31574 Num true examples 33866\n",
      "  Batch 32,760  of  44,637.    Elapsed: 0:18:43. Training loss. 0.003583331126719713 Num fake examples 31615 Num true examples 33905\n",
      "  Batch 32,800  of  44,637.    Elapsed: 0:18:44. Training loss. 0.0035250592045485973 Num fake examples 31658 Num true examples 33942\n",
      "  Batch 32,840  of  44,637.    Elapsed: 0:18:45. Training loss. 0.0033412440679967403 Num fake examples 31701 Num true examples 33979\n",
      "  Batch 32,880  of  44,637.    Elapsed: 0:18:47. Training loss. 0.0037547992542386055 Num fake examples 31744 Num true examples 34016\n",
      "  Batch 32,920  of  44,637.    Elapsed: 0:18:48. Training loss. 0.0032999785616993904 Num fake examples 31776 Num true examples 34064\n",
      "  Batch 32,960  of  44,637.    Elapsed: 0:18:49. Training loss. 0.003586909733712673 Num fake examples 31813 Num true examples 34107\n",
      "  Batch 33,000  of  44,637.    Elapsed: 0:18:51. Training loss. 0.0039209481328725815 Num fake examples 31859 Num true examples 34141\n",
      "  Batch 33,040  of  44,637.    Elapsed: 0:18:52. Training loss. 0.0031768768094480038 Num fake examples 31893 Num true examples 34187\n",
      "  Batch 33,080  of  44,637.    Elapsed: 0:18:54. Training loss. 0.003619126509875059 Num fake examples 31940 Num true examples 34220\n",
      "  Batch 33,120  of  44,637.    Elapsed: 0:18:55. Training loss. 0.0033159905578941107 Num fake examples 31976 Num true examples 34264\n",
      "  Batch 33,160  of  44,637.    Elapsed: 0:18:56. Training loss. 0.003518975805491209 Num fake examples 32018 Num true examples 34302\n",
      "  Batch 33,200  of  44,637.    Elapsed: 0:18:58. Training loss. 0.0033398065716028214 Num fake examples 32051 Num true examples 34349\n",
      "  Batch 33,240  of  44,637.    Elapsed: 0:18:59. Training loss. 0.003616784233599901 Num fake examples 32096 Num true examples 34384\n",
      "  Batch 33,280  of  44,637.    Elapsed: 0:19:00. Training loss. 0.003326704492792487 Num fake examples 32143 Num true examples 34417\n",
      "  Batch 33,320  of  44,637.    Elapsed: 0:19:02. Training loss. 0.003971678204834461 Num fake examples 32179 Num true examples 34461\n",
      "  Batch 33,360  of  44,637.    Elapsed: 0:19:03. Training loss. 0.0040327939204871655 Num fake examples 32220 Num true examples 34500\n",
      "  Batch 33,400  of  44,637.    Elapsed: 0:19:04. Training loss. 0.0031968967523425817 Num fake examples 32255 Num true examples 34545\n",
      "  Batch 33,440  of  44,637.    Elapsed: 0:19:06. Training loss. 0.0032878946512937546 Num fake examples 32298 Num true examples 34582\n",
      "  Batch 33,480  of  44,637.    Elapsed: 0:19:07. Training loss. 0.0031200465746223927 Num fake examples 32336 Num true examples 34624\n",
      "  Batch 33,520  of  44,637.    Elapsed: 0:19:08. Training loss. 0.00406496599316597 Num fake examples 32371 Num true examples 34669\n",
      "  Batch 33,560  of  44,637.    Elapsed: 0:19:10. Training loss. 0.00338917039334774 Num fake examples 32410 Num true examples 34710\n",
      "  Batch 33,600  of  44,637.    Elapsed: 0:19:11. Training loss. 0.003672495251521468 Num fake examples 32448 Num true examples 34752\n",
      "  Batch 33,640  of  44,637.    Elapsed: 0:19:13. Training loss. 0.003552018664777279 Num fake examples 32481 Num true examples 34799\n",
      "  Batch 33,680  of  44,637.    Elapsed: 0:19:14. Training loss. 0.00364510016515851 Num fake examples 32513 Num true examples 34847\n",
      "  Batch 33,720  of  44,637.    Elapsed: 0:19:15. Training loss. 0.004089218098670244 Num fake examples 32552 Num true examples 34888\n",
      "  Batch 33,760  of  44,637.    Elapsed: 0:19:17. Training loss. 0.0036659494508057833 Num fake examples 32590 Num true examples 34930\n",
      "  Batch 33,800  of  44,637.    Elapsed: 0:19:18. Training loss. 0.004443933255970478 Num fake examples 32635 Num true examples 34965\n",
      "  Batch 33,840  of  44,637.    Elapsed: 0:19:19. Training loss. 0.0036561195738613605 Num fake examples 32668 Num true examples 35012\n",
      "  Batch 33,880  of  44,637.    Elapsed: 0:19:21. Training loss. 0.0038573879282921553 Num fake examples 32701 Num true examples 35059\n",
      "  Batch 33,920  of  44,637.    Elapsed: 0:19:22. Training loss. 0.004435562063008547 Num fake examples 32741 Num true examples 35099\n",
      "  Batch 33,960  of  44,637.    Elapsed: 0:19:23. Training loss. 0.003961142618209124 Num fake examples 32786 Num true examples 35134\n",
      "  Batch 34,000  of  44,637.    Elapsed: 0:19:25. Training loss. 0.004274829290807247 Num fake examples 32828 Num true examples 35172\n",
      "  Batch 34,040  of  44,637.    Elapsed: 0:19:26. Training loss. 0.004007915034890175 Num fake examples 32867 Num true examples 35213\n",
      "  Batch 34,080  of  44,637.    Elapsed: 0:19:27. Training loss. 0.003669471014291048 Num fake examples 32900 Num true examples 35260\n",
      "  Batch 34,120  of  44,637.    Elapsed: 0:19:29. Training loss. 0.0036646085791289806 Num fake examples 32940 Num true examples 35300\n",
      "  Batch 34,160  of  44,637.    Elapsed: 0:19:30. Training loss. 0.004316868260502815 Num fake examples 32974 Num true examples 35346\n",
      "  Batch 34,200  of  44,637.    Elapsed: 0:19:32. Training loss. 0.004289043135941029 Num fake examples 33013 Num true examples 35387\n",
      "  Batch 34,240  of  44,637.    Elapsed: 0:19:33. Training loss. 0.003933290019631386 Num fake examples 33047 Num true examples 35433\n",
      "  Batch 34,280  of  44,637.    Elapsed: 0:19:34. Training loss. 0.0034806025214493275 Num fake examples 33081 Num true examples 35479\n",
      "  Batch 34,320  of  44,637.    Elapsed: 0:19:36. Training loss. 0.0030383788980543613 Num fake examples 33119 Num true examples 35521\n",
      "  Batch 34,360  of  44,637.    Elapsed: 0:19:37. Training loss. 0.003359749913215637 Num fake examples 33168 Num true examples 35552\n",
      "  Batch 34,400  of  44,637.    Elapsed: 0:19:38. Training loss. 0.0035092534963041544 Num fake examples 33203 Num true examples 35597\n",
      "  Batch 34,440  of  44,637.    Elapsed: 0:19:40. Training loss. 0.0035861805081367493 Num fake examples 33241 Num true examples 35639\n",
      "  Batch 34,480  of  44,637.    Elapsed: 0:19:41. Training loss. 0.003971526399254799 Num fake examples 33285 Num true examples 35675\n",
      "  Batch 34,520  of  44,637.    Elapsed: 0:19:43. Training loss. 0.0039653773419559 Num fake examples 33317 Num true examples 35723\n",
      "  Batch 34,560  of  44,637.    Elapsed: 0:19:44. Training loss. 0.003565034829080105 Num fake examples 33355 Num true examples 35765\n",
      "  Batch 34,600  of  44,637.    Elapsed: 0:19:45. Training loss. 0.003073106985539198 Num fake examples 33394 Num true examples 35806\n",
      "  Batch 34,640  of  44,637.    Elapsed: 0:19:47. Training loss. 0.0035913363099098206 Num fake examples 33435 Num true examples 35845\n",
      "  Batch 34,680  of  44,637.    Elapsed: 0:19:48. Training loss. 0.004572238773107529 Num fake examples 33476 Num true examples 35884\n",
      "  Batch 34,720  of  44,637.    Elapsed: 0:19:49. Training loss. 0.0036478638648986816 Num fake examples 33514 Num true examples 35926\n",
      "  Batch 34,760  of  44,637.    Elapsed: 0:19:51. Training loss. 0.00393537525087595 Num fake examples 33564 Num true examples 35956\n",
      "  Batch 34,800  of  44,637.    Elapsed: 0:19:52. Training loss. 0.004114416427910328 Num fake examples 33600 Num true examples 36000\n",
      "  Batch 34,840  of  44,637.    Elapsed: 0:19:53. Training loss. 0.0037655397318303585 Num fake examples 33639 Num true examples 36041\n",
      "  Batch 34,880  of  44,637.    Elapsed: 0:19:55. Training loss. 2.9237520694732666 Num fake examples 33679 Num true examples 36081\n",
      "  Batch 34,920  of  44,637.    Elapsed: 0:19:56. Training loss. 0.003090428188443184 Num fake examples 33723 Num true examples 36117\n",
      "  Batch 34,960  of  44,637.    Elapsed: 0:19:58. Training loss. 0.0036363541148602962 Num fake examples 33762 Num true examples 36158\n",
      "  Batch 35,000  of  44,637.    Elapsed: 0:19:59. Training loss. 0.0036858748644590378 Num fake examples 33799 Num true examples 36201\n",
      "  Batch 35,040  of  44,637.    Elapsed: 0:20:00. Training loss. 0.003305567894130945 Num fake examples 33835 Num true examples 36245\n",
      "  Batch 35,080  of  44,637.    Elapsed: 0:20:02. Training loss. 0.003944360185414553 Num fake examples 33868 Num true examples 36292\n",
      "  Batch 35,120  of  44,637.    Elapsed: 0:20:03. Training loss. 0.003457722719758749 Num fake examples 33908 Num true examples 36332\n",
      "  Batch 35,160  of  44,637.    Elapsed: 0:20:04. Training loss. 0.003994673024863005 Num fake examples 33948 Num true examples 36372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 35,200  of  44,637.    Elapsed: 0:20:06. Training loss. 0.00386889954097569 Num fake examples 33988 Num true examples 36412\n",
      "  Batch 35,240  of  44,637.    Elapsed: 0:20:07. Training loss. 0.0036314460448920727 Num fake examples 34027 Num true examples 36453\n",
      "  Batch 35,280  of  44,637.    Elapsed: 0:20:08. Training loss. 0.002904499415308237 Num fake examples 34054 Num true examples 36506\n",
      "  Batch 35,320  of  44,637.    Elapsed: 0:20:10. Training loss. 0.003850539680570364 Num fake examples 34096 Num true examples 36544\n",
      "  Batch 35,360  of  44,637.    Elapsed: 0:20:11. Training loss. 0.0036886404268443584 Num fake examples 34142 Num true examples 36578\n",
      "  Batch 35,400  of  44,637.    Elapsed: 0:20:13. Training loss. 0.0031843092292547226 Num fake examples 34180 Num true examples 36620\n",
      "  Batch 35,440  of  44,637.    Elapsed: 0:20:14. Training loss. 0.0032216773834079504 Num fake examples 34220 Num true examples 36660\n",
      "  Batch 35,480  of  44,637.    Elapsed: 0:20:15. Training loss. 0.003647829405963421 Num fake examples 34256 Num true examples 36704\n",
      "  Batch 35,520  of  44,637.    Elapsed: 0:20:17. Training loss. 0.00390914361923933 Num fake examples 34302 Num true examples 36738\n",
      "  Batch 35,560  of  44,637.    Elapsed: 0:20:18. Training loss. 0.003977249842137098 Num fake examples 34345 Num true examples 36775\n",
      "  Batch 35,600  of  44,637.    Elapsed: 0:20:19. Training loss. 0.003749826457351446 Num fake examples 34386 Num true examples 36814\n",
      "  Batch 35,640  of  44,637.    Elapsed: 0:20:21. Training loss. 0.0031837415881454945 Num fake examples 34423 Num true examples 36857\n",
      "  Batch 35,680  of  44,637.    Elapsed: 0:20:22. Training loss. 0.003731474746018648 Num fake examples 34466 Num true examples 36894\n",
      "  Batch 35,720  of  44,637.    Elapsed: 0:20:23. Training loss. 2.872468948364258 Num fake examples 34507 Num true examples 36933\n",
      "  Batch 35,760  of  44,637.    Elapsed: 0:20:25. Training loss. 0.004486025311052799 Num fake examples 34545 Num true examples 36975\n",
      "  Batch 35,800  of  44,637.    Elapsed: 0:20:26. Training loss. 0.003864761907607317 Num fake examples 34589 Num true examples 37011\n",
      "  Batch 35,840  of  44,637.    Elapsed: 0:20:28. Training loss. 0.0034189443103969097 Num fake examples 34634 Num true examples 37046\n",
      "  Batch 35,880  of  44,637.    Elapsed: 0:20:29. Training loss. 0.003180628176778555 Num fake examples 34671 Num true examples 37089\n",
      "  Batch 35,920  of  44,637.    Elapsed: 0:20:30. Training loss. 0.0035059931688010693 Num fake examples 34711 Num true examples 37129\n",
      "  Batch 35,960  of  44,637.    Elapsed: 0:20:32. Training loss. 0.004321342334151268 Num fake examples 34751 Num true examples 37169\n",
      "  Batch 36,000  of  44,637.    Elapsed: 0:20:33. Training loss. 0.00374784879386425 Num fake examples 34783 Num true examples 37217\n",
      "  Batch 36,040  of  44,637.    Elapsed: 0:20:34. Training loss. 0.0037451141979545355 Num fake examples 34826 Num true examples 37254\n",
      "  Batch 36,080  of  44,637.    Elapsed: 0:20:36. Training loss. 0.003800319042056799 Num fake examples 34872 Num true examples 37288\n",
      "  Batch 36,120  of  44,637.    Elapsed: 0:20:37. Training loss. 0.0039026024751365185 Num fake examples 34907 Num true examples 37333\n",
      "  Batch 36,160  of  44,637.    Elapsed: 0:20:39. Training loss. 0.0031001102179288864 Num fake examples 34944 Num true examples 37376\n",
      "  Batch 36,200  of  44,637.    Elapsed: 0:20:40. Training loss. 0.00393844535574317 Num fake examples 34979 Num true examples 37421\n",
      "  Batch 36,240  of  44,637.    Elapsed: 0:20:41. Training loss. 0.0034036398865282536 Num fake examples 35020 Num true examples 37460\n",
      "  Batch 36,280  of  44,637.    Elapsed: 0:20:43. Training loss. 0.0033140662126243114 Num fake examples 35059 Num true examples 37501\n",
      "  Batch 36,320  of  44,637.    Elapsed: 0:20:44. Training loss. 0.003570277476683259 Num fake examples 35097 Num true examples 37543\n",
      "  Batch 36,360  of  44,637.    Elapsed: 0:20:45. Training loss. 0.0035130023024976254 Num fake examples 35143 Num true examples 37577\n",
      "  Batch 36,400  of  44,637.    Elapsed: 0:20:47. Training loss. 0.003565544728189707 Num fake examples 35183 Num true examples 37617\n",
      "  Batch 36,440  of  44,637.    Elapsed: 0:20:48. Training loss. 0.0034187419805675745 Num fake examples 35228 Num true examples 37652\n",
      "  Batch 36,480  of  44,637.    Elapsed: 0:20:49. Training loss. 0.003330550855025649 Num fake examples 35268 Num true examples 37692\n",
      "  Batch 36,520  of  44,637.    Elapsed: 0:20:51. Training loss. 0.004645554348826408 Num fake examples 35309 Num true examples 37731\n",
      "  Batch 36,560  of  44,637.    Elapsed: 0:20:52. Training loss. 0.003991109784692526 Num fake examples 35351 Num true examples 37769\n",
      "  Batch 36,600  of  44,637.    Elapsed: 0:20:54. Training loss. 0.003356846747919917 Num fake examples 35391 Num true examples 37809\n",
      "  Batch 36,640  of  44,637.    Elapsed: 0:20:55. Training loss. 0.002954379189759493 Num fake examples 35424 Num true examples 37856\n",
      "  Batch 36,680  of  44,637.    Elapsed: 0:20:56. Training loss. 0.003846952458843589 Num fake examples 35467 Num true examples 37893\n",
      "  Batch 36,720  of  44,637.    Elapsed: 0:20:58. Training loss. 0.0035724062472581863 Num fake examples 35503 Num true examples 37937\n",
      "  Batch 36,760  of  44,637.    Elapsed: 0:20:59. Training loss. 2.6353447437286377 Num fake examples 35548 Num true examples 37972\n",
      "  Batch 36,800  of  44,637.    Elapsed: 0:21:00. Training loss. 0.003501455532386899 Num fake examples 35585 Num true examples 38015\n",
      "  Batch 36,840  of  44,637.    Elapsed: 0:21:02. Training loss. 0.0037532388232648373 Num fake examples 35630 Num true examples 38050\n",
      "  Batch 36,880  of  44,637.    Elapsed: 0:21:03. Training loss. 0.0034999700728803873 Num fake examples 35663 Num true examples 38097\n",
      "  Batch 36,920  of  44,637.    Elapsed: 0:21:04. Training loss. 0.0038543951231986284 Num fake examples 35701 Num true examples 38139\n",
      "  Batch 36,960  of  44,637.    Elapsed: 0:21:06. Training loss. 0.003646255237981677 Num fake examples 35742 Num true examples 38178\n",
      "  Batch 37,000  of  44,637.    Elapsed: 0:21:07. Training loss. 0.0038217520341277122 Num fake examples 35778 Num true examples 38222\n",
      "  Batch 37,040  of  44,637.    Elapsed: 0:21:08. Training loss. 0.003539317287504673 Num fake examples 35820 Num true examples 38260\n",
      "  Batch 37,080  of  44,637.    Elapsed: 0:21:10. Training loss. 0.0035535451024770737 Num fake examples 35858 Num true examples 38302\n",
      "  Batch 37,120  of  44,637.    Elapsed: 0:21:11. Training loss. 0.004220256581902504 Num fake examples 35897 Num true examples 38343\n",
      "  Batch 37,160  of  44,637.    Elapsed: 0:21:13. Training loss. 0.003049365244805813 Num fake examples 35935 Num true examples 38385\n",
      "  Batch 37,200  of  44,637.    Elapsed: 0:21:14. Training loss. 0.0030844758730381727 Num fake examples 35968 Num true examples 38432\n",
      "  Batch 37,240  of  44,637.    Elapsed: 0:21:15. Training loss. 0.0036453495267778635 Num fake examples 36011 Num true examples 38469\n",
      "  Batch 37,280  of  44,637.    Elapsed: 0:21:17. Training loss. 0.003917702008038759 Num fake examples 36045 Num true examples 38515\n",
      "  Batch 37,320  of  44,637.    Elapsed: 0:21:18. Training loss. 0.003148960182443261 Num fake examples 36087 Num true examples 38553\n",
      "  Batch 37,360  of  44,637.    Elapsed: 0:21:19. Training loss. 0.003680738154798746 Num fake examples 36125 Num true examples 38595\n",
      "  Batch 37,400  of  44,637.    Elapsed: 0:21:21. Training loss. 2.790935754776001 Num fake examples 36164 Num true examples 38636\n",
      "  Batch 37,440  of  44,637.    Elapsed: 0:21:22. Training loss. 0.003830790054053068 Num fake examples 36207 Num true examples 38673\n",
      "  Batch 37,480  of  44,637.    Elapsed: 0:21:24. Training loss. 0.0032676076516509056 Num fake examples 36247 Num true examples 38713\n",
      "  Batch 37,520  of  44,637.    Elapsed: 0:21:25. Training loss. 0.0035690171644091606 Num fake examples 36286 Num true examples 38754\n",
      "  Batch 37,560  of  44,637.    Elapsed: 0:21:26. Training loss. 0.0037079229950904846 Num fake examples 36327 Num true examples 38793\n",
      "  Batch 37,600  of  44,637.    Elapsed: 0:21:28. Training loss. 0.00343282800167799 Num fake examples 36366 Num true examples 38834\n",
      "  Batch 37,640  of  44,637.    Elapsed: 0:21:29. Training loss. 0.00341690331697464 Num fake examples 36410 Num true examples 38870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 37,680  of  44,637.    Elapsed: 0:21:30. Training loss. 0.0024615456350147724 Num fake examples 36455 Num true examples 38905\n",
      "  Batch 37,720  of  44,637.    Elapsed: 0:21:32. Training loss. 0.003379537956789136 Num fake examples 36500 Num true examples 38940\n",
      "  Batch 37,760  of  44,637.    Elapsed: 0:21:33. Training loss. 0.003327693324536085 Num fake examples 36536 Num true examples 38984\n",
      "  Batch 37,800  of  44,637.    Elapsed: 0:21:35. Training loss. 0.003252462949603796 Num fake examples 36578 Num true examples 39022\n",
      "  Batch 37,840  of  44,637.    Elapsed: 0:21:36. Training loss. 0.003298703581094742 Num fake examples 36623 Num true examples 39057\n",
      "  Batch 37,880  of  44,637.    Elapsed: 0:21:37. Training loss. 0.003258143085986376 Num fake examples 36674 Num true examples 39086\n",
      "  Batch 37,920  of  44,637.    Elapsed: 0:21:39. Training loss. 0.003007102757692337 Num fake examples 36717 Num true examples 39123\n",
      "  Batch 37,960  of  44,637.    Elapsed: 0:21:40. Training loss. 0.003955494612455368 Num fake examples 36747 Num true examples 39173\n",
      "  Batch 38,000  of  44,637.    Elapsed: 0:21:41. Training loss. 0.0037310842890292406 Num fake examples 36789 Num true examples 39211\n",
      "  Batch 38,040  of  44,637.    Elapsed: 0:21:43. Training loss. 0.003487971145659685 Num fake examples 36832 Num true examples 39248\n",
      "  Batch 38,080  of  44,637.    Elapsed: 0:21:44. Training loss. 0.003755322890356183 Num fake examples 36868 Num true examples 39292\n",
      "  Batch 38,120  of  44,637.    Elapsed: 0:21:46. Training loss. 0.003134023630991578 Num fake examples 36906 Num true examples 39334\n",
      "  Batch 38,160  of  44,637.    Elapsed: 0:21:47. Training loss. 0.004812213592231274 Num fake examples 36954 Num true examples 39366\n",
      "  Batch 38,200  of  44,637.    Elapsed: 0:21:48. Training loss. 2.820760488510132 Num fake examples 36996 Num true examples 39404\n",
      "  Batch 38,240  of  44,637.    Elapsed: 0:21:50. Training loss. 0.003974647261202335 Num fake examples 37034 Num true examples 39446\n",
      "  Batch 38,280  of  44,637.    Elapsed: 0:21:51. Training loss. 0.003227323992177844 Num fake examples 37070 Num true examples 39490\n",
      "  Batch 38,320  of  44,637.    Elapsed: 0:21:52. Training loss. 2.838669538497925 Num fake examples 37113 Num true examples 39527\n",
      "  Batch 38,360  of  44,637.    Elapsed: 0:21:54. Training loss. 0.002645370550453663 Num fake examples 37149 Num true examples 39571\n",
      "  Batch 38,400  of  44,637.    Elapsed: 0:21:55. Training loss. 0.0037058794405311346 Num fake examples 37186 Num true examples 39614\n",
      "  Batch 38,440  of  44,637.    Elapsed: 0:21:57. Training loss. 0.0034744315780699253 Num fake examples 37223 Num true examples 39657\n",
      "  Batch 38,480  of  44,637.    Elapsed: 0:21:58. Training loss. 0.003514081472530961 Num fake examples 37264 Num true examples 39696\n",
      "  Batch 38,520  of  44,637.    Elapsed: 0:21:59. Training loss. 0.004326821304857731 Num fake examples 37296 Num true examples 39744\n",
      "  Batch 38,560  of  44,637.    Elapsed: 0:22:01. Training loss. 0.0037600724026560783 Num fake examples 37336 Num true examples 39784\n",
      "  Batch 38,600  of  44,637.    Elapsed: 0:22:02. Training loss. 0.003458349732682109 Num fake examples 37373 Num true examples 39827\n",
      "  Batch 38,640  of  44,637.    Elapsed: 0:22:03. Training loss. 0.003287697210907936 Num fake examples 37406 Num true examples 39874\n",
      "  Batch 38,680  of  44,637.    Elapsed: 0:22:05. Training loss. 0.0042228493839502335 Num fake examples 37446 Num true examples 39914\n",
      "  Batch 38,720  of  44,637.    Elapsed: 0:22:06. Training loss. 0.004542018286883831 Num fake examples 37495 Num true examples 39945\n",
      "  Batch 38,760  of  44,637.    Elapsed: 0:22:07. Training loss. 0.003407756332308054 Num fake examples 37539 Num true examples 39981\n",
      "  Batch 38,800  of  44,637.    Elapsed: 0:22:09. Training loss. 0.0038559774402529 Num fake examples 37584 Num true examples 40016\n",
      "  Batch 38,840  of  44,637.    Elapsed: 0:22:10. Training loss. 0.003374772146344185 Num fake examples 37629 Num true examples 40051\n",
      "  Batch 38,880  of  44,637.    Elapsed: 0:22:11. Training loss. 0.0031402641907334328 Num fake examples 37672 Num true examples 40088\n",
      "  Batch 38,920  of  44,637.    Elapsed: 0:22:13. Training loss. 2.8469655513763428 Num fake examples 37711 Num true examples 40129\n",
      "  Batch 38,960  of  44,637.    Elapsed: 0:22:14. Training loss. 0.003391352714970708 Num fake examples 37754 Num true examples 40166\n",
      "  Batch 39,000  of  44,637.    Elapsed: 0:22:16. Training loss. 0.003929218277335167 Num fake examples 37789 Num true examples 40211\n",
      "  Batch 39,040  of  44,637.    Elapsed: 0:22:17. Training loss. 0.0034174213651567698 Num fake examples 37822 Num true examples 40258\n",
      "  Batch 39,080  of  44,637.    Elapsed: 0:22:18. Training loss. 0.004034661687910557 Num fake examples 37862 Num true examples 40298\n",
      "  Batch 39,120  of  44,637.    Elapsed: 0:22:20. Training loss. 0.0034378438722342253 Num fake examples 37893 Num true examples 40347\n",
      "  Batch 39,160  of  44,637.    Elapsed: 0:22:21. Training loss. 0.0030817862134426832 Num fake examples 37941 Num true examples 40379\n",
      "  Batch 39,200  of  44,637.    Elapsed: 0:22:22. Training loss. 0.0034050955437123775 Num fake examples 37978 Num true examples 40422\n",
      "  Batch 39,240  of  44,637.    Elapsed: 0:22:24. Training loss. 0.003439473919570446 Num fake examples 38019 Num true examples 40461\n",
      "  Batch 39,280  of  44,637.    Elapsed: 0:22:25. Training loss. 0.0036986125633120537 Num fake examples 38060 Num true examples 40500\n",
      "  Batch 39,320  of  44,637.    Elapsed: 0:22:26. Training loss. 0.0034228104632347822 Num fake examples 38107 Num true examples 40533\n",
      "  Batch 39,360  of  44,637.    Elapsed: 0:22:28. Training loss. 0.0037270456086844206 Num fake examples 38150 Num true examples 40570\n",
      "  Batch 39,400  of  44,637.    Elapsed: 0:22:29. Training loss. 0.0036459406837821007 Num fake examples 38190 Num true examples 40610\n",
      "  Batch 39,440  of  44,637.    Elapsed: 0:22:31. Training loss. 0.0038354923017323017 Num fake examples 38233 Num true examples 40647\n",
      "  Batch 39,480  of  44,637.    Elapsed: 0:22:32. Training loss. 2.8049612045288086 Num fake examples 38268 Num true examples 40692\n",
      "  Batch 39,520  of  44,637.    Elapsed: 0:22:33. Training loss. 0.004333131946623325 Num fake examples 38306 Num true examples 40734\n",
      "  Batch 39,560  of  44,637.    Elapsed: 0:22:35. Training loss. 0.004059415310621262 Num fake examples 38342 Num true examples 40778\n",
      "  Batch 39,600  of  44,637.    Elapsed: 0:22:36. Training loss. 0.003094109008088708 Num fake examples 38386 Num true examples 40814\n",
      "  Batch 39,640  of  44,637.    Elapsed: 0:22:37. Training loss. 2.83613920211792 Num fake examples 38430 Num true examples 40850\n",
      "  Batch 39,680  of  44,637.    Elapsed: 0:22:39. Training loss. 0.0032450128346681595 Num fake examples 38467 Num true examples 40893\n",
      "  Batch 39,720  of  44,637.    Elapsed: 0:22:40. Training loss. 0.004113093484193087 Num fake examples 38507 Num true examples 40933\n",
      "  Batch 39,760  of  44,637.    Elapsed: 0:22:41. Training loss. 0.0038663456216454506 Num fake examples 38551 Num true examples 40969\n",
      "  Batch 39,800  of  44,637.    Elapsed: 0:22:43. Training loss. 0.0034019318409264088 Num fake examples 38598 Num true examples 41002\n",
      "  Batch 39,840  of  44,637.    Elapsed: 0:22:44. Training loss. 0.004200482740998268 Num fake examples 38636 Num true examples 41044\n",
      "  Batch 39,880  of  44,637.    Elapsed: 0:22:46. Training loss. 0.0041937679052352905 Num fake examples 38672 Num true examples 41088\n",
      "  Batch 39,920  of  44,637.    Elapsed: 0:22:47. Training loss. 0.0033202236518263817 Num fake examples 38705 Num true examples 41135\n",
      "  Batch 39,960  of  44,637.    Elapsed: 0:22:48. Training loss. 0.0035200142301619053 Num fake examples 38745 Num true examples 41175\n",
      "  Batch 40,000  of  44,637.    Elapsed: 0:22:50. Training loss. 0.003510862123221159 Num fake examples 38790 Num true examples 41210\n",
      "  Batch 40,040  of  44,637.    Elapsed: 0:22:51. Training loss. 0.0036170559469610453 Num fake examples 38828 Num true examples 41252\n",
      "  Batch 40,080  of  44,637.    Elapsed: 0:22:52. Training loss. 0.0033243128564208746 Num fake examples 38863 Num true examples 41297\n",
      "  Batch 40,120  of  44,637.    Elapsed: 0:22:54. Training loss. 0.004080123268067837 Num fake examples 38911 Num true examples 41329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 40,160  of  44,637.    Elapsed: 0:22:55. Training loss. 0.003550773486495018 Num fake examples 38954 Num true examples 41366\n",
      "  Batch 40,200  of  44,637.    Elapsed: 0:22:57. Training loss. 0.0037427640054374933 Num fake examples 38996 Num true examples 41404\n",
      "  Batch 40,240  of  44,637.    Elapsed: 0:22:58. Training loss. 2.905529499053955 Num fake examples 39035 Num true examples 41445\n",
      "  Batch 40,280  of  44,637.    Elapsed: 0:22:59. Training loss. 0.003340526483952999 Num fake examples 39080 Num true examples 41480\n",
      "  Batch 40,320  of  44,637.    Elapsed: 0:23:01. Training loss. 0.003293250687420368 Num fake examples 39124 Num true examples 41516\n",
      "  Batch 40,360  of  44,637.    Elapsed: 0:23:02. Training loss. 0.003886784892529249 Num fake examples 39164 Num true examples 41556\n",
      "  Batch 40,400  of  44,637.    Elapsed: 0:23:03. Training loss. 0.003645306918770075 Num fake examples 39212 Num true examples 41588\n",
      "  Batch 40,440  of  44,637.    Elapsed: 0:23:05. Training loss. 0.003296174108982086 Num fake examples 39256 Num true examples 41624\n",
      "  Batch 40,480  of  44,637.    Elapsed: 0:23:06. Training loss. 0.004162660334259272 Num fake examples 39299 Num true examples 41661\n",
      "  Batch 40,520  of  44,637.    Elapsed: 0:23:07. Training loss. 0.003302833065390587 Num fake examples 39331 Num true examples 41709\n",
      "  Batch 40,560  of  44,637.    Elapsed: 0:23:09. Training loss. 0.0032775525469332933 Num fake examples 39372 Num true examples 41748\n",
      "  Batch 40,600  of  44,637.    Elapsed: 0:23:10. Training loss. 0.003639403497800231 Num fake examples 39420 Num true examples 41780\n",
      "  Batch 40,640  of  44,637.    Elapsed: 0:23:11. Training loss. 0.003355521708726883 Num fake examples 39449 Num true examples 41831\n",
      "  Batch 40,680  of  44,637.    Elapsed: 0:23:13. Training loss. 0.003329011145979166 Num fake examples 39484 Num true examples 41876\n",
      "  Batch 40,720  of  44,637.    Elapsed: 0:23:14. Training loss. 0.0034889699891209602 Num fake examples 39515 Num true examples 41925\n",
      "  Batch 40,760  of  44,637.    Elapsed: 0:23:15. Training loss. 0.003518112003803253 Num fake examples 39557 Num true examples 41963\n",
      "  Batch 40,800  of  44,637.    Elapsed: 0:23:17. Training loss. 0.003952271305024624 Num fake examples 39600 Num true examples 42000\n",
      "  Batch 40,840  of  44,637.    Elapsed: 0:23:18. Training loss. 0.0034236209467053413 Num fake examples 39629 Num true examples 42051\n",
      "  Batch 40,880  of  44,637.    Elapsed: 0:23:20. Training loss. 0.003326204139739275 Num fake examples 39666 Num true examples 42094\n",
      "  Batch 40,920  of  44,637.    Elapsed: 0:23:21. Training loss. 0.0038256461266428232 Num fake examples 39708 Num true examples 42132\n",
      "  Batch 40,960  of  44,637.    Elapsed: 0:23:22. Training loss. 0.004043786320835352 Num fake examples 39751 Num true examples 42169\n",
      "  Batch 41,000  of  44,637.    Elapsed: 0:23:24. Training loss. 0.0035446020774543285 Num fake examples 39792 Num true examples 42208\n",
      "  Batch 41,040  of  44,637.    Elapsed: 0:23:25. Training loss. 2.880019426345825 Num fake examples 39827 Num true examples 42253\n",
      "  Batch 41,080  of  44,637.    Elapsed: 0:23:26. Training loss. 0.0031830845400691032 Num fake examples 39874 Num true examples 42286\n",
      "  Batch 41,120  of  44,637.    Elapsed: 0:23:28. Training loss. 0.004100610036402941 Num fake examples 39913 Num true examples 42327\n",
      "  Batch 41,160  of  44,637.    Elapsed: 0:23:29. Training loss. 0.003978741355240345 Num fake examples 39951 Num true examples 42369\n",
      "  Batch 41,200  of  44,637.    Elapsed: 0:23:30. Training loss. 0.0041356575675308704 Num fake examples 39991 Num true examples 42409\n",
      "  Batch 41,240  of  44,637.    Elapsed: 0:23:32. Training loss. 0.00351045373827219 Num fake examples 40027 Num true examples 42453\n",
      "  Batch 41,280  of  44,637.    Elapsed: 0:23:33. Training loss. 0.0035778144374489784 Num fake examples 40062 Num true examples 42498\n",
      "  Batch 41,320  of  44,637.    Elapsed: 0:23:35. Training loss. 0.004082519561052322 Num fake examples 40105 Num true examples 42535\n",
      "  Batch 41,360  of  44,637.    Elapsed: 0:23:36. Training loss. 0.003186034969985485 Num fake examples 40142 Num true examples 42578\n",
      "  Batch 41,400  of  44,637.    Elapsed: 0:23:37. Training loss. 0.003959298133850098 Num fake examples 40186 Num true examples 42614\n",
      "  Batch 41,440  of  44,637.    Elapsed: 0:23:39. Training loss. 0.004552855156362057 Num fake examples 40220 Num true examples 42660\n",
      "  Batch 41,480  of  44,637.    Elapsed: 0:23:40. Training loss. 0.0036274176090955734 Num fake examples 40257 Num true examples 42703\n",
      "  Batch 41,520  of  44,637.    Elapsed: 0:23:41. Training loss. 0.004071669653058052 Num fake examples 40304 Num true examples 42736\n",
      "  Batch 41,560  of  44,637.    Elapsed: 0:23:43. Training loss. 0.0032432880252599716 Num fake examples 40337 Num true examples 42783\n",
      "  Batch 41,600  of  44,637.    Elapsed: 0:23:44. Training loss. 0.003263321239501238 Num fake examples 40373 Num true examples 42827\n",
      "  Batch 41,640  of  44,637.    Elapsed: 0:23:46. Training loss. 0.0034095675218850374 Num fake examples 40406 Num true examples 42874\n",
      "  Batch 41,680  of  44,637.    Elapsed: 0:23:47. Training loss. 0.0030313043389469385 Num fake examples 40449 Num true examples 42911\n",
      "  Batch 41,720  of  44,637.    Elapsed: 0:23:48. Training loss. 0.003507407382130623 Num fake examples 40489 Num true examples 42951\n",
      "  Batch 41,760  of  44,637.    Elapsed: 0:23:50. Training loss. 0.0034523368813097477 Num fake examples 40519 Num true examples 43001\n",
      "  Batch 41,800  of  44,637.    Elapsed: 0:23:51. Training loss. 0.0034388459753245115 Num fake examples 40563 Num true examples 43037\n",
      "  Batch 41,840  of  44,637.    Elapsed: 0:23:52. Training loss. 0.002880774438381195 Num fake examples 40608 Num true examples 43072\n",
      "  Batch 41,880  of  44,637.    Elapsed: 0:23:54. Training loss. 0.0039028802420943975 Num fake examples 40646 Num true examples 43114\n",
      "  Batch 41,920  of  44,637.    Elapsed: 0:23:55. Training loss. 0.003640389535576105 Num fake examples 40682 Num true examples 43158\n",
      "  Batch 41,960  of  44,637.    Elapsed: 0:23:57. Training loss. 0.0032759476453065872 Num fake examples 40726 Num true examples 43194\n",
      "  Batch 42,000  of  44,637.    Elapsed: 0:23:58. Training loss. 0.004085625987499952 Num fake examples 40768 Num true examples 43232\n",
      "  Batch 42,040  of  44,637.    Elapsed: 0:23:59. Training loss. 0.0035855513997375965 Num fake examples 40811 Num true examples 43269\n",
      "  Batch 42,080  of  44,637.    Elapsed: 0:24:01. Training loss. 0.0031930766999721527 Num fake examples 40844 Num true examples 43316\n",
      "  Batch 42,120  of  44,637.    Elapsed: 0:24:02. Training loss. 0.0033117136918008327 Num fake examples 40891 Num true examples 43349\n",
      "  Batch 42,160  of  44,637.    Elapsed: 0:24:04. Training loss. 0.003178277052938938 Num fake examples 40928 Num true examples 43392\n",
      "  Batch 42,200  of  44,637.    Elapsed: 0:24:05. Training loss. 0.003707848722115159 Num fake examples 40969 Num true examples 43431\n",
      "  Batch 42,240  of  44,637.    Elapsed: 0:24:06. Training loss. 0.0036433397326618433 Num fake examples 41014 Num true examples 43466\n",
      "  Batch 42,280  of  44,637.    Elapsed: 0:24:08. Training loss. 0.0035064327530562878 Num fake examples 41054 Num true examples 43506\n",
      "  Batch 42,320  of  44,637.    Elapsed: 0:24:09. Training loss. 0.003071107901632786 Num fake examples 41091 Num true examples 43549\n",
      "  Batch 42,360  of  44,637.    Elapsed: 0:24:10. Training loss. 0.003639455419033766 Num fake examples 41129 Num true examples 43591\n",
      "  Batch 42,400  of  44,637.    Elapsed: 0:24:12. Training loss. 0.0036732449661940336 Num fake examples 41171 Num true examples 43629\n",
      "  Batch 42,440  of  44,637.    Elapsed: 0:24:13. Training loss. 0.0031868433579802513 Num fake examples 41213 Num true examples 43667\n",
      "  Batch 42,480  of  44,637.    Elapsed: 0:24:15. Training loss. 0.003962610848248005 Num fake examples 41248 Num true examples 43712\n",
      "  Batch 42,520  of  44,637.    Elapsed: 0:24:16. Training loss. 0.0033850595355033875 Num fake examples 41286 Num true examples 43754\n",
      "  Batch 42,560  of  44,637.    Elapsed: 0:24:17. Training loss. 0.003811988979578018 Num fake examples 41324 Num true examples 43796\n",
      "  Batch 42,600  of  44,637.    Elapsed: 0:24:19. Training loss. 0.0034354999661445618 Num fake examples 41357 Num true examples 43843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 42,640  of  44,637.    Elapsed: 0:24:20. Training loss. 0.003329220926389098 Num fake examples 41392 Num true examples 43888\n",
      "  Batch 42,680  of  44,637.    Elapsed: 0:24:21. Training loss. 0.0031634513288736343 Num fake examples 41432 Num true examples 43928\n",
      "  Batch 42,720  of  44,637.    Elapsed: 0:24:23. Training loss. 0.00364414369687438 Num fake examples 41472 Num true examples 43968\n",
      "  Batch 42,760  of  44,637.    Elapsed: 0:24:24. Training loss. 0.003652191488072276 Num fake examples 41509 Num true examples 44011\n",
      "  Batch 42,800  of  44,637.    Elapsed: 0:24:26. Training loss. 0.00382065586745739 Num fake examples 41545 Num true examples 44055\n",
      "  Batch 42,840  of  44,637.    Elapsed: 0:24:27. Training loss. 0.003237821627408266 Num fake examples 41590 Num true examples 44090\n",
      "  Batch 42,880  of  44,637.    Elapsed: 0:24:28. Training loss. 0.0039036429952830076 Num fake examples 41625 Num true examples 44135\n",
      "  Batch 42,920  of  44,637.    Elapsed: 0:24:30. Training loss. 0.0038167585153132677 Num fake examples 41658 Num true examples 44182\n",
      "  Batch 42,960  of  44,637.    Elapsed: 0:24:31. Training loss. 0.004115082323551178 Num fake examples 41695 Num true examples 44225\n",
      "  Batch 43,000  of  44,637.    Elapsed: 0:24:32. Training loss. 0.0030932817608118057 Num fake examples 41727 Num true examples 44273\n",
      "  Batch 43,040  of  44,637.    Elapsed: 0:24:34. Training loss. 0.0035937007050961256 Num fake examples 41766 Num true examples 44314\n",
      "  Batch 43,080  of  44,637.    Elapsed: 0:24:35. Training loss. 0.0031031807884573936 Num fake examples 41808 Num true examples 44352\n",
      "  Batch 43,120  of  44,637.    Elapsed: 0:24:37. Training loss. 0.0031502405181527138 Num fake examples 41846 Num true examples 44394\n",
      "  Batch 43,160  of  44,637.    Elapsed: 0:24:38. Training loss. 0.00435749301686883 Num fake examples 41893 Num true examples 44427\n",
      "  Batch 43,200  of  44,637.    Elapsed: 0:24:39. Training loss. 0.003919241949915886 Num fake examples 41922 Num true examples 44478\n",
      "  Batch 43,240  of  44,637.    Elapsed: 0:24:41. Training loss. 0.003808310255408287 Num fake examples 41962 Num true examples 44518\n",
      "  Batch 43,280  of  44,637.    Elapsed: 0:24:42. Training loss. 0.003657026682049036 Num fake examples 42002 Num true examples 44558\n",
      "  Batch 43,320  of  44,637.    Elapsed: 0:24:43. Training loss. 0.003969479352235794 Num fake examples 42046 Num true examples 44594\n",
      "  Batch 43,360  of  44,637.    Elapsed: 0:24:45. Training loss. 0.003986002877354622 Num fake examples 42086 Num true examples 44634\n",
      "  Batch 43,400  of  44,637.    Elapsed: 0:24:46. Training loss. 0.0037763258442282677 Num fake examples 42130 Num true examples 44670\n",
      "  Batch 43,440  of  44,637.    Elapsed: 0:24:48. Training loss. 0.0034352890215814114 Num fake examples 42179 Num true examples 44701\n",
      "  Batch 43,480  of  44,637.    Elapsed: 0:24:49. Training loss. 0.003512459108605981 Num fake examples 42219 Num true examples 44741\n",
      "  Batch 43,520  of  44,637.    Elapsed: 0:24:50. Training loss. 2.7762138843536377 Num fake examples 42248 Num true examples 44792\n",
      "  Batch 43,560  of  44,637.    Elapsed: 0:24:52. Training loss. 0.0033233584836125374 Num fake examples 42284 Num true examples 44836\n",
      "  Batch 43,600  of  44,637.    Elapsed: 0:24:53. Training loss. 0.003539815777912736 Num fake examples 42319 Num true examples 44881\n",
      "  Batch 43,640  of  44,637.    Elapsed: 0:24:54. Training loss. 0.003238226054236293 Num fake examples 42358 Num true examples 44922\n",
      "  Batch 43,680  of  44,637.    Elapsed: 0:24:56. Training loss. 0.003863917663693428 Num fake examples 42393 Num true examples 44967\n",
      "  Batch 43,720  of  44,637.    Elapsed: 0:24:57. Training loss. 0.003660876303911209 Num fake examples 42434 Num true examples 45006\n",
      "  Batch 43,760  of  44,637.    Elapsed: 0:24:58. Training loss. 0.0034797980915755033 Num fake examples 42474 Num true examples 45046\n",
      "  Batch 43,800  of  44,637.    Elapsed: 0:25:00. Training loss. 0.003798458492383361 Num fake examples 42514 Num true examples 45086\n",
      "  Batch 43,840  of  44,637.    Elapsed: 0:25:01. Training loss. 0.0037787053734064102 Num fake examples 42546 Num true examples 45134\n",
      "  Batch 43,880  of  44,637.    Elapsed: 0:25:03. Training loss. 0.0039017703384160995 Num fake examples 42586 Num true examples 45174\n",
      "  Batch 43,920  of  44,637.    Elapsed: 0:25:04. Training loss. 0.003842422738671303 Num fake examples 42629 Num true examples 45211\n",
      "  Batch 43,960  of  44,637.    Elapsed: 0:25:05. Training loss. 0.0030809000600129366 Num fake examples 42674 Num true examples 45246\n",
      "  Batch 44,000  of  44,637.    Elapsed: 0:25:07. Training loss. 0.004078633617609739 Num fake examples 42714 Num true examples 45286\n",
      "  Batch 44,040  of  44,637.    Elapsed: 0:25:08. Training loss. 0.003710234072059393 Num fake examples 42756 Num true examples 45324\n",
      "  Batch 44,080  of  44,637.    Elapsed: 0:25:09. Training loss. 0.00330107263289392 Num fake examples 42796 Num true examples 45364\n",
      "  Batch 44,120  of  44,637.    Elapsed: 0:25:11. Training loss. 0.00303461542353034 Num fake examples 42842 Num true examples 45398\n",
      "  Batch 44,160  of  44,637.    Elapsed: 0:25:12. Training loss. 0.004673202522099018 Num fake examples 42886 Num true examples 45434\n",
      "  Batch 44,200  of  44,637.    Elapsed: 0:25:14. Training loss. 0.003942183218896389 Num fake examples 42931 Num true examples 45469\n",
      "  Batch 44,240  of  44,637.    Elapsed: 0:25:15. Training loss. 0.003121492452919483 Num fake examples 42974 Num true examples 45506\n",
      "  Batch 44,280  of  44,637.    Elapsed: 0:25:16. Training loss. 0.003930447623133659 Num fake examples 43005 Num true examples 45555\n",
      "  Batch 44,320  of  44,637.    Elapsed: 0:25:18. Training loss. 0.0037547298707067966 Num fake examples 43034 Num true examples 45606\n",
      "  Batch 44,360  of  44,637.    Elapsed: 0:25:19. Training loss. 0.0036680945195257664 Num fake examples 43070 Num true examples 45650\n",
      "  Batch 44,400  of  44,637.    Elapsed: 0:25:21. Training loss. 0.004084518179297447 Num fake examples 43110 Num true examples 45690\n",
      "  Batch 44,440  of  44,637.    Elapsed: 0:25:22. Training loss. 0.0035683177411556244 Num fake examples 43151 Num true examples 45729\n",
      "  Batch 44,480  of  44,637.    Elapsed: 0:25:23. Training loss. 0.0039050832856446505 Num fake examples 43183 Num true examples 45777\n",
      "  Batch 44,520  of  44,637.    Elapsed: 0:25:25. Training loss. 0.003653381485491991 Num fake examples 43229 Num true examples 45811\n",
      "  Batch 44,560  of  44,637.    Elapsed: 0:25:26. Training loss. 0.0034655826166272163 Num fake examples 43270 Num true examples 45850\n",
      "  Batch 44,600  of  44,637.    Elapsed: 0:25:27. Training loss. 0.0036656344309449196 Num fake examples 43310 Num true examples 45890\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epcoh took: 0:25:29\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "  Validation Loss: 0.18\n",
      "  Validation took: 0:01:54\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:54:55 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_fake_examples = 0\n",
    "    total_true_examples = 0\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        #if step > 2000:\n",
    "        #    break\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Training loss. {:} Num fake examples {:} Num true examples {:}'.format(step, len(train_dataloader), elapsed, train_loss,total_fake_examples, total_true_examples ))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(torch.int64).to(device)\n",
    "        total_fake_examples += (b_labels == 1).sum().item()\n",
    "        total_true_examples += (b_labels == 0).sum().item()\n",
    "        #print (f\"{b_labels.shape=}\")\n",
    "        b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "        #print (b_input_ids.shape, b_labels.shape, b_input_mask.shape, b_labels_one_hot.shape, b_labels_one_hot.dtype)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels_one_hot)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        train_loss= loss.item()\n",
    "        total_train_loss += train_loss\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        #print (f\"Training loss\", loss.item())\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    \n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "        #if step > 2000:\n",
    "        #    break\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(torch.int64).to(device)\n",
    "        b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            \n",
    "            output = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels_one_hot)\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    #Save model checkpoint\n",
    "    model.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6bb68-246e-4093-afb2-cf4363067b8a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "873821bc-1f66-44fe-acd0-7a312c358621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 5.7428, -5.7319]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/jcrowe/truth/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def encode(sentence):\n",
    "    return tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 410,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "SENTENCE_INDEX = 5000\n",
    "encoded_dict = encode(sentences[SENTENCE_INDEX])\n",
    "input_id = encoded_dict['input_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "print (input_id.shape)\n",
    "model.eval()\n",
    "output = model(\n",
    "            input_id.cuda(),\n",
    "            token_type_ids=None, \n",
    "            attention_mask=attention_mask.cuda(), return_dict=True)\n",
    "print (output)\n",
    "print (labels[SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdd7d7-aae7-4b04-afcc-664161c3e215",
   "metadata": {},
   "source": [
    "## Using validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e7064487-72c4-43ba-a7ac-128221794554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a534156a-198b-495a-94b8-20b3e28f10a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SENTENCE_INDEX = 5000\n",
    "model.eval()\n",
    "print (torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(), dim=0).shape)\n",
    "output = model(torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(), dim=0),\n",
    "            token_type_ids=None, \n",
    "            attention_mask=torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(),dim=0), return_dict=True)\n",
    "print (output)\n",
    "print (labels[SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca1659b2-4081-4161-b331-c0092badb90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.4784, -9.5297],\n",
      "        [ 9.4872, -9.5353]], device='cuda:0') tensor([0, 0], device='cuda:0')\n",
      "tensor([[ 9.4721, -9.4860],\n",
      "        [ 9.4724, -9.5277]], device='cuda:0') tensor([0, 0], device='cuda:0')\n",
      "tensor([[-9.0873,  9.0787],\n",
      "        [-9.5947,  9.6193]], device='cuda:0') tensor([1, 1], device='cuda:0')\n",
      "tensor([[ 9.4725, -9.5319],\n",
      "        [-9.5947,  9.6335]], device='cuda:0') tensor([0, 1], device='cuda:0')\n",
      "tensor([[-9.0775,  9.0677],\n",
      "        [-9.3149,  9.3171]], device='cuda:0') tensor([1, 1], device='cuda:0')\n",
      "tensor([[-9.5118,  9.5746],\n",
      "        [-9.5674,  9.6171]], device='cuda:0') tensor([1, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(validation_dataloader):\n",
    "    if step > 5:\n",
    "        break\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(torch.int64).to(device)\n",
    "    b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "    \n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        \n",
    "        output = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels_one_hot)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "        print (logits, b_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f170c-91e6-4153-a3cf-872a6550bf69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Truth",
   "language": "python",
   "name": "truth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
